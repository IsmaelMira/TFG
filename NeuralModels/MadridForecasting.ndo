<NeuralDesignerOutput>
 <Task Id="w6Uov1" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="FqqoW6" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="XfFs51" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="O1F1dh" Title="Quasi-Newton method errors history">
   <Caption Id="mgHNvn">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.78349, and the final value after 170 epochs is 0.0821993.
The initial value of the selection error is 1.64658, and the final value after 170 epochs is 0.152497.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170</X2Data>
   <Y1Data>1.78\0.8\0.714\0.54\0.485\0.442\0.376\0.336\0.303\0.283\0.27\0.258\0.252\0.24\0.23\0.222\0.217\0.21\0.202\0.195\0.19\0.186\0.179\0.173\0.169\0.166\0.162\0.158\0.153\0.148\0.143\0.139\0.136\0.134\0.131\0.129\0.127\0.125\0.122\0.117\0.116\0.114\0.112\0.11\0.109\0.108\0.106\0.105\0.104\0.103\0.102\0.102\0.102\0.101\0.101\0.1\0.0998\0.0994\0.099\0.0987\0.0985\0.098\0.0977\0.0975\0.0972\0.0969\0.0966\0.0962\0.0959\0.0955\0.095\0.0945\0.0941\0.0935\0.0931\0.0926\0.0923\0.0919\0.0916\0.0912\0.0909\0.0906\0.0902\0.0898\0.0895\0.0893\0.089\0.0887\0.0885\0.088\0.0878\0.0875\0.0873\0.0869\0.0867\0.0864\0.0861\0.0858\0.0856\0.0853\0.0851\0.0848\0.0846\0.0845\0.0843\0.0841\0.084\0.0838\0.0837\0.0835\0.0835\0.0833\0.0833\0.0831\0.083\0.083\0.0828\0.0827\0.0826\0.0825\0.0825\0.0825\0.0825\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822\0.0822</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.65\0.837\0.736\0.604\0.528\0.504\0.499\0.466\0.397\0.383\0.357\0.345\0.327\0.317\0.292\0.296\0.301\0.302\0.303\0.288\0.281\0.273\0.261\0.253\0.253\0.246\0.228\0.218\0.222\0.217\0.217\0.205\0.203\0.196\0.192\0.188\0.187\0.178\0.173\0.185\0.192\0.195\0.202\0.199\0.193\0.193\0.192\0.192\0.194\0.194\0.19\0.193\0.193\0.188\0.184\0.18\0.181\0.178\0.175\0.177\0.181\0.182\0.184\0.182\0.18\0.183\0.182\0.18\0.181\0.18\0.177\0.177\0.176\0.172\0.173\0.174\0.179\0.179\0.178\0.179\0.177\0.176\0.174\0.174\0.173\0.173\0.17\0.168\0.166\0.165\0.164\0.162\0.162\0.162\0.163\0.163\0.162\0.16\0.159\0.159\0.159\0.159\0.16\0.16\0.161\0.16\0.16\0.16\0.158\0.157\0.157\0.155\0.154\0.153\0.152\0.153\0.154\0.153\0.155\0.154\0.154\0.154\0.154\0.154\0.154\0.154\0.154\0.154\0.154\0.154\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.152\0.152\0.152\0.152</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>171</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="m1wvNj" Title="Quasi-Newton method results">
   <Caption Id="4MsJ9s">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0822
0.152
170
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="5fr8VW" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="rClqMF" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="km1Jh3" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="yl53eb">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0179443\95.7384\5.93014\7.40943
0.000115028\0.613708\0.0380137\0.0474963
0.0115028\61.3708\3.80137\4.74963</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="6EHw71" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="owtKCh" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="C9EAqd" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="ANkK0v" Title="Quasi-Newton method errors history">
   <Caption Id="kxl8zD">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.85941, and the final value after 158 epochs is 0.280931.
The initial value of the selection error is 1.70968, and the final value after 158 epochs is 0.287086.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158</X2Data>
   <Y1Data>1.86\0.772\0.627\0.557\0.52\0.507\0.483\0.47\0.463\0.448\0.441\0.43\0.424\0.416\0.411\0.4\0.394\0.388\0.381\0.373\0.369\0.366\0.36\0.357\0.355\0.351\0.346\0.342\0.34\0.338\0.336\0.334\0.332\0.33\0.329\0.328\0.326\0.326\0.323\0.322\0.321\0.32\0.319\0.318\0.317\0.316\0.315\0.315\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.312\0.311\0.311\0.31\0.309\0.309\0.308\0.308\0.307\0.306\0.306\0.306\0.305\0.304\0.304\0.303\0.302\0.302\0.301\0.3\0.3\0.299\0.298\0.297\0.296\0.296\0.295\0.294\0.293\0.293\0.292\0.291\0.291\0.29\0.29\0.289\0.289\0.288\0.288\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.71\0.647\0.473\0.431\0.372\0.363\0.339\0.299\0.302\0.311\0.306\0.298\0.298\0.308\0.317\0.323\0.31\0.323\0.319\0.315\0.305\0.312\0.3\0.301\0.298\0.294\0.286\0.29\0.293\0.299\0.299\0.293\0.289\0.284\0.285\0.29\0.288\0.285\0.282\0.284\0.288\0.284\0.284\0.275\0.267\0.264\0.262\0.264\0.266\0.26\0.257\0.257\0.259\0.263\0.267\0.271\0.267\0.267\0.267\0.269\0.27\0.268\0.265\0.268\0.266\0.264\0.27\0.269\0.27\0.271\0.274\0.275\0.273\0.271\0.27\0.272\0.27\0.277\0.275\0.278\0.277\0.275\0.279\0.279\0.281\0.278\0.281\0.279\0.279\0.279\0.276\0.276\0.275\0.274\0.275\0.274\0.276\0.276\0.277\0.279\0.281\0.282\0.282\0.282\0.282\0.282\0.284\0.284\0.284\0.283\0.284\0.284\0.285\0.286\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.289\0.289\0.288\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>159</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="COarQt" Title="Quasi-Newton method results">
   <Caption Id="wqo6L8">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.281
0.287
158
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="N0OTtB" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="IsR1HG" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="3zNqWz" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="oyqFq6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228121 and its percentage error 2.28121</Caption>
   <Data>0.0274229\300.605\7.36631\13.9064
8.06556e-5\0.884134\0.0216656\0.0409012
0.00806556\88.4134\2.16656\4.09012</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ennW0S" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="S0dqwE" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="ZdE02t" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="kZacry" Title="Quasi-Newton method errors history">
   <Caption Id="UfNzEZ">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 4.33642, and the final value after 155 epochs is 0.404517.
The initial value of the selection error is 4.89974, and the final value after 155 epochs is 0.31428.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155</X2Data>
   <Y1Data>4.34\0.783\0.745\0.713\0.693\0.672\0.657\0.647\0.631\0.619\0.611\0.604\0.599\0.591\0.579\0.573\0.57\0.562\0.558\0.556\0.553\0.548\0.545\0.534\0.529\0.521\0.515\0.512\0.51\0.505\0.498\0.49\0.484\0.479\0.474\0.469\0.466\0.462\0.459\0.455\0.452\0.45\0.447\0.444\0.441\0.439\0.437\0.436\0.435\0.434\0.433\0.432\0.431\0.431\0.43\0.429\0.428\0.427\0.426\0.426\0.426\0.425\0.424\0.423\0.423\0.422\0.421\0.421\0.42\0.42\0.419\0.419\0.418\0.418\0.418\0.417\0.417\0.417\0.416\0.415\0.414\0.414\0.413\0.413\0.412\0.412\0.411\0.411\0.411\0.41\0.41\0.409\0.409\0.409\0.409\0.408\0.408\0.408\0.408\0.408\0.407\0.407\0.407\0.407\0.406\0.406\0.406\0.406\0.406\0.406\0.406\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>4.9\0.456\0.477\0.473\0.454\0.476\0.437\0.409\0.425\0.426\0.398\0.397\0.409\0.426\0.404\0.379\0.383\0.393\0.372\0.365\0.338\0.344\0.362\0.36\0.358\0.346\0.336\0.34\0.351\0.355\0.367\0.378\0.366\0.355\0.336\0.354\0.362\0.35\0.331\0.326\0.321\0.31\0.307\0.289\0.292\0.291\0.293\0.29\0.299\0.298\0.294\0.3\0.297\0.296\0.299\0.302\0.298\0.295\0.289\0.294\0.296\0.294\0.291\0.291\0.29\0.295\0.295\0.292\0.292\0.287\0.287\0.284\0.284\0.283\0.284\0.282\0.282\0.286\0.287\0.293\0.294\0.292\0.288\0.291\0.295\0.296\0.296\0.297\0.298\0.303\0.303\0.304\0.303\0.302\0.304\0.306\0.306\0.307\0.308\0.308\0.309\0.31\0.311\0.311\0.312\0.313\0.312\0.313\0.311\0.311\0.312\0.312\0.312\0.312\0.313\0.312\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.315\0.314\0.314\0.314\0.314\0.315\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>156</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>5</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="P4nvNP" Title="Quasi-Newton method results">
   <Caption Id="zRrUXC">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.405
0.314
155
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="LRtP9m" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="Yk8lvT" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="xPmnUd" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="6z6W2j">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0796335 and its percentage error 7.96336</Caption>
   <Data>0.0572777\28.4248\5.09922\4.21668
0.000230958\0.114616\0.0205614\0.0170027
0.0230958\11.4616\2.05614\1.70027</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="FugOnP" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="agEGp1" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="lbpzu0" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="ebgoLm" Title="Quasi-Newton method errors history">
   <Caption Id="NgOAoK">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.77823, and the final value after 240 epochs is 0.347785.
The initial value of the selection error is 1.35511, and the final value after 240 epochs is 0.450213.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240</X2Data>
   <Y1Data>1.78\0.899\0.8\0.768\0.729\0.684\0.64\0.631\0.612\0.59\0.562\0.554\0.549\0.534\0.511\0.494\0.487\0.481\0.474\0.469\0.463\0.461\0.457\0.454\0.451\0.448\0.445\0.441\0.438\0.434\0.431\0.428\0.426\0.424\0.422\0.421\0.42\0.419\0.418\0.417\0.415\0.414\0.412\0.411\0.41\0.409\0.408\0.406\0.405\0.403\0.402\0.401\0.401\0.4\0.399\0.399\0.398\0.397\0.397\0.396\0.395\0.395\0.395\0.394\0.394\0.393\0.393\0.393\0.392\0.392\0.392\0.392\0.391\0.391\0.391\0.391\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.387\0.388\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.384\0.384\0.384\0.384\0.384\0.385\0.385\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.384\0.384\0.384\0.383\0.383\0.382\0.382\0.381\0.38\0.378\0.376\0.375\0.374\0.373\0.371\0.368\0.366\0.365\0.363\0.362\0.361\0.361\0.36\0.359\0.359\0.358\0.357\0.356\0.355\0.355\0.354\0.354\0.353\0.353\0.353\0.352\0.352\0.351\0.351\0.351\0.35\0.35\0.35\0.349\0.349\0.348</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.36\1.3\1.22\1.05\1.16\0.901\0.765\0.797\0.837\0.67\0.707\0.662\0.651\0.665\0.716\0.734\0.68\0.668\0.672\0.663\0.656\0.659\0.675\0.677\0.663\0.663\0.675\0.691\0.683\0.639\0.605\0.607\0.611\0.614\0.605\0.594\0.574\0.572\0.57\0.594\0.604\0.599\0.579\0.555\0.552\0.557\0.563\0.568\0.583\0.572\0.577\0.567\0.562\0.559\0.567\0.572\0.575\0.57\0.563\0.555\0.551\0.551\0.557\0.547\0.54\0.547\0.545\0.549\0.549\0.549\0.543\0.537\0.532\0.538\0.538\0.533\0.529\0.528\0.529\0.531\0.53\0.526\0.525\0.524\0.525\0.522\0.523\0.521\0.523\0.523\0.523\0.519\0.52\0.522\0.524\0.524\0.522\0.522\0.522\0.524\0.524\0.523\0.521\0.524\0.524\0.527\0.525\0.528\0.527\0.527\0.526\0.526\0.525\0.522\0.523\0.523\0.524\0.523\0.524\0.523\0.523\0.524\0.525\0.525\0.524\0.523\0.522\0.523\0.521\0.523\0.521\0.52\0.521\0.519\0.518\0.518\0.515\0.517\0.515\0.515\0.514\0.516\0.515\0.516\0.516\0.515\0.514\0.512\0.51\0.51\0.51\0.511\0.511\0.51\0.508\0.507\0.506\0.508\0.508\0.508\0.508\0.507\0.507\0.507\0.507\0.507\0.506\0.506\0.506\0.506\0.506\0.506\0.505\0.506\0.506\0.505\0.505\0.505\0.505\0.505\0.505\0.505\0.505\0.505\0.505\0.505\0.505\0.504\0.505\0.505\0.505\0.504\0.502\0.502\0.501\0.499\0.497\0.492\0.488\0.482\0.483\0.479\0.489\0.485\0.478\0.473\0.467\0.461\0.478\0.461\0.448\0.452\0.453\0.459\0.458\0.456\0.45\0.445\0.452\0.462\0.466\0.452\0.445\0.441\0.451\0.453\0.454\0.456\0.457\0.458\0.461\0.462\0.458\0.463\0.458\0.459\0.46\0.455\0.45\0.448\0.45</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>241</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="DKXtOB" Title="Quasi-Newton method results">
   <Caption Id="pBgAhz">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.348
0.45
240
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Y3vWwt" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="nuZnVI" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="AJOKc1" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="p7NPq6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.082324 and its percentage error 8.2324</Caption>
   <Data>0.000352859\37.5726\5.73084\4.52258
4.90083e-6\0.521841\0.079595\0.0628136
0.000490083\52.1841\7.9595\6.28136</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="vn7mH2" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="SNYAUl" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="vNUu2V" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="VUsEBc" Title="Quasi-Newton method errors history">
   <Caption Id="AlWUJK">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.81921, and the final value after 252 epochs is 0.233365.
The initial value of the selection error is 1.47719, and the final value after 252 epochs is 0.215801.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252</X2Data>
   <Y1Data>1.82\0.759\0.67\0.622\0.584\0.465\0.446\0.428\0.407\0.399\0.387\0.378\0.369\0.363\0.356\0.348\0.344\0.337\0.333\0.326\0.323\0.312\0.305\0.3\0.298\0.292\0.287\0.284\0.279\0.277\0.274\0.27\0.268\0.265\0.263\0.26\0.258\0.256\0.255\0.253\0.252\0.251\0.251\0.25\0.249\0.249\0.249\0.248\0.248\0.247\0.247\0.247\0.246\0.246\0.246\0.246\0.246\0.245\0.245\0.245\0.245\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.242\0.242\0.242\0.242\0.242\0.242\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.237\0.238\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.236\0.236\0.236\0.236\0.236\0.236\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.233\0.233\0.233\0.233</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.48\0.476\0.452\0.485\0.376\0.331\0.314\0.295\0.324\0.343\0.355\0.352\0.341\0.34\0.321\0.307\0.293\0.296\0.295\0.306\0.303\0.314\0.302\0.291\0.285\0.283\0.273\0.267\0.273\0.266\0.267\0.267\0.271\0.272\0.276\0.273\0.269\0.26\0.258\0.251\0.245\0.248\0.249\0.241\0.239\0.238\0.23\0.231\0.228\0.231\0.234\0.235\0.237\0.233\0.236\0.233\0.231\0.229\0.228\0.228\0.229\0.228\0.229\0.229\0.227\0.226\0.228\0.229\0.229\0.228\0.228\0.228\0.228\0.229\0.227\0.228\0.229\0.229\0.229\0.229\0.23\0.23\0.228\0.23\0.228\0.227\0.226\0.225\0.225\0.225\0.225\0.224\0.224\0.223\0.223\0.224\0.225\0.225\0.225\0.224\0.224\0.225\0.226\0.227\0.227\0.226\0.225\0.226\0.226\0.227\0.228\0.227\0.228\0.228\0.227\0.228\0.228\0.228\0.23\0.23\0.229\0.229\0.229\0.23\0.23\0.23\0.231\0.23\0.231\0.231\0.23\0.23\0.23\0.23\0.23\0.23\0.23\0.229\0.228\0.227\0.227\0.226\0.226\0.225\0.225\0.224\0.223\0.224\0.223\0.223\0.223\0.223\0.223\0.222\0.223\0.223\0.223\0.222\0.222\0.223\0.222\0.223\0.223\0.223\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.222\0.221\0.221\0.221\0.221\0.221\0.221\0.22\0.22\0.22\0.22\0.22\0.22\0.22\0.22\0.22\0.22\0.22\0.22\0.22\0.22\0.219\0.219\0.22\0.22\0.221\0.221\0.221\0.219\0.219\0.219\0.219\0.22\0.219\0.218\0.218\0.218\0.218\0.218\0.219\0.218\0.218\0.218\0.217\0.216\0.216\0.215\0.215\0.215\0.215\0.215\0.215\0.215\0.214\0.214\0.214\0.214\0.215\0.215\0.215\0.215\0.215\0.215\0.216\0.215\0.215\0.216\0.216\0.216\0.216\0.216\0.216</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>253</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="XmWMd8" Title="Quasi-Newton method results">
   <Caption Id="pEyeBB">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.233
0.216
252
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="FwMQR2" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="fbuhA1" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="K53FkW" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="g0Zpgv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222382 and its percentage error 2.22381</Caption>
   <Data>0\14.9619\0.64666\0.992721
0\0.880113\0.0380388\0.0583954
0\88.0113\3.80388\5.83954</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="PWS0Uu" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="8mGFz6" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="PZoGg5" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="QTC7i0" Title="Quasi-Newton method errors history">
   <Caption Id="MwsvsE">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.18279, and the final value after 226 epochs is 0.244706.
The initial value of the selection error is 0.638493, and the final value after 226 epochs is 0.207887.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226</X2Data>
   <Y1Data>2.18\1.13\1.08\1.01\0.967\0.803\0.678\0.611\0.548\0.466\0.429\0.406\0.398\0.385\0.373\0.366\0.348\0.342\0.336\0.331\0.325\0.322\0.318\0.314\0.31\0.305\0.3\0.297\0.295\0.292\0.289\0.282\0.279\0.276\0.271\0.269\0.265\0.264\0.262\0.261\0.26\0.259\0.258\0.257\0.256\0.255\0.255\0.254\0.254\0.254\0.254\0.254\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.249\0.249\0.249\0.249\0.249\0.249\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.247\0.247\0.247\0.247\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.638\0.493\0.491\0.638\0.77\0.642\0.624\0.386\0.335\0.368\0.326\0.341\0.365\0.318\0.31\0.332\0.376\0.38\0.365\0.361\0.334\0.33\0.328\0.321\0.318\0.308\0.296\0.287\0.287\0.295\0.282\0.259\0.246\0.25\0.243\0.243\0.234\0.231\0.232\0.231\0.226\0.226\0.223\0.227\0.231\0.232\0.232\0.226\0.227\0.223\0.226\0.228\0.23\0.23\0.228\0.226\0.227\0.231\0.231\0.229\0.228\0.225\0.224\0.223\0.224\0.225\0.223\0.223\0.223\0.222\0.222\0.222\0.222\0.223\0.222\0.222\0.221\0.221\0.22\0.22\0.219\0.218\0.219\0.22\0.219\0.219\0.218\0.219\0.218\0.218\0.218\0.218\0.217\0.218\0.218\0.218\0.218\0.219\0.218\0.217\0.218\0.218\0.218\0.218\0.218\0.219\0.22\0.22\0.219\0.219\0.219\0.219\0.218\0.219\0.219\0.219\0.219\0.218\0.218\0.218\0.218\0.217\0.217\0.216\0.217\0.216\0.216\0.215\0.215\0.215\0.215\0.214\0.214\0.213\0.213\0.213\0.212\0.212\0.212\0.21\0.21\0.21\0.21\0.209\0.209\0.209\0.209\0.209\0.21\0.21\0.21\0.21\0.21\0.211\0.211\0.21\0.21\0.211\0.211\0.212\0.211\0.212\0.213\0.212\0.212\0.212\0.211\0.212\0.212\0.212\0.212\0.211\0.211\0.21\0.21\0.21\0.21\0.21\0.21\0.209\0.209\0.209\0.209\0.209\0.208\0.209\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>227</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="dBBJRp" Title="Quasi-Newton method results">
   <Caption Id="BfcytF">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.245
0.208
226
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="dSxvuj" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="QuV7Rx" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="B3XpKm" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="XeArN2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222382 and its percentage error 2.22381</Caption>
   <Data>0.00322676\13.9951\0.634051\0.955699
0.000189809\0.823243\0.0372971\0.0562176
0.0189809\82.3243\3.72971\5.62176</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Q4dY82" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="hBThK6" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="z4YWV8" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="SM5938" Title="Quasi-Newton method errors history">
   <Caption Id="nC37PQ">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.16494, and the final value after 189 epochs is 0.0982587.
The initial value of the selection error is 2.1054, and the final value after 189 epochs is 0.176801.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189</X2Data>
   <Y1Data>2.16\0.63\0.58\0.533\0.495\0.421\0.371\0.353\0.339\0.309\0.298\0.29\0.275\0.264\0.256\0.25\0.244\0.235\0.224\0.217\0.204\0.198\0.193\0.185\0.181\0.174\0.17\0.166\0.162\0.16\0.157\0.154\0.152\0.15\0.147\0.145\0.142\0.14\0.137\0.136\0.133\0.132\0.13\0.128\0.127\0.125\0.124\0.123\0.121\0.12\0.119\0.118\0.117\0.116\0.116\0.115\0.114\0.113\0.112\0.112\0.111\0.11\0.11\0.11\0.109\0.109\0.108\0.108\0.107\0.107\0.107\0.106\0.105\0.105\0.105\0.104\0.104\0.104\0.104\0.103\0.103\0.103\0.103\0.102\0.102\0.102\0.102\0.102\0.102\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.1\0.1\0.1\0.0997\0.0995\0.0995\0.0994\0.0992\0.099\0.0989\0.0989\0.0987\0.0987\0.0987\0.0986\0.0986\0.0986\0.0985\0.0985\0.0985\0.0984\0.0984\0.0984\0.0984\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0981\0.0981\0.0982\0.0982\0.0982\0.0981\0.0981\0.0981\0.0981\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0982\0.0983\0.0983\0.0983\0.0983\0.0983</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.11\0.63\0.577\0.542\0.537\0.465\0.446\0.433\0.438\0.423\0.391\0.373\0.373\0.351\0.327\0.312\0.317\0.314\0.294\0.287\0.287\0.29\0.27\0.261\0.26\0.256\0.251\0.244\0.248\0.242\0.241\0.23\0.228\0.225\0.224\0.229\0.231\0.238\0.227\0.219\0.218\0.211\0.208\0.198\0.192\0.199\0.199\0.198\0.19\0.19\0.188\0.186\0.188\0.191\0.195\0.195\0.194\0.194\0.194\0.195\0.196\0.193\0.188\0.185\0.189\0.191\0.193\0.192\0.196\0.205\0.202\0.199\0.199\0.194\0.194\0.195\0.196\0.194\0.201\0.199\0.197\0.196\0.193\0.19\0.189\0.192\0.19\0.19\0.189\0.19\0.193\0.19\0.187\0.186\0.186\0.187\0.19\0.19\0.189\0.186\0.185\0.183\0.181\0.18\0.177\0.176\0.176\0.176\0.177\0.177\0.177\0.178\0.18\0.178\0.177\0.176\0.177\0.177\0.177\0.176\0.178\0.177\0.177\0.177\0.177\0.178\0.177\0.178\0.177\0.177\0.178\0.178\0.178\0.178\0.178\0.177\0.177\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.177\0.178\0.178\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>190</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="rZJfw8" Title="Quasi-Newton method results">
   <Caption Id="8fWzbX">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0983
0.177
189
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ZdVMh2" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="JXOiBE" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="BfLQ7d" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="2cwC42">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.020668\122.557\6.39808\8.56034
0.000132487\0.785623\0.0410134\0.054874
0.0132487\78.5623\4.10134\5.4874</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="W2imA1" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="tiQLE1" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="PnWuFj" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="1e1W5z" Title="Quasi-Newton method errors history">
   <Caption Id="JIku1L">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.39343, and the final value after 149 epochs is 0.349571.
The initial value of the selection error is 1.0426, and the final value after 149 epochs is 0.283733.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149</X2Data>
   <Y1Data>1.39\0.71\0.66\0.621\0.594\0.556\0.533\0.514\0.494\0.485\0.472\0.462\0.452\0.449\0.444\0.436\0.433\0.429\0.424\0.417\0.413\0.408\0.406\0.403\0.398\0.395\0.392\0.388\0.385\0.383\0.38\0.378\0.375\0.373\0.371\0.369\0.367\0.366\0.364\0.363\0.362\0.361\0.36\0.359\0.358\0.358\0.357\0.357\0.357\0.357\0.356\0.356\0.356\0.356\0.355\0.355\0.355\0.354\0.354\0.354\0.354\0.354\0.353\0.353\0.353\0.353\0.353\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.04\0.437\0.463\0.416\0.402\0.366\0.346\0.349\0.341\0.345\0.351\0.34\0.315\0.313\0.312\0.308\0.307\0.317\0.31\0.308\0.312\0.324\0.313\0.304\0.309\0.307\0.308\0.311\0.303\0.301\0.292\0.293\0.29\0.291\0.283\0.287\0.286\0.279\0.279\0.279\0.284\0.286\0.282\0.279\0.277\0.28\0.279\0.283\0.279\0.278\0.279\0.278\0.277\0.283\0.282\0.279\0.281\0.281\0.283\0.284\0.285\0.284\0.281\0.282\0.281\0.282\0.282\0.283\0.283\0.281\0.281\0.279\0.28\0.279\0.281\0.28\0.28\0.28\0.28\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.282\0.282\0.281\0.283\0.283\0.284\0.284\0.283\0.283\0.284\0.284\0.284\0.284\0.284\0.284\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>150</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="nZEC63" Title="Quasi-Newton method results">
   <Caption Id="OzvRD8">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.35
0.284
149
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="gc56kW" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="U4rr59" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="cuIKLF" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="5uL5Zt">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228121 and its percentage error 2.28121</Caption>
   <Data>0.0204639\301.326\6.75764\13.894
6.01881e-5\0.886253\0.0198754\0.0408648
0.00601881\88.6253\1.98754\4.08648</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="gC6BfP" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="ipwhd9" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="qrV4qq" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="9d1AdJ" Title="Quasi-Newton method errors history">
   <Caption Id="hAof8F">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.48954, and the final value after 164 epochs is 0.437014.
The initial value of the selection error is 1.92893, and the final value after 164 epochs is 0.288221.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164</X2Data>
   <Y1Data>2.49\0.923\0.817\0.771\0.694\0.666\0.638\0.618\0.607\0.598\0.593\0.574\0.566\0.562\0.554\0.548\0.542\0.535\0.532\0.527\0.524\0.519\0.513\0.511\0.506\0.503\0.499\0.494\0.491\0.486\0.483\0.48\0.478\0.476\0.474\0.473\0.471\0.47\0.469\0.469\0.468\0.467\0.467\0.466\0.465\0.464\0.464\0.463\0.463\0.463\0.462\0.462\0.462\0.462\0.461\0.461\0.461\0.46\0.46\0.459\0.458\0.458\0.457\0.456\0.456\0.454\0.454\0.453\0.453\0.452\0.451\0.45\0.449\0.448\0.448\0.447\0.446\0.446\0.445\0.445\0.444\0.444\0.444\0.443\0.443\0.443\0.443\0.442\0.442\0.441\0.441\0.44\0.44\0.44\0.44\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.93\0.729\0.53\0.608\0.635\0.499\0.452\0.439\0.427\0.421\0.414\0.399\0.371\0.376\0.347\0.36\0.367\0.35\0.357\0.333\0.329\0.314\0.319\0.332\0.322\0.317\0.311\0.304\0.289\0.292\0.287\0.295\0.298\0.3\0.296\0.287\0.279\0.282\0.277\0.281\0.29\0.294\0.295\0.289\0.289\0.285\0.278\0.278\0.28\0.276\0.276\0.277\0.277\0.279\0.286\0.291\0.285\0.292\0.292\0.288\0.281\0.286\0.284\0.279\0.275\0.283\0.28\0.277\0.278\0.283\0.283\0.28\0.278\0.278\0.286\0.284\0.284\0.286\0.285\0.288\0.286\0.286\0.286\0.284\0.284\0.284\0.285\0.281\0.281\0.281\0.284\0.283\0.285\0.284\0.283\0.283\0.283\0.283\0.284\0.285\0.285\0.287\0.287\0.286\0.287\0.286\0.287\0.287\0.288\0.287\0.288\0.288\0.288\0.289\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>165</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="Bwd9xL" Title="Quasi-Newton method results">
   <Caption Id="BSiojH">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.437
0.288
164
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="t7b7bk" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="VUBZ6z" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="7Kh7UX" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="GnTuJL">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0796335 and its percentage error 7.96336</Caption>
   <Data>0.0157166\30.0712\4.93719\4.11162
6.33732e-5\0.121255\0.019908\0.0165791
0.00633732\12.1255\1.9908\1.65791</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="xGYFnO" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="UFKsOC" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="ThltgU" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="IBHb9X" Title="Quasi-Newton method errors history">
   <Caption Id="nq1AlV">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.88699, and the final value after 172 epochs is 0.393324.
The initial value of the selection error is 1.69064, and the final value after 172 epochs is 0.416714.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172</X2Data>
   <Y1Data>2.89\0.864\0.803\0.78\0.723\0.675\0.627\0.615\0.605\0.594\0.581\0.566\0.561\0.555\0.539\0.533\0.529\0.524\0.515\0.507\0.501\0.498\0.492\0.49\0.486\0.482\0.478\0.474\0.472\0.468\0.465\0.462\0.46\0.458\0.457\0.455\0.454\0.453\0.452\0.451\0.449\0.449\0.447\0.447\0.446\0.444\0.443\0.443\0.442\0.442\0.441\0.441\0.44\0.439\0.439\0.438\0.438\0.437\0.437\0.437\0.437\0.436\0.436\0.436\0.436\0.436\0.435\0.435\0.435\0.435\0.434\0.434\0.434\0.434\0.433\0.433\0.433\0.432\0.431\0.43\0.429\0.428\0.427\0.424\0.42\0.419\0.417\0.415\0.411\0.409\0.408\0.406\0.405\0.404\0.403\0.403\0.402\0.402\0.401\0.401\0.4\0.399\0.399\0.399\0.398\0.398\0.398\0.397\0.397\0.397\0.397\0.396\0.396\0.396\0.396\0.396\0.395\0.395\0.395\0.395\0.395\0.395\0.395\0.395\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.69\1.01\0.985\1.11\0.838\0.755\0.808\0.693\0.695\0.747\0.688\0.624\0.616\0.591\0.529\0.533\0.532\0.502\0.56\0.632\0.605\0.582\0.56\0.601\0.604\0.595\0.592\0.601\0.619\0.615\0.635\0.611\0.605\0.569\0.576\0.586\0.588\0.58\0.586\0.561\0.539\0.523\0.52\0.518\0.513\0.526\0.511\0.507\0.502\0.493\0.49\0.497\0.5\0.505\0.497\0.499\0.494\0.498\0.5\0.505\0.503\0.498\0.496\0.501\0.505\0.502\0.504\0.5\0.496\0.499\0.5\0.503\0.501\0.498\0.497\0.496\0.497\0.497\0.493\0.479\0.48\0.487\0.483\0.5\0.507\0.489\0.482\0.464\0.457\0.452\0.449\0.451\0.441\0.441\0.435\0.433\0.436\0.43\0.423\0.417\0.409\0.413\0.418\0.418\0.413\0.411\0.409\0.409\0.407\0.407\0.408\0.408\0.408\0.406\0.407\0.404\0.403\0.403\0.404\0.403\0.406\0.406\0.406\0.407\0.406\0.405\0.404\0.407\0.407\0.408\0.408\0.408\0.409\0.409\0.409\0.41\0.41\0.41\0.411\0.413\0.412\0.413\0.413\0.413\0.413\0.413\0.414\0.414\0.415\0.415\0.415\0.415\0.415\0.415\0.415\0.415\0.416\0.416\0.416\0.416\0.416\0.416\0.416\0.416\0.416\0.416\0.416\0.416\0.416\0.417\0.417\0.417\0.417</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>173</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="od88VB" Title="Quasi-Newton method results">
   <Caption Id="vFBWDf">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.393
0.417
172
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="D2flaY" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="zAsZSk" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="J2tZMq" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QGN1zi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.082324 and its percentage error 8.2324</Caption>
   <Data>0.0473881\44.9537\5.49978\4.44091
0.000658168\0.624357\0.0763859\0.0616793
0.0658168\62.4357\7.63859\6.16793</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="KuIeWs" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="FpUyw9" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="JDPbbr" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="a9216B" Title="Quasi-Newton method errors history">
   <Caption Id="9e9Ddv">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.6652, and the final value after 112 epochs is 0.368372.
The initial value of the selection error is 1.99255, and the final value after 112 epochs is 0.364378.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112</X2Data>
   <Y1Data>1.67\0.802\0.697\0.666\0.644\0.62\0.605\0.585\0.573\0.565\0.552\0.543\0.535\0.53\0.525\0.518\0.512\0.508\0.501\0.496\0.491\0.485\0.479\0.472\0.467\0.462\0.458\0.454\0.45\0.444\0.438\0.434\0.431\0.427\0.421\0.418\0.416\0.413\0.411\0.408\0.405\0.402\0.4\0.398\0.395\0.392\0.39\0.387\0.386\0.384\0.383\0.382\0.38\0.378\0.377\0.376\0.375\0.375\0.374\0.373\0.372\0.372\0.371\0.371\0.37\0.37\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.99\0.724\0.615\0.597\0.582\0.598\0.579\0.557\0.546\0.526\0.501\0.505\0.515\0.502\0.491\0.476\0.473\0.471\0.468\0.469\0.465\0.451\0.443\0.44\0.449\0.448\0.435\0.424\0.416\0.412\0.417\0.416\0.415\0.411\0.398\0.395\0.391\0.389\0.387\0.389\0.381\0.382\0.383\0.382\0.385\0.379\0.376\0.373\0.373\0.371\0.372\0.375\0.374\0.372\0.37\0.37\0.368\0.366\0.364\0.365\0.366\0.368\0.369\0.367\0.366\0.364\0.364\0.364\0.365\0.366\0.366\0.366\0.366\0.366\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>113</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="mOW1Ba" Title="Quasi-Newton method results">
   <Caption Id="hrhKfI">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.368
0.364
112
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="kFlUt2" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="Ldwirp" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="M9KBtj" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="IUSVRm">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0254745\102.5\8.21369\9.06266
0.000163298\0.657054\0.0526519\0.058094
0.0163298\65.7054\5.26519\5.8094</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="q243Uf" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="U8WWrD">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00471115\308.271\6.70335\14.2056
1.38563e-5\0.906678\0.0197157\0.0417811
0.00138563\90.6678\1.97157\4.17811</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="xd14f5" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="32hoCr">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00498199\33.3742\5.44569\4.73862
2.00887e-5\0.134573\0.0219584\0.0191073
0.00200887\13.4573\2.19584\1.91073</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="YzsHCq" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="StHJxg">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.038765\39.2859\7.72138\5.10109
0.000538402\0.545637\0.107241\0.0708485
0.0538402\54.5637\10.7241\7.08485</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uZsFrC" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Jy5sXw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0107267\14.4598\0.777147\0.99279
0.000630982\0.850577\0.0457145\0.0583994
0.0630982\85.0577\4.57145\5.83994</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="bTyybD" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="uXoBB9" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="pQd0H1" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="NNahd6" Title="Quasi-Newton method errors history">
   <Caption Id="Dld6HY">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.06112, and the final value after 121 epochs is 0.381115.
The initial value of the selection error is 1.63866, and the final value after 121 epochs is 0.367745.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121</X2Data>
   <Y1Data>2.06\1.07\0.897\0.846\0.812\0.761\0.727\0.701\0.674\0.646\0.632\0.616\0.601\0.587\0.573\0.562\0.55\0.543\0.524\0.513\0.502\0.492\0.487\0.481\0.476\0.47\0.465\0.46\0.457\0.453\0.446\0.442\0.435\0.431\0.426\0.421\0.417\0.414\0.411\0.408\0.406\0.404\0.402\0.4\0.398\0.396\0.394\0.392\0.391\0.39\0.389\0.389\0.388\0.387\0.387\0.386\0.385\0.385\0.385\0.384\0.384\0.384\0.383\0.383\0.382\0.382\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.64\0.951\0.812\0.841\0.826\0.722\0.667\0.677\0.652\0.63\0.588\0.561\0.541\0.53\0.539\0.521\0.505\0.501\0.484\0.467\0.462\0.461\0.458\0.453\0.456\0.442\0.44\0.436\0.429\0.423\0.419\0.409\0.405\0.405\0.399\0.397\0.397\0.392\0.387\0.386\0.385\0.385\0.387\0.384\0.382\0.377\0.377\0.371\0.369\0.369\0.369\0.371\0.371\0.37\0.368\0.367\0.367\0.368\0.369\0.37\0.369\0.368\0.368\0.369\0.369\0.369\0.368\0.367\0.367\0.366\0.366\0.366\0.366\0.366\0.367\0.367\0.368\0.368\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.367\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>122</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="sMU8uO" Title="Quasi-Newton method results">
   <Caption Id="ue7YEM">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.381
0.368
121
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="rFaUOt" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="uzMsFh" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="9mmhxO" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="1z98TH">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0229492\113.647\8.03078\9.12927
0.00014711\0.728506\0.0514794\0.058521
0.014711\72.8506\5.14794\5.8521</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="JZWEdS" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="NOvhD6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0431995\308.734\6.85915\14.3007
0.000127057\0.90804\0.020174\0.0420608
0.0127057\90.804\2.0174\4.20608</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="hwOVLL" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="V4KSQL">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0100479\26.1314\5.49537\4.6814
4.05158e-5\0.105369\0.0221588\0.0188766
0.00405158\10.5369\2.21588\1.88766</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="31FJDa" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="GtBgNN">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000806808\42.2758\7.75685\5.14445
1.12057e-5\0.587165\0.107734\0.0714507
0.00112057\58.7165\10.7734\7.14507</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="qJcTWu" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="37z6ls">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00141215\14.6103\0.736724\0.985295
8.30678e-5\0.85943\0.0433367\0.0579585
0.00830678\85.943\4.33367\5.79585</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="jZoZWU" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="gdICc0" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="qVHGpm" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="B6ROE2" Title="Quasi-Newton method errors history">
   <Caption Id="z38Lzu">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.85953, and the final value after 188 epochs is 0.0986105.
The initial value of the selection error is 2.66322, and the final value after 188 epochs is 0.177148.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188</X2Data>
   <Y1Data>1.86\0.84\0.77\0.497\0.48\0.458\0.408\0.367\0.322\0.307\0.3\0.289\0.282\0.276\0.267\0.252\0.244\0.234\0.224\0.207\0.191\0.184\0.175\0.171\0.167\0.163\0.16\0.156\0.151\0.147\0.144\0.14\0.137\0.134\0.131\0.127\0.124\0.123\0.121\0.119\0.118\0.116\0.115\0.114\0.113\0.112\0.111\0.111\0.11\0.11\0.11\0.11\0.109\0.109\0.109\0.109\0.109\0.109\0.108\0.108\0.108\0.108\0.108\0.108\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.109\0.108\0.108\0.108\0.108\0.108\0.108\0.107\0.107\0.106\0.106\0.106\0.106\0.106\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.104\0.104\0.104\0.104\0.104\0.104\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.102\0.102\0.102\0.102\0.102\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.1\0.1\0.0999\0.0998\0.0996\0.0995\0.0993\0.0992\0.0991\0.099\0.0989\0.0989\0.0989\0.0987\0.0988\0.0987\0.0987\0.0986\0.0986\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0986\0.0985\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.66\0.94\0.837\0.519\0.495\0.503\0.435\0.428\0.412\0.407\0.397\0.389\0.369\0.364\0.35\0.314\0.319\0.3\0.285\0.277\0.271\0.264\0.244\0.235\0.244\0.235\0.229\0.226\0.222\0.214\0.21\0.212\0.206\0.2\0.205\0.208\0.21\0.216\0.21\0.204\0.201\0.2\0.194\0.198\0.197\0.196\0.19\0.187\0.185\0.183\0.185\0.184\0.182\0.185\0.187\0.186\0.183\0.183\0.182\0.182\0.182\0.182\0.183\0.183\0.185\0.187\0.192\0.19\0.188\0.191\0.191\0.19\0.19\0.19\0.192\0.194\0.19\0.19\0.191\0.192\0.192\0.189\0.187\0.188\0.186\0.188\0.188\0.187\0.186\0.183\0.18\0.181\0.185\0.187\0.188\0.189\0.188\0.188\0.188\0.186\0.185\0.184\0.182\0.182\0.182\0.182\0.182\0.181\0.181\0.179\0.177\0.177\0.177\0.18\0.18\0.18\0.18\0.18\0.182\0.183\0.183\0.184\0.182\0.183\0.18\0.178\0.18\0.181\0.181\0.182\0.182\0.184\0.185\0.184\0.184\0.183\0.182\0.181\0.181\0.18\0.179\0.178\0.177\0.176\0.176\0.176\0.174\0.176\0.176\0.175\0.176\0.177\0.177\0.177\0.176\0.176\0.177\0.177\0.177\0.178\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.178\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>189</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="THjLjd" Title="Quasi-Newton method results">
   <Caption Id="SUiY5T">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0986
0.177
188
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ccts88" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="41XV35" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="VHYbvD" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="diP24K">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0206833\122.616\6.4259\8.57269
0.000132585\0.786003\0.0411917\0.0549532
0.0132585\78.6003\4.11917\5.49531</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="TM271Z" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="KXftES" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="e87Ypz" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="aKe1b8" Title="Quasi-Newton method errors history">
   <Caption Id="kk7bis">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.81436, and the final value after 132 epochs is 0.0986697.
The initial value of the selection error is 2.04628, and the final value after 132 epochs is 0.170692.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132</X2Data>
   <Y1Data>1.81\1.06\0.669\0.567\0.471\0.458\0.417\0.386\0.353\0.328\0.311\0.305\0.299\0.284\0.271\0.263\0.259\0.251\0.236\0.225\0.216\0.208\0.195\0.187\0.176\0.168\0.161\0.154\0.149\0.144\0.143\0.14\0.137\0.133\0.131\0.127\0.126\0.123\0.121\0.119\0.116\0.115\0.114\0.112\0.111\0.11\0.109\0.109\0.108\0.107\0.107\0.106\0.105\0.105\0.104\0.104\0.103\0.103\0.103\0.102\0.102\0.101\0.101\0.101\0.101\0.1\0.1\0.0999\0.0998\0.0996\0.0995\0.0993\0.0992\0.0991\0.099\0.0989\0.0988\0.0987\0.0986\0.0986\0.0985\0.0984\0.0984\0.0983\0.0983\0.0982\0.0982\0.0982\0.0981\0.0981\0.0981\0.0981\0.0981\0.0981\0.0981\0.0982\0.0982\0.0982\0.0983\0.0983\0.0984\0.0984\0.0985\0.0985\0.0985\0.0985\0.0985\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0986\0.0987\0.0987\0.0987\0.0987\0.0987</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.05\1.39\0.825\0.717\0.493\0.491\0.433\0.485\0.424\0.446\0.454\0.433\0.432\0.434\0.399\0.354\0.355\0.343\0.349\0.35\0.353\0.343\0.316\0.285\0.295\0.267\0.249\0.229\0.228\0.213\0.211\0.217\0.209\0.209\0.209\0.201\0.199\0.194\0.202\0.202\0.21\0.201\0.202\0.199\0.195\0.193\0.198\0.2\0.194\0.189\0.181\0.184\0.19\0.196\0.198\0.196\0.194\0.191\0.189\0.188\0.186\0.182\0.179\0.177\0.176\0.172\0.172\0.176\0.176\0.177\0.177\0.176\0.175\0.175\0.173\0.173\0.173\0.174\0.173\0.172\0.171\0.172\0.172\0.17\0.17\0.171\0.171\0.17\0.17\0.17\0.169\0.169\0.168\0.169\0.168\0.168\0.169\0.169\0.169\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.171\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>133</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="QWXyPW" Title="Quasi-Newton method results">
   <Caption Id="3CG9P2">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0987
0.171
132
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="mUHEB4" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="1l0Qvi" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="3qgtXW" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="EKL9cw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00352478\121.082\6.27337\8.37983
2.25947e-5\0.776169\0.0402139\0.0537168
0.00225947\77.6169\4.02139\5.37168</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Dvfgnh" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="gMjVKV" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="qTbc09" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="FmnzyV" Title="Quasi-Newton method errors history">
   <Caption Id="GD1lrK">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 3.07083, and the final value after 179 epochs is 0.0983385.
The initial value of the selection error is 2.94699, and the final value after 179 epochs is 0.176488.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179</X2Data>
   <Y1Data>3.07\0.737\0.587\0.466\0.415\0.385\0.356\0.335\0.311\0.296\0.287\0.273\0.263\0.252\0.236\0.23\0.222\0.215\0.211\0.204\0.199\0.192\0.185\0.176\0.17\0.167\0.162\0.159\0.157\0.156\0.154\0.151\0.147\0.144\0.139\0.137\0.133\0.132\0.13\0.127\0.126\0.124\0.123\0.121\0.12\0.118\0.117\0.116\0.115\0.114\0.113\0.112\0.111\0.11\0.11\0.109\0.108\0.108\0.107\0.107\0.107\0.106\0.106\0.106\0.105\0.105\0.105\0.105\0.104\0.104\0.104\0.104\0.104\0.104\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.102\0.102\0.102\0.102\0.102\0.101\0.101\0.101\0.101\0.101\0.1\0.1\0.1\0.1\0.0998\0.0997\0.0995\0.0993\0.0991\0.0991\0.099\0.0989\0.0988\0.0987\0.0986\0.0986\0.0985\0.0985\0.0984\0.0984\0.0984\0.0984\0.0984\0.0983\0.0983\0.0983\0.0984\0.0984\0.0984\0.0984\0.0984\0.0984\0.0984\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0986\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0985\0.0984\0.0984\0.0984\0.0984\0.0984\0.0984\0.0984\0.0984\0.0984\0.0984\0.0984\0.0983\0.0983\0.0984\0.0983\0.0983\0.0984\0.0984\0.0984\0.0984\0.0984\0.0984\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983\0.0983</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.95\0.891\0.661\0.537\0.475\0.43\0.417\0.433\0.392\0.387\0.374\0.36\0.341\0.362\0.332\0.317\0.295\0.279\0.264\0.257\0.251\0.258\0.291\0.264\0.243\0.236\0.231\0.224\0.224\0.217\0.208\0.217\0.219\0.229\0.225\0.219\0.217\0.22\0.234\0.241\0.236\0.229\0.222\0.211\0.208\0.2\0.202\0.204\0.199\0.193\0.19\0.186\0.182\0.184\0.185\0.184\0.186\0.191\0.196\0.194\0.196\0.192\0.192\0.187\0.186\0.189\0.189\0.193\0.192\0.189\0.188\0.189\0.19\0.192\0.188\0.189\0.189\0.191\0.19\0.188\0.186\0.184\0.183\0.183\0.183\0.183\0.184\0.186\0.185\0.185\0.184\0.182\0.181\0.18\0.182\0.181\0.18\0.179\0.178\0.177\0.178\0.179\0.179\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.178\0.178\0.177\0.178\0.176\0.177\0.176\0.176\0.177\0.177\0.177\0.176\0.175\0.175\0.174\0.175\0.175\0.175\0.174\0.175\0.174\0.174\0.175\0.175\0.175\0.174\0.174\0.174\0.174\0.175\0.175\0.175\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.177\0.176\0.176\0.176</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>180</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>4</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="mcdIS9" Title="Quasi-Newton method results">
   <Caption Id="jz9PaD">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0983
0.176
179
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="QqAbDW" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="UUyMmL" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="8wKA8q" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Lwljd3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00936508\122.499\6.39357\8.55535
6.00326e-5\0.785252\0.0409844\0.054842
0.00600326\78.5252\4.09844\5.4842</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="aD2yGx" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="RkRIey" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="h2RSk9" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="TYCpBf" Title="Quasi-Newton method errors history">
   <Caption Id="rNGPrK">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.30359, and the final value after 197 epochs is 0.0830159.
The initial value of the selection error is 1.24434, and the final value after 197 epochs is 0.151085.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197</X2Data>
   <Y1Data>1.3\0.715\0.57\0.423\0.391\0.363\0.347\0.319\0.3\0.288\0.276\0.264\0.244\0.235\0.227\0.221\0.212\0.205\0.198\0.192\0.18\0.176\0.171\0.164\0.159\0.155\0.148\0.146\0.142\0.139\0.135\0.133\0.13\0.127\0.125\0.122\0.121\0.119\0.118\0.116\0.114\0.112\0.111\0.11\0.109\0.108\0.107\0.105\0.105\0.104\0.103\0.102\0.101\0.1\0.0992\0.0984\0.0974\0.0963\0.0953\0.0944\0.0937\0.0932\0.0927\0.0919\0.0914\0.0909\0.0904\0.09\0.0896\0.0893\0.089\0.0887\0.0884\0.0881\0.0879\0.0877\0.0874\0.0872\0.0869\0.0868\0.0866\0.0864\0.0863\0.0861\0.086\0.0858\0.0857\0.0856\0.0854\0.0853\0.0852\0.0852\0.085\0.0848\0.0847\0.0846\0.0845\0.0845\0.0844\0.0843\0.0842\0.0842\0.0842\0.0841\0.0841\0.0841\0.084\0.084\0.0841\0.0841\0.0841\0.084\0.084\0.0839\0.0839\0.0839\0.0839\0.0839\0.0838\0.0838\0.0838\0.0838\0.0838\0.0838\0.0838\0.0837\0.0837\0.0837\0.0837\0.0837\0.0837\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0835\0.0835\0.0835\0.0835\0.0834\0.0834\0.0835\0.0834\0.0834\0.0834\0.0834\0.0833\0.0833\0.0832\0.0832\0.0832\0.0832\0.0831\0.0832\0.0831\0.0831\0.0831\0.0831\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.24\0.682\0.528\0.451\0.411\0.407\0.384\0.373\0.363\0.368\0.36\0.338\0.317\0.307\0.302\0.305\0.303\0.307\0.319\0.312\0.279\0.282\0.265\0.255\0.249\0.249\0.243\0.23\0.218\0.214\0.21\0.21\0.202\0.193\0.175\0.173\0.169\0.164\0.164\0.162\0.16\0.16\0.167\0.17\0.174\0.18\0.182\0.184\0.185\0.189\0.181\0.178\0.175\0.174\0.178\0.173\0.169\0.169\0.167\0.168\0.167\0.166\0.163\0.16\0.164\0.163\0.16\0.157\0.156\0.157\0.159\0.158\0.16\0.159\0.158\0.159\0.158\0.158\0.158\0.159\0.159\0.158\0.159\0.157\0.157\0.157\0.153\0.153\0.155\0.155\0.156\0.157\0.156\0.155\0.154\0.154\0.154\0.153\0.153\0.153\0.153\0.154\0.153\0.153\0.153\0.153\0.153\0.153\0.152\0.152\0.152\0.153\0.153\0.152\0.153\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.15\0.151\0.151\0.15\0.151\0.151\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.15\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>198</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="R27jPY" Title="Quasi-Newton method results">
   <Caption Id="j6ELX5">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.083
0.151
197
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="uLHcPS" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="8SnqLE" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="6j0mSg" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="74tRgT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00233459\93.8088\5.94628\7.40285
1.49654e-5\0.601339\0.0381172\0.0474542
0.00149654\60.1339\3.81172\4.74542</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="7owZzu" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="VXE9f4" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="xBRawu" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="FfECCA" Title="Quasi-Newton method errors history">
   <Caption Id="1kDO7p">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.3715, and the final value after 235 epochs is 0.08348.
The initial value of the selection error is 0.937191, and the final value after 235 epochs is 0.16478.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235</X2Data>
   <Y1Data>1.37\0.624\0.579\0.524\0.497\0.449\0.375\0.342\0.336\0.315\0.305\0.295\0.286\0.265\0.246\0.236\0.23\0.222\0.22\0.216\0.207\0.198\0.188\0.18\0.174\0.167\0.16\0.157\0.154\0.151\0.15\0.148\0.144\0.141\0.14\0.136\0.133\0.131\0.129\0.126\0.124\0.122\0.119\0.118\0.117\0.116\0.115\0.113\0.112\0.111\0.11\0.109\0.108\0.107\0.106\0.105\0.103\0.102\0.102\0.101\0.1\0.0999\0.0993\0.0989\0.0986\0.098\0.0972\0.0967\0.0964\0.0959\0.0957\0.0954\0.095\0.0948\0.0944\0.0941\0.0939\0.0935\0.0932\0.0928\0.0925\0.0921\0.0917\0.0914\0.0911\0.0908\0.0906\0.0904\0.0903\0.09\0.0899\0.0897\0.0896\0.0894\0.0893\0.0891\0.089\0.0889\0.0887\0.0885\0.0884\0.0882\0.088\0.0878\0.0876\0.0874\0.0872\0.0871\0.087\0.0869\0.0868\0.0867\0.0866\0.0865\0.0864\0.0865\0.0864\0.0864\0.0863\0.0863\0.0862\0.0862\0.0862\0.0862\0.0862\0.0862\0.0861\0.0861\0.0861\0.0861\0.0861\0.0861\0.086\0.086\0.0859\0.0858\0.0858\0.0856\0.0856\0.0855\0.0855\0.0855\0.0855\0.0855\0.0855\0.0854\0.0855\0.0855\0.0855\0.0855\0.0855\0.0855\0.0855\0.0854\0.0855\0.0855\0.0855\0.0855\0.0854\0.0855\0.0854\0.0854\0.0854\0.0854\0.0854\0.0854\0.0854\0.0854\0.0853\0.0853\0.0852\0.0851\0.085\0.085\0.0849\0.0849\0.0847\0.0847\0.0846\0.0846\0.0844\0.0844\0.0842\0.0841\0.084\0.0839\0.0839\0.0837\0.0838\0.0837\0.0837\0.0837\0.0836\0.0837\0.0835\0.0836\0.0835\0.0836\0.0835\0.0836\0.0834\0.0834\0.0834\0.0834\0.0833\0.0833\0.0834\0.0834\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0834\0.0835\0.0834\0.0834\0.0834\0.0834\0.0834\0.0834\0.0834\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.937\0.579\0.578\0.592\0.556\0.49\0.465\0.413\0.423\0.381\0.384\0.402\0.409\0.359\0.345\0.325\0.326\0.321\0.319\0.32\0.315\0.289\0.274\0.254\0.244\0.235\0.244\0.251\0.25\0.239\0.238\0.23\0.228\0.221\0.225\0.22\0.205\0.2\0.202\0.204\0.206\0.21\0.205\0.203\0.205\0.21\0.209\0.21\0.207\0.209\0.202\0.203\0.205\0.2\0.193\0.189\0.186\0.183\0.188\0.19\0.184\0.181\0.181\0.181\0.181\0.182\0.184\0.187\0.185\0.177\0.174\0.178\0.176\0.175\0.18\0.177\0.177\0.175\0.177\0.183\0.184\0.187\0.187\0.184\0.183\0.181\0.184\0.184\0.185\0.183\0.182\0.179\0.178\0.179\0.179\0.18\0.182\0.18\0.179\0.18\0.181\0.18\0.182\0.18\0.18\0.18\0.181\0.182\0.182\0.181\0.183\0.183\0.181\0.182\0.182\0.182\0.183\0.184\0.184\0.183\0.184\0.183\0.182\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.18\0.18\0.178\0.178\0.178\0.179\0.18\0.18\0.177\0.179\0.179\0.18\0.181\0.18\0.18\0.181\0.181\0.181\0.181\0.181\0.182\0.181\0.181\0.18\0.18\0.18\0.18\0.18\0.18\0.179\0.179\0.179\0.179\0.179\0.179\0.179\0.179\0.179\0.178\0.178\0.178\0.177\0.176\0.176\0.176\0.176\0.175\0.174\0.173\0.174\0.172\0.172\0.172\0.171\0.171\0.17\0.17\0.169\0.17\0.169\0.168\0.168\0.168\0.167\0.166\0.166\0.165\0.166\0.166\0.167\0.167\0.166\0.166\0.167\0.166\0.165\0.164\0.165\0.165\0.165\0.165\0.165\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.165\0.165\0.165\0.165\0.165\0.165\0.164\0.165\0.165</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>236</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="Ax6CUF" Title="Quasi-Newton method results">
   <Caption Id="jgfJzM">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0835
0.165
235
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Af4AMF" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="hJ1VHm" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="V4NEy5" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="mZLtrn">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0274124\115.674\6.0086\8.12199
0.000175721\0.741497\0.0385167\0.052064
0.0175721\74.1497\3.85167\5.20641</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="UZ9pK3" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="h0jVdX" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="hG3AMO" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="S2j1Yf" Title="Quasi-Newton method errors history">
   <Caption Id="BeHqa9">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.08348, and the final value after 116 epochs is 0.0823276.
The initial value of the selection error is 0.164781, and the final value after 116 epochs is 0.169976.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116</X2Data>
   <Y1Data>0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0834\0.0834\0.0834\0.0834\0.0834\0.0834\0.0834\0.0834\0.0833\0.0833\0.0833\0.0833\0.0833\0.0832\0.0832\0.0832\0.0832\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.083\0.0831\0.083\0.0831\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.083\0.0829\0.0829\0.0828\0.0828\0.0828\0.0828\0.0827\0.0827\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0823\0.0824\0.0823\0.0824\0.0823\0.0823\0.0823\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0824\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.164\0.164\0.164\0.164\0.164\0.165\0.165\0.165\0.165\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.166\0.166\0.165\0.166\0.166\0.167\0.167\0.168\0.168\0.168\0.168\0.169\0.168\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.168\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>117</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="bzTmTM" Title="Quasi-Newton method results">
   <Caption Id="IHLEjS">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0823
0.17
116
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="kLFM7D" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="BRlDi5" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="YbgN1k" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ZdJROq">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0135231\115.36\5.96616\8.16227
8.66866e-5\0.739486\0.0382446\0.0523222
0.00866865\73.9486\3.82446\5.23222</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="HCVyfU" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="XCt2sg" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="4lJ7NG" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="OAXK6u" Title="Quasi-Newton method errors history">
   <Caption Id="xjQJyu">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.92242, and the final value after 216 epochs is 0.366347.
The initial value of the selection error is 1.73512, and the final value after 216 epochs is 0.361381.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216</X2Data>
   <Y1Data>1.92\0.856\0.803\0.762\0.741\0.721\0.705\0.697\0.69\0.68\0.668\0.646\0.634\0.622\0.614\0.606\0.601\0.595\0.59\0.584\0.577\0.575\0.57\0.565\0.559\0.555\0.549\0.546\0.54\0.536\0.531\0.526\0.52\0.517\0.512\0.508\0.502\0.499\0.494\0.488\0.483\0.478\0.472\0.468\0.464\0.457\0.452\0.448\0.445\0.44\0.435\0.43\0.427\0.424\0.421\0.418\0.415\0.411\0.408\0.405\0.403\0.402\0.4\0.397\0.395\0.392\0.39\0.389\0.387\0.386\0.384\0.383\0.381\0.38\0.379\0.378\0.376\0.376\0.375\0.374\0.373\0.372\0.372\0.371\0.371\0.371\0.37\0.37\0.37\0.369\0.369\0.369\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.74\0.748\0.751\0.708\0.653\0.665\0.674\0.642\0.619\0.602\0.607\0.612\0.598\0.587\0.57\0.566\0.564\0.572\0.57\0.557\0.543\0.545\0.551\0.56\0.55\0.558\0.552\0.559\0.545\0.542\0.537\0.535\0.538\0.545\0.53\0.524\0.517\0.516\0.508\0.504\0.493\0.488\0.494\0.502\0.494\0.47\0.453\0.443\0.446\0.443\0.443\0.441\0.435\0.429\0.42\0.417\0.418\0.426\0.426\0.418\0.411\0.401\0.394\0.391\0.395\0.395\0.398\0.397\0.393\0.386\0.381\0.379\0.378\0.376\0.379\0.379\0.378\0.377\0.376\0.375\0.376\0.376\0.377\0.376\0.376\0.373\0.372\0.372\0.371\0.37\0.372\0.372\0.371\0.371\0.37\0.369\0.37\0.37\0.37\0.37\0.37\0.369\0.369\0.369\0.369\0.369\0.368\0.369\0.369\0.368\0.368\0.369\0.369\0.369\0.369\0.369\0.369\0.368\0.368\0.369\0.369\0.369\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.367\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.368\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.366\0.366\0.366\0.366\0.366\0.365\0.364\0.364\0.364\0.364\0.363\0.363\0.361\0.361\0.36\0.361\0.361\0.361\0.361\0.36\0.361\0.361\0.361\0.36\0.36\0.36\0.36\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.362\0.361\0.361\0.361\0.362\0.362\0.361\0.361\0.361\0.361\0.362\0.362\0.362\0.362\0.362\0.361\0.361\0.361\0.361\0.361\0.361</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>217</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="x8SE6s" Title="Quasi-Newton method results">
   <Caption Id="2eEuf8">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.366
0.361
216
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="aLE6LU" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="EuFaYx" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="UTsR5n" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="2gEwai">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.0214233\113.733\8.3325\9.23227
0.000137329\0.729058\0.0534134\0.0591812
0.0137329\72.9058\5.34134\5.91812</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="bt0Bza" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="MZabNi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.0215111\309.403\6.85652\14.3023
6.32679e-5\0.910009\0.0201662\0.0420655
0.00632679\91.0009\2.01662\4.20655</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="z6MSPE" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="s7KcmF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.00432587\27.1799\5.96253\4.84336
1.7443e-5\0.109597\0.0240425\0.0195297
0.0017443\10.9597\2.40425\1.95297</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="44wyYV" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="tE1IrG">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.00754547\43.3232\7.46398\4.97485
0.000104798\0.601711\0.103666\0.0690951
0.0104798\60.1711\10.3666\6.90951</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="rPT3vN" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="3p9AYW">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.00604773\14.844\0.666846\0.979131
0.000355749\0.873176\0.0392262\0.057596
0.0355749\87.3176\3.92262\5.75959</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="7AJgk7" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="1FfSNY" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="x9lSbn" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="pKanwd" Title="Quasi-Newton method errors history">
   <Caption Id="JO6oTF">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 3.87157, and the final value after 246 epochs is 0.07303.
The initial value of the selection error is 2.90682, and the final value after 246 epochs is 0.156094.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246</X2Data>
   <Y1Data>3.87\0.91\0.828\0.73\0.639\0.576\0.544\0.516\0.47\0.428\0.404\0.393\0.381\0.366\0.358\0.35\0.343\0.333\0.324\0.311\0.304\0.296\0.291\0.286\0.278\0.273\0.27\0.267\0.261\0.256\0.253\0.244\0.232\0.228\0.225\0.221\0.217\0.212\0.208\0.205\0.199\0.196\0.192\0.189\0.186\0.182\0.177\0.174\0.172\0.169\0.164\0.16\0.158\0.155\0.152\0.149\0.145\0.142\0.14\0.137\0.135\0.133\0.131\0.129\0.127\0.126\0.123\0.122\0.12\0.119\0.117\0.115\0.113\0.112\0.11\0.109\0.107\0.106\0.105\0.104\0.103\0.102\0.101\0.1\0.0993\0.0984\0.0975\0.0967\0.0961\0.0956\0.0949\0.0941\0.0936\0.0927\0.0917\0.0912\0.0906\0.0898\0.0889\0.0882\0.0873\0.0864\0.0859\0.085\0.0843\0.0839\0.0832\0.0828\0.0823\0.0818\0.0812\0.0809\0.0805\0.0802\0.0798\0.0795\0.0793\0.0788\0.0786\0.0784\0.0781\0.078\0.0777\0.0775\0.0773\0.0772\0.0771\0.0769\0.0767\0.0766\0.0764\0.0762\0.076\0.0758\0.0756\0.0754\0.0753\0.0752\0.075\0.0749\0.0749\0.0748\0.0747\0.0746\0.0746\0.0745\0.0744\0.0743\0.0742\0.0742\0.0741\0.074\0.074\0.0739\0.0739\0.0738\0.0738\0.0738\0.0737\0.0737\0.0737\0.0736\0.0735\0.0735\0.0735\0.0734\0.0734\0.0734\0.0733\0.0733\0.0733\0.0733\0.0733\0.0732\0.0732\0.0732\0.0732\0.0732\0.0732\0.0732\0.0732\0.0732\0.0732\0.0731\0.0731\0.0731\0.0731\0.0731\0.0731\0.0731\0.0731\0.0731\0.0731\0.0731\0.0731\0.073\0.073\0.073\0.073\0.073\0.073\0.0731\0.0731\0.073\0.073\0.073\0.0731\0.0731\0.0731\0.073\0.073\0.0731\0.0731\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073\0.073</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.91\0.946\0.922\0.787\0.685\0.578\0.554\0.525\0.496\0.491\0.479\0.462\0.449\0.442\0.428\0.413\0.413\0.418\0.421\0.401\0.406\0.402\0.405\0.41\0.392\0.398\0.389\0.385\0.377\0.366\0.358\0.334\0.317\0.311\0.301\0.301\0.304\0.299\0.298\0.295\0.289\0.288\0.285\0.287\0.276\0.262\0.252\0.255\0.244\0.238\0.241\0.243\0.231\0.226\0.224\0.231\0.227\0.219\0.216\0.213\0.201\0.197\0.198\0.195\0.197\0.197\0.202\0.199\0.195\0.192\0.192\0.186\0.185\0.182\0.18\0.174\0.171\0.176\0.176\0.175\0.176\0.176\0.176\0.177\0.171\0.167\0.165\0.166\0.162\0.162\0.161\0.163\0.162\0.163\0.165\0.169\0.169\0.171\0.17\0.168\0.166\0.169\0.17\0.174\0.174\0.176\0.173\0.17\0.171\0.172\0.172\0.172\0.171\0.167\0.17\0.169\0.169\0.167\0.167\0.169\0.17\0.169\0.169\0.168\0.167\0.165\0.165\0.165\0.165\0.165\0.165\0.163\0.162\0.162\0.161\0.162\0.163\0.162\0.161\0.161\0.162\0.161\0.16\0.16\0.159\0.159\0.159\0.16\0.16\0.159\0.16\0.159\0.158\0.159\0.159\0.159\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>247</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>4</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="UI0dDk" Title="Quasi-Newton method results">
   <Caption Id="VB6e1c">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.073
0.156
246
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="NwQC2n" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="VBhAS6" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="HUu8H9" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="wvAEno">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788808 and its percentage error 7.88807</Caption>
   <Data>0.00902557\105.761\6.24071\8.17014
5.78562e-5\0.677957\0.0400046\0.0523727
0.00578562\67.7957\4.00046\5.23727</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="lZT589" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="hMepe4" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="Z0JbMT" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="DU1dsJ" Title="Quasi-Newton method errors history">
   <Caption Id="rkPKfO">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.02014, and the final value after 254 epochs is 0.0883362.
The initial value of the selection error is 2.64381, and the final value after 254 epochs is 0.168511.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254</X2Data>
   <Y1Data>2.02\0.988\0.893\0.754\0.687\0.625\0.587\0.539\0.531\0.505\0.466\0.429\0.406\0.396\0.377\0.362\0.349\0.331\0.325\0.317\0.31\0.302\0.295\0.286\0.28\0.276\0.272\0.268\0.265\0.257\0.255\0.248\0.246\0.241\0.238\0.232\0.222\0.214\0.209\0.203\0.201\0.198\0.194\0.191\0.189\0.185\0.18\0.177\0.175\0.171\0.167\0.164\0.16\0.157\0.154\0.153\0.149\0.147\0.144\0.141\0.139\0.136\0.134\0.133\0.13\0.129\0.127\0.126\0.124\0.122\0.121\0.119\0.118\0.117\0.115\0.114\0.113\0.111\0.11\0.109\0.108\0.107\0.106\0.106\0.105\0.104\0.104\0.103\0.103\0.102\0.101\0.101\0.1\0.0997\0.0991\0.0983\0.0979\0.0975\0.0971\0.0966\0.0963\0.096\0.0957\0.0955\0.0952\0.0949\0.0947\0.0945\0.0943\0.094\0.0938\0.0937\0.0934\0.0932\0.0931\0.0928\0.0927\0.0925\0.0923\0.0922\0.092\0.092\0.0919\0.0917\0.0916\0.0915\0.0915\0.0914\0.0913\0.0912\0.0911\0.0911\0.091\0.091\0.0909\0.0908\0.0908\0.0908\0.0908\0.0907\0.0907\0.0907\0.0906\0.0906\0.0906\0.0905\0.0905\0.0905\0.0904\0.0904\0.0904\0.0903\0.0903\0.0903\0.0903\0.0902\0.0902\0.0902\0.0901\0.0901\0.0901\0.09\0.09\0.09\0.09\0.09\0.09\0.0899\0.0899\0.0899\0.0899\0.0898\0.0898\0.0898\0.0898\0.0897\0.0897\0.0896\0.0896\0.0896\0.0895\0.0895\0.0896\0.0896\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0894\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0895\0.0896\0.0896\0.0896\0.0896\0.0896\0.0896\0.0896\0.0896\0.0896\0.0896\0.0895\0.0895\0.0895\0.0894\0.0894\0.0893\0.0892\0.0892\0.0892\0.0891\0.089\0.089\0.0889\0.0888\0.0888\0.0887\0.0886\0.0886\0.0885\0.0884\0.0883</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.64\0.974\0.853\0.685\0.655\0.598\0.597\0.561\0.555\0.559\0.554\0.493\0.443\0.435\0.426\0.409\0.418\0.399\0.411\0.402\0.367\0.35\0.335\0.337\0.337\0.339\0.335\0.348\0.346\0.341\0.347\0.348\0.349\0.34\0.326\0.33\0.331\0.327\0.327\0.315\0.316\0.313\0.313\0.306\0.302\0.294\0.274\0.27\0.266\0.254\0.25\0.241\0.239\0.242\0.241\0.229\0.234\0.228\0.225\0.225\0.225\0.225\0.223\0.213\0.203\0.201\0.199\0.201\0.199\0.191\0.183\0.186\0.185\0.185\0.178\0.18\0.181\0.18\0.185\0.183\0.181\0.178\0.178\0.177\0.174\0.172\0.171\0.171\0.171\0.172\0.175\0.176\0.172\0.174\0.179\0.183\0.186\0.181\0.179\0.176\0.175\0.176\0.177\0.178\0.18\0.181\0.181\0.179\0.177\0.177\0.177\0.177\0.178\0.178\0.18\0.179\0.178\0.176\0.175\0.173\0.176\0.176\0.175\0.175\0.175\0.175\0.175\0.174\0.174\0.173\0.173\0.173\0.172\0.173\0.174\0.173\0.173\0.173\0.173\0.172\0.172\0.172\0.172\0.173\0.173\0.173\0.174\0.174\0.174\0.174\0.174\0.174\0.174\0.174\0.174\0.174\0.174\0.174\0.174\0.174\0.174\0.174\0.173\0.173\0.173\0.173\0.173\0.173\0.173\0.174\0.174\0.173\0.173\0.173\0.173\0.173\0.174\0.173\0.173\0.173\0.172\0.172\0.173\0.172\0.172\0.172\0.171\0.171\0.171\0.17\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.169\0.168\0.167\0.167\0.166\0.166\0.165\0.166\0.165\0.165\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.165\0.165\0.165\0.165\0.166\0.167\0.167\0.167\0.167\0.167\0.168\0.168\0.169</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>255</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="jgDkC5" Title="Quasi-Newton method results">
   <Caption Id="85SNMS">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0883
0.169
254
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="BuSwUJ" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="mA7YZk" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="6xFi8r" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QpR6DO">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788808 and its percentage error 7.88807</Caption>
   <Data>0.0250511\118.807\6.2055\8.18303
0.000160584\0.761581\0.0397788\0.0524553
0.0160584\76.1581\3.97788\5.24553</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="evaRgF" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="w8w27d" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="ZkvKRi" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="CRYoFJ" Title="Quasi-Newton method errors history">
   <Caption Id="BYQlOD">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.61573, and the final value after 118 epochs is 0.37666.
The initial value of the selection error is 1.34401, and the final value after 118 epochs is 0.363188.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118</X2Data>
   <Y1Data>1.62\0.88\0.791\0.738\0.712\0.694\0.674\0.659\0.644\0.626\0.603\0.586\0.567\0.562\0.554\0.549\0.543\0.532\0.525\0.52\0.514\0.51\0.501\0.496\0.49\0.486\0.479\0.475\0.467\0.461\0.454\0.449\0.445\0.441\0.437\0.434\0.43\0.426\0.422\0.418\0.415\0.413\0.41\0.408\0.405\0.403\0.4\0.398\0.396\0.395\0.392\0.389\0.387\0.385\0.384\0.383\0.382\0.381\0.38\0.38\0.38\0.379\0.379\0.379\0.379\0.378\0.378\0.378\0.378\0.378\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.34\0.851\0.748\0.648\0.66\0.649\0.613\0.585\0.578\0.544\0.545\0.535\0.513\0.504\0.494\0.499\0.496\0.504\0.493\0.479\0.479\0.474\0.471\0.465\0.458\0.45\0.445\0.446\0.446\0.436\0.422\0.413\0.407\0.407\0.401\0.4\0.4\0.402\0.401\0.397\0.396\0.391\0.39\0.387\0.386\0.386\0.381\0.379\0.378\0.382\0.38\0.38\0.378\0.375\0.373\0.37\0.368\0.368\0.367\0.367\0.367\0.367\0.366\0.365\0.364\0.364\0.365\0.365\0.365\0.365\0.366\0.365\0.365\0.365\0.365\0.364\0.365\0.365\0.365\0.365\0.364\0.364\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.364\0.364\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>119</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="td4HGs" Title="Quasi-Newton method results">
   <Caption Id="BG4GYp">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.377
0.363
118
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="vhhTzP" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="wZUkCB" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="Q64074" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="q38FZU">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0413666\113.8\8.22479\9.18977
0.00026517\0.729484\0.052723\0.0589088
0.026517\72.9484\5.2723\5.89088</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="oMJtFc" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="tfVr44">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00707245\308.625\6.84933\14.284
2.08013e-5\0.90772\0.0201451\0.0420119
0.00208013\90.772\2.01451\4.20119</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="yYLDfo" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="eeLa38">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0193596\25.6236\5.62408\4.69112
7.80629e-5\0.103321\0.0226777\0.0189158
0.00780629\10.3321\2.26777\1.89158</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="pX8DrI" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="HacHbo">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00425339\42.5631\7.61061\5.06417
5.90748e-5\0.591154\0.105703\0.0703357
0.00590748\59.1154\10.5703\7.03357</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="pYoG5S" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Eo7zsQ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0001297\14.4171\0.688159\0.97135
7.62939e-6\0.848067\0.0404799\0.0571382
0.000762939\84.8067\4.04799\5.71382</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="yrp7oB" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="VjK0LO" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="LRqVfX" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="tJpxHL" Title="Quasi-Newton method errors history">
   <Caption Id="ELUfDw">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.37172, and the final value after 147 epochs is 0.363999.
The initial value of the selection error is 1.97362, and the final value after 147 epochs is 0.358064.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147</X2Data>
   <Y1Data>2.37\1.01\0.849\0.784\0.765\0.74\0.722\0.707\0.682\0.672\0.662\0.656\0.646\0.638\0.631\0.621\0.613\0.61\0.609\0.602\0.59\0.578\0.573\0.569\0.56\0.553\0.548\0.543\0.534\0.528\0.524\0.517\0.513\0.506\0.501\0.496\0.491\0.485\0.477\0.473\0.469\0.464\0.459\0.455\0.45\0.447\0.441\0.435\0.431\0.427\0.424\0.42\0.416\0.413\0.409\0.406\0.403\0.4\0.397\0.395\0.393\0.39\0.388\0.386\0.384\0.383\0.381\0.38\0.378\0.377\0.376\0.374\0.373\0.372\0.371\0.37\0.369\0.369\0.368\0.368\0.367\0.367\0.367\0.366\0.366\0.366\0.366\0.366\0.365\0.365\0.365\0.365\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.97\1\0.833\0.795\0.698\0.654\0.636\0.617\0.618\0.611\0.599\0.565\0.585\0.582\0.589\0.577\0.561\0.568\0.581\0.586\0.547\0.543\0.543\0.543\0.537\0.523\0.518\0.516\0.502\0.486\0.485\0.491\0.487\0.474\0.469\0.469\0.469\0.461\0.448\0.448\0.451\0.446\0.44\0.436\0.425\0.419\0.411\0.409\0.406\0.403\0.399\0.399\0.396\0.389\0.38\0.379\0.378\0.38\0.382\0.381\0.378\0.37\0.366\0.365\0.368\0.369\0.37\0.368\0.364\0.361\0.356\0.358\0.359\0.36\0.36\0.358\0.355\0.354\0.354\0.356\0.359\0.361\0.361\0.36\0.359\0.358\0.359\0.36\0.36\0.359\0.358\0.358\0.358\0.36\0.36\0.361\0.36\0.359\0.359\0.359\0.36\0.359\0.359\0.358\0.358\0.359\0.359\0.36\0.359\0.359\0.359\0.359\0.359\0.359\0.358\0.358\0.358\0.359\0.359\0.359\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>148</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="eervl8" Title="Quasi-Newton method results">
   <Caption Id="2CobMb">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.364
0.358
147
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Iz4lyg" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="hSfX2e" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="k9PqN4" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="YpWZm9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00942993\103.025\8.30853\9.08325
6.04483e-5\0.660417\0.0532598\0.0582259
0.00604483\66.0417\5.32598\5.82259</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="sWYXFc" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Y5zH2U">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0241652\308.604\6.69184\14.2201
7.1074e-5\0.90766\0.0196819\0.0418238
0.0071074\90.766\1.96819\4.18238</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="yAQf6h" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="M9UPV0">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0617867\33.8632\5.54996\4.7632
0.00024914\0.136545\0.0223789\0.0192065
0.024914\13.6545\2.23789\1.92065</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="UvKa2E" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="a6c72i">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0202465\39.7295\7.58047\5.02925
0.000281201\0.551799\0.105284\0.0698506
0.0281201\55.1799\10.5284\6.98506</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="cXZpeO" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="VtiTtq">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000700951\14.1421\0.733315\0.970884
4.12324e-5\0.831888\0.0431362\0.0571108
0.00412324\83.1888\4.31362\5.71108</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ZyEzce" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="cPuPif" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="2vw1Xd" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="nsv9Fe" Title="Quasi-Newton method errors history">
   <Caption Id="7sVqng">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.53099, and the final value after 190 epochs is 0.100855.
The initial value of the selection error is 1.63844, and the final value after 190 epochs is 0.181675.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190</X2Data>
   <Y1Data>1.53\0.781\0.596\0.505\0.464\0.423\0.395\0.369\0.336\0.321\0.313\0.301\0.29\0.281\0.275\0.261\0.247\0.236\0.226\0.218\0.211\0.207\0.202\0.198\0.193\0.189\0.185\0.18\0.176\0.171\0.164\0.16\0.155\0.151\0.145\0.142\0.136\0.131\0.128\0.126\0.124\0.122\0.12\0.116\0.114\0.112\0.112\0.111\0.11\0.11\0.109\0.109\0.108\0.108\0.108\0.108\0.108\0.107\0.107\0.107\0.107\0.107\0.107\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.105\0.105\0.105\0.105\0.105\0.104\0.104\0.104\0.104\0.104\0.104\0.104\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.103\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.102\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101\0.101</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.64\0.805\0.562\0.494\0.467\0.45\0.45\0.396\0.398\0.396\0.385\0.353\0.348\0.323\0.311\0.294\0.264\0.252\0.237\0.24\0.243\0.241\0.229\0.23\0.24\0.234\0.22\0.216\0.225\0.228\0.209\0.206\0.199\0.197\0.222\0.228\0.233\0.239\0.229\0.221\0.217\0.209\0.202\0.188\0.182\0.182\0.185\0.185\0.186\0.183\0.182\0.181\0.179\0.182\0.184\0.186\0.185\0.182\0.187\0.187\0.181\0.188\0.187\0.183\0.185\0.185\0.186\0.193\0.191\0.189\0.187\0.186\0.18\0.18\0.182\0.18\0.18\0.179\0.179\0.178\0.175\0.175\0.175\0.175\0.174\0.173\0.173\0.173\0.174\0.175\0.174\0.175\0.175\0.175\0.173\0.172\0.172\0.174\0.174\0.174\0.175\0.176\0.176\0.177\0.177\0.177\0.176\0.176\0.176\0.177\0.177\0.178\0.178\0.178\0.177\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.178\0.179\0.18\0.181\0.181\0.182\0.182\0.182\0.183\0.183\0.183\0.183\0.184\0.184\0.184\0.184\0.184\0.184\0.184\0.184\0.184\0.184\0.184\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.183\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.182</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>191</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="Suctmu" Title="Quasi-Newton method results">
   <Caption Id="rxcnXu">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.101
0.182
190
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="pkKWvn" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="dqw7mD" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="MkZhD0" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="RxUa5p">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0326691\120.072\6.43737\8.50021
0.000209417\0.769691\0.0412652\0.0544885
0.0209417\76.9691\4.12652\5.44885</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="cMWdc6" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="u7uhfN" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="AlbPPL" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="LCQsHv" Title="Quasi-Newton method errors history">
   <Caption Id="5yrF1e">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.63295, and the final value after 199 epochs is 0.0871078.
The initial value of the selection error is 2.33864, and the final value after 199 epochs is 0.159666.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199</X2Data>
   <Y1Data>2.63\0.731\0.587\0.464\0.432\0.394\0.363\0.326\0.297\0.281\0.264\0.255\0.249\0.244\0.234\0.22\0.214\0.207\0.201\0.195\0.192\0.187\0.18\0.171\0.167\0.161\0.158\0.154\0.149\0.146\0.143\0.139\0.135\0.133\0.131\0.129\0.127\0.125\0.122\0.119\0.117\0.115\0.113\0.112\0.111\0.109\0.108\0.107\0.106\0.105\0.105\0.104\0.104\0.103\0.103\0.102\0.102\0.101\0.101\0.101\0.1\0.1\0.0999\0.0996\0.0993\0.0989\0.0982\0.0975\0.0971\0.0969\0.0965\0.0963\0.0959\0.0955\0.0952\0.095\0.0948\0.0946\0.0943\0.094\0.0937\0.0934\0.093\0.0929\0.0927\0.0924\0.0922\0.0919\0.0917\0.0915\0.0913\0.0909\0.0907\0.0905\0.0902\0.0901\0.0899\0.0898\0.0896\0.0895\0.0892\0.0891\0.089\0.0889\0.0888\0.0887\0.0886\0.0885\0.0885\0.0884\0.0883\0.0882\0.0882\0.0881\0.088\0.0878\0.0877\0.0876\0.0875\0.0875\0.0874\0.0874\0.0873\0.0872\0.0872\0.0871\0.0871\0.0871\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.0869\0.0869\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.087\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.34\0.811\0.684\0.579\0.516\0.487\0.432\0.378\0.367\0.398\0.369\0.356\0.352\0.343\0.321\0.314\0.305\0.311\0.307\0.288\0.28\0.284\0.271\0.262\0.261\0.252\0.244\0.233\0.217\0.215\0.206\0.202\0.2\0.199\0.194\0.19\0.185\0.185\0.186\0.181\0.18\0.188\0.187\0.19\0.192\0.193\0.189\0.187\0.184\0.185\0.186\0.185\0.183\0.181\0.175\0.174\0.173\0.175\0.174\0.173\0.169\0.169\0.167\0.171\0.174\0.174\0.175\0.174\0.176\0.174\0.173\0.177\0.178\0.179\0.177\0.177\0.173\0.173\0.175\0.176\0.179\0.178\0.177\0.176\0.174\0.174\0.172\0.173\0.175\0.178\0.177\0.174\0.175\0.174\0.172\0.17\0.169\0.171\0.168\0.168\0.167\0.166\0.167\0.167\0.165\0.164\0.163\0.164\0.163\0.163\0.164\0.162\0.162\0.163\0.164\0.164\0.163\0.163\0.162\0.162\0.163\0.163\0.163\0.163\0.163\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.161\0.161\0.161\0.161\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.159\0.159\0.159\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>200</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="5BuYjq" Title="Quasi-Newton method results">
   <Caption Id="y6OQNn">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0871
0.16
199
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="pH1YYv" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="CG8Z8D" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="SFnx32" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="SMOcxy">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0208778\105.436\6.0817\7.9679
0.000133832\0.675873\0.0389852\0.0510763
0.0133832\67.5872\3.89852\5.10763</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="pXqZNF" Title="Inputs correlations" Component="Data set" Name="Calculate inputs correlations">
  <Text Id="yEU1oi" Title="Task description">This task calculates the absolute values of the correlations among all inputs.
These correlations can be of different types depending on the nature of the variables: linear, exponential, power, logarithmic, or logistic.
The correlation is a numeric value between 0 and 1 that expresses the strength of the relationship between two input variables in the data set.
When it is close to 1, it indicates a strong relationship, and a value close to 0 indicates no relationship.
</Text>
  <Table Id="EjR5Dn" Title="Inputs correlations">
   <Caption Id="yJ48cX">The following table shows the value of the correlations among all input variables.</Caption>
   <Data>1\0.00562\0.0024\0.0653\0.0492\0.0218\0.0282\-0.0161\-0.00562\0.0165\0.0252\0.0191\0.0199\-0.0663\0.01\0.808\0.00765\0.00362\0.0619\0.0357\0.0434\0.0124\-0.0148\-0.0113\0.0203\0.0296\0.0267\0.0217\-0.063\0.0208\0.674\0.00962\0.00381\-0.017\0.0258\0.0302\0.0319\0.02\-0.0511\0.0152
0.00562\1\0.00146\0.144\0.161\0.153\0.0937\-0.108\-0.00268\0.431\0.417\0.442\-0.208\-0.137\-0.228\0.00463\0.985\0.00324\0.149\0.16\0.157\0.102\-0.0957\-0.00426\0.419\0.405\0.429\-0.19\-0.146\-0.217\0.00364\0.97\0.00128\-0.00457\0.408\0.394\0.418\-0.177\-0.15\-0.209
0.0024\0.00146\1\-0.0238\-0.111\0.0586\-0.185\-0.0328\0.0221\-0.00834\-0.0119\-0.00504\0.00283\0.00872\0.0127\-0.00913\-0.0035\0.388\-0.0634\-0.0656\0.0295\-0.12\-0.0222\-0.00613\0.00185\-0.00248\-0.00292\-0.00936\0.0069\0.00674\-0.013\-0.00343\-0.404\-0.00866\-0.00137\0.00338\-0.00833\0.00152\0.0082\-0.0186
0.0653\0.144\-0.0238\1\0.648\-0.197\0.298\0.185\-0.0644\0.18\0.258\0.112\0.314\-0.594\-0.0883\0.0497\0.14\0.0709\0.702\0.44\-0.143\0.166\0.14\-0.0396\0.182\0.218\0.158\0.22\-0.429\-0.0541\0.0396\0.139\0.0866\-0.0201\0.15\0.172\0.14\0.173\-0.245\-0.0618
0.0492\0.161\-0.111\0.648\1\-0.113\0.467\0.194\-0.22\0.317\0.409\0.213\0.266\-0.406\-0.28\0.0557\0.16\-0.0767\0.884\0.726\0.0876\0.26\0.127\-0.112\0.328\0.407\0.243\0.233\-0.5\-0.265\0.0433\0.158\0.0846\-0.0517\0.321\0.355\0.287\0.12\-0.349\-0.224
0.0218\0.153\0.0586\-0.197\-0.113\1\-0.459\-0.277\-0.0708\0.67\0.656\0.611\-0.474\0.273\-0.619\-0.0316\0.162\0.0583\-0.29\0.125\0.764\-0.374\-0.252\-0.117\0.691\0.682\0.616\-0.491\0.325\-0.663\-0.0306\0.172\-0.0256\-0.0847\0.688\0.671\0.628\-0.473\0.329\-0.708
0.0282\0.0937\-0.185\0.298\0.467\-0.459\1\0.457\-0.138\-0.289\-0.18\-0.371\0.472\-0.335\0.184\0.0479\0.0907\-0.199\0.482\0.368\-0.401\0.734\0.383\-0.0969\-0.275\-0.153\-0.377\0.483\-0.497\0.16\0.0339\0.0857\0.152\-0.0384\-0.262\-0.183\-0.312\0.397\-0.409\0.166
-0.0161\-0.108\-0.0328\0.185\0.194\-0.277\0.457\1\-0.0858\-0.155\-0.0921\-0.197\0.274\-0.0891\0.0524\-0.0103\-0.111\-0.0468\0.245\0.179\-0.254\0.401\0.838\-0.0745\-0.15\-0.0763\-0.202\0.29\-0.148\0.0334\-0.00931\-0.115\0.0291\-0.0528\-0.145\-0.0836\-0.187\0.27\-0.143\0.0242
-0.00562\-0.00268\0.0221\-0.0644\-0.22\-0.0708\-0.138\-0.0858\1\-0.0998\-0.18\0.00339\-0.187\0.0806\0.18\0.00233\0.00207\0.0104\-0.147\-0.181\-0.0515\-0.0927\-0.0625\0.2\-0.107\-0.179\-0.0015\-0.259\0.148\0.324\0.00198\0.00377\0.00482\0.0974\-0.104\-0.148\-0.0391\-0.182\0.13\0.241
0.0165\0.431\-0.00834\0.18\0.317\0.67\-0.289\-0.155\-0.0998\1\0.968\0.95\-0.437\0.0998\-0.784\0.0108\0.441\-0.00237\0.162\0.287\0.654\-0.305\-0.157\-0.0731\0.971\0.935\0.931\-0.459\0.148\-0.789\-0.00987\0.45\0.00386\-0.065\0.939\0.906\0.902\-0.456\0.148\-0.767
0.0252\0.417\-0.0119\0.258\0.409\0.656\-0.18\-0.0921\-0.18\0.968\1\0.857\-0.299\-0.12\-0.812\0.0138\0.427\-0.00674\0.248\0.358\0.642\-0.227\-0.111\-0.11\0.948\0.954\0.867\-0.344\-0.0714\-0.834\-0.012\0.433\0.0033\-0.0861\0.918\0.915\0.853\-0.374\0.0704\-0.799
0.0191\0.442\-0.00504\0.112\0.213\0.611\-0.371\-0.197\0.00339\0.95\0.857\1\-0.539\0.202\-0.656\0.0142\0.452\0.00354\0.0796\0.205\0.597\-0.352\-0.181\-0.0235\0.912\0.838\0.929\-0.527\0.23\-0.635\-0.00989\0.462\0.00935\-0.0342\0.882\0.824\0.883\-0.488\0.194\-0.644
0.0199\-0.208\0.00283\0.314\0.266\-0.474\0.472\0.274\-0.187\-0.437\-0.299\-0.539\1\-0.526\0.296\0.0265\-0.222\0.00796\0.328\0.215\-0.443\0.409\0.241\-0.118\-0.407\-0.296\-0.483\0.851\-0.515\0.279\0.0295\-0.233\0.0125\-0.0758\-0.408\-0.322\-0.458\0.646\-0.406\0.279
-0.0663\-0.137\0.00872\-0.594\-0.406\0.273\-0.335\-0.0891\0.0806\0.0998\-0.12\0.202\-0.526\1\-0.18\-0.0531\-0.133\-0.00787\-0.444\-0.275\0.233\-0.227\-0.0484\0.0187\0.0776\-0.0838\0.115\-0.367\0.602\-0.146\-0.0534\-0.132\-0.0435\0.00906\0.102\0.0628\0.118\-0.27\0.364\-0.137
0.01\-0.228\0.0127\-0.0883\-0.28\-0.619\0.184\0.0524\0.18\-0.784\-0.812\-0.656\0.296\-0.18\1\0.00719\-0.236\0.0179\-0.111\-0.266\-0.606\0.207\0.0631\0.108\-0.769\-0.786\-0.671\0.326\-0.149\0.881\0.00739\-0.243\0.0115\0.074\-0.76\-0.769\-0.678\0.355\-0.144\0.802
0.808\0.00463\-0.00913\0.0497\0.0557\-0.0316\0.0479\-0.0103\0.00233\0.0108\0.0138\0.0142\0.0265\-0.0531\0.00719\1\0.00417\0.00261\0.0647\0.0483\0.0216\0.0283\-0.0167\-0.00509\0.0159\0.0244\0.0189\0.0207\-0.0666\0.0121\0.808\0.0062\0.00425\-0.0108\0.0201\0.0292\0.0269\0.0217\-0.0625\0.0227
0.00765\0.985\-0.0035\0.14\0.16\0.162\0.0907\-0.111\0.00207\0.441\0.427\0.452\-0.222\-0.133\-0.236\0.00417\1\0.00122\0.144\0.16\0.153\0.0936\-0.108\-0.00174\0.431\0.416\0.442\-0.208\-0.137\-0.227\0.00326\0.985\0.00319\-0.00335\0.419\0.404\0.429\-0.191\-0.145\-0.217
0.00362\0.00324\0.388\0.0709\-0.0767\0.0583\-0.199\-0.0468\0.0104\-0.00237\-0.00674\0.00354\0.00796\-0.00787\0.0179\0.00261\0.00122\1\-0.0241\-0.111\0.0585\-0.185\-0.033\0.0221\-0.00844\-0.012\-0.0051\0.00305\0.00873\0.0129\-0.00932\-0.00388\0.388\-0.00599\0.00179\-0.00248\-0.00286\-0.00929\0.00689\0.00688
0.0619\0.149\-0.0634\0.702\0.884\-0.29\0.482\0.245\-0.147\0.162\0.248\0.0796\0.328\-0.444\-0.111\0.0647\0.144\-0.0241\1\0.648\-0.198\0.298\0.184\-0.064\0.18\0.258\0.112\0.314\-0.594\-0.0877\0.0491\0.14\0.071\-0.0391\0.181\0.217\0.158\0.22\-0.429\-0.0536
0.0357\0.16\-0.0656\0.44\0.726\0.125\0.368\0.179\-0.181\0.287\0.358\0.205\0.215\-0.275\-0.266\0.0483\0.16\-0.111\0.648\1\-0.113\0.467\0.193\-0.22\0.317\0.408\0.213\0.267\-0.406\-0.28\0.0557\0.159\-0.0762\-0.112\0.328\0.407\0.243\0.234\-0.5\-0.264
0.0434\0.157\0.0295\-0.143\0.0876\0.764\-0.401\-0.254\-0.0515\0.654\0.642\0.597\-0.443\0.233\-0.606\0.0216\0.153\0.0585\-0.198\-0.113\1\-0.459\-0.277\-0.0705\0.67\0.655\0.611\-0.474\0.273\-0.619\-0.0323\0.162\0.0584\-0.117\0.691\0.682\0.616\-0.491\0.326\-0.663
0.0124\0.102\-0.12\0.166\0.26\-0.374\0.734\0.401\-0.0927\-0.305\-0.227\-0.352\0.409\-0.227\0.207\0.0283\0.0936\-0.185\0.298\0.467\-0.459\1\0.457\-0.138\-0.289\-0.18\-0.371\0.472\-0.335\0.184\0.048\0.0906\-0.198\-0.0968\-0.275\-0.153\-0.377\0.483\-0.497\0.161
-0.0148\-0.0957\-0.0222\0.14\0.127\-0.252\0.383\0.838\-0.0625\-0.157\-0.111\-0.181\0.241\-0.0484\0.0631\-0.0167\-0.108\-0.033\0.184\0.193\-0.277\0.457\1\-0.0855\-0.156\-0.0924\-0.197\0.274\-0.089\0.0526\-0.0109\-0.111\-0.0469\-0.0742\-0.15\-0.0765\-0.202\0.289\-0.147\0.0337
-0.0113\-0.00426\-0.00613\-0.0396\-0.112\-0.117\-0.0969\-0.0745\0.2\-0.0731\-0.11\-0.0235\-0.118\0.0187\0.108\-0.00509\-0.00174\0.0221\-0.064\-0.22\-0.0705\-0.138\-0.0855\1\-0.0995\-0.18\0.00351\-0.188\0.0806\0.179\0.00311\0.00278\0.0103\0.199\-0.107\-0.179\-0.00156\-0.259\0.147\0.323
0.0203\0.419\0.00185\0.182\0.328\0.691\-0.275\-0.15\-0.107\0.971\0.948\0.912\-0.407\0.0776\-0.769\0.0159\0.431\-0.00844\0.18\0.317\0.67\-0.289\-0.156\-0.0995\1\0.968\0.95\-0.437\0.0998\-0.784\0.0102\0.441\-0.00235\-0.0727\0.971\0.935\0.931\-0.459\0.148\-0.789
0.0296\0.405\-0.00248\0.218\0.407\0.682\-0.153\-0.0763\-0.179\0.935\0.954\0.838\-0.296\-0.0838\-0.786\0.0244\0.416\-0.012\0.258\0.408\0.655\-0.18\-0.0924\-0.18\0.968\1\0.857\-0.299\-0.12\-0.812\0.0131\0.427\-0.00667\-0.109\0.948\0.954\0.867\-0.345\-0.0711\-0.834
0.0267\0.429\-0.00292\0.158\0.243\0.616\-0.377\-0.202\-0.0015\0.931\0.867\0.929\-0.483\0.115\-0.671\0.0189\0.442\-0.0051\0.112\0.213\0.611\-0.371\-0.197\0.00351\0.95\0.857\1\-0.539\0.202\-0.656\0.0139\0.452\0.00347\-0.0234\0.912\0.839\0.929\-0.527\0.23\-0.635
0.0217\-0.19\-0.00936\0.22\0.233\-0.491\0.483\0.29\-0.259\-0.459\-0.344\-0.527\0.851\-0.367\0.326\0.0207\-0.208\0.00305\0.314\0.267\-0.474\0.472\0.274\-0.188\-0.437\-0.299\-0.539\1\-0.526\0.296\0.0272\-0.222\0.00819\-0.118\-0.406\-0.296\-0.483\0.851\-0.515\0.279
-0.063\-0.146\0.0069\-0.429\-0.5\0.325\-0.497\-0.148\0.148\0.148\-0.0714\0.23\-0.515\0.602\-0.149\-0.0666\-0.137\0.00873\-0.594\-0.406\0.273\-0.335\-0.089\0.0806\0.0998\-0.12\0.202\-0.526\1\-0.181\-0.0533\-0.133\-0.00793\0.0186\0.0776\-0.0839\0.115\-0.367\0.602\-0.146
0.0208\-0.217\0.00674\-0.0541\-0.265\-0.663\0.16\0.0334\0.324\-0.789\-0.834\-0.635\0.279\-0.146\0.881\0.0121\-0.227\0.0129\-0.0877\-0.28\-0.619\0.184\0.0526\0.179\-0.784\-0.812\-0.656\0.296\-0.181\1\0.00875\-0.235\0.0179\0.108\-0.769\-0.786\-0.672\0.326\-0.15\0.881
0.674\0.00364\-0.013\0.0396\0.0433\-0.0306\0.0339\-0.00931\0.00198\-0.00987\-0.012\-0.00989\0.0295\-0.0534\0.00739\0.808\0.00326\-0.00932\0.0491\0.0557\-0.0323\0.048\-0.0109\0.00311\0.0102\0.0131\0.0139\0.0272\-0.0533\0.00875\1\0.00279\0.00327\-0.00463\0.0157\0.024\0.0191\0.0208\-0.0663\0.0134
0.00962\0.97\-0.00343\0.139\0.158\0.172\0.0857\-0.115\0.00377\0.45\0.433\0.462\-0.233\-0.132\-0.243\0.0062\0.985\-0.00388\0.14\0.159\0.162\0.0906\-0.111\0.00278\0.441\0.427\0.452\-0.222\-0.133\-0.235\0.00279\1\0.00115\0.00125\0.431\0.416\0.442\-0.208\-0.137\-0.227
0.00381\0.00128\-0.404\0.0866\0.0846\-0.0256\0.152\0.0291\0.00482\0.00386\0.0033\0.00935\0.0125\-0.0435\0.0115\0.00425\0.00319\0.388\0.071\-0.0762\0.0584\-0.198\-0.0469\0.0103\-0.00235\-0.00667\0.00347\0.00819\-0.00793\0.0179\0.00327\0.00115\1\0.0219\-0.00832\-0.0118\-0.00503\0.00326\0.00842\0.0128
-0.017\-0.00457\-0.00866\-0.0201\-0.0517\-0.0847\-0.0384\-0.0528\0.0974\-0.065\-0.0861\-0.0342\-0.0758\0.00906\0.074\-0.0108\-0.00335\-0.00599\-0.0391\-0.112\-0.117\-0.0968\-0.0742\0.199\-0.0727\-0.109\-0.0234\-0.118\0.0186\0.108\-0.00463\0.00125\0.0219\1\-0.0993\-0.179\0.00345\-0.188\0.0803\0.179
0.0258\0.408\-0.00137\0.15\0.321\0.688\-0.262\-0.145\-0.104\0.939\0.918\0.882\-0.408\0.102\-0.76\0.0201\0.419\0.00179\0.181\0.328\0.691\-0.275\-0.15\-0.107\0.971\0.948\0.912\-0.406\0.0776\-0.769\0.0157\0.431\-0.00832\-0.0993\1\0.968\0.95\-0.437\0.1\-0.784
0.0302\0.394\0.00338\0.172\0.355\0.671\-0.183\-0.0836\-0.148\0.906\0.915\0.824\-0.322\0.0628\-0.769\0.0292\0.404\-0.00248\0.217\0.407\0.682\-0.153\-0.0765\-0.179\0.935\0.954\0.839\-0.296\-0.0839\-0.786\0.024\0.416\-0.0118\-0.179\0.968\1\0.857\-0.299\-0.12\-0.812
0.0319\0.418\-0.00833\0.14\0.287\0.628\-0.312\-0.187\-0.0391\0.902\0.853\0.883\-0.458\0.118\-0.678\0.0269\0.429\-0.00286\0.158\0.243\0.616\-0.377\-0.202\-0.00156\0.931\0.867\0.929\-0.483\0.115\-0.672\0.0191\0.442\-0.00503\0.00345\0.95\0.857\1\-0.539\0.202\-0.657
0.02\-0.177\0.00152\0.173\0.12\-0.473\0.397\0.27\-0.182\-0.456\-0.374\-0.488\0.646\-0.27\0.355\0.0217\-0.191\-0.00929\0.22\0.234\-0.491\0.483\0.289\-0.259\-0.459\-0.345\-0.527\0.851\-0.367\0.326\0.0208\-0.208\0.00326\-0.188\-0.437\-0.299\-0.539\1\-0.526\0.296
-0.0511\-0.15\0.0082\-0.245\-0.349\0.329\-0.409\-0.143\0.13\0.148\0.0704\0.194\-0.406\0.364\-0.144\-0.0625\-0.145\0.00689\-0.429\-0.5\0.326\-0.497\-0.147\0.147\0.148\-0.0711\0.23\-0.515\0.602\-0.15\-0.0663\-0.137\0.00842\0.0803\0.1\-0.12\0.202\-0.526\1\-0.181
0.0152\-0.209\-0.0186\-0.0618\-0.224\-0.708\0.166\0.0242\0.241\-0.767\-0.799\-0.644\0.279\-0.137\0.802\0.0227\-0.217\0.00688\-0.0536\-0.264\-0.663\0.161\0.0337\0.323\-0.789\-0.834\-0.635\0.279\-0.146\0.881\0.0134\-0.227\0.0128\0.179\-0.784\-0.812\-0.657\0.296\-0.181\1</Data>
   <RowsName>DAY_lag_1\MONTH_lag_1\WEEKDAY_lag_1\PM2.5(microg/m3)_lag_1\PM10(microg/m3)_lag_1\O3(microg/m3)_lag_1\NO2(microg/m3)_lag_1\SO2(microg/m3)_lag_1\PRECIPITATIONS(mm)_lag_1\TAVG(C)_lag_1\TMAX(C)_lag_1\TMIN(C)_lag_1\PRESSURE(hPa)_lag_1\WINDSPEED(km/h)_lag_1\HUMIDITY(percentage)_lag_1\DAY_lag_0\MONTH_lag_0\WEEKDAY_lag_0\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_0\O3(microg/m3)_lag_0\NO2(microg/m3)_lag_0\SO2(microg/m3)_lag_0\PRECIPITATIONS(mm)_lag_0\TAVG(C)_lag_0\TMAX(C)_lag_0\TMIN(C)_lag_0\PRESSURE(hPa)_lag_0\WINDSPEED(km/h)_lag_0\HUMIDITY(percentage)_lag_0\DAY_ahead_1\MONTH_ahead_1\WEEKDAY_ahead_1\PRECIPITATIONS(mm)_ahead_1\TAVG(C)_ahead_1\TMAX(C)_ahead_1\TMIN(C)_ahead_1\PRESSURE(hPa)_ahead_1\WINDSPEED(km/h)_ahead_1\HUMIDITY(percentage)_ahead_1</RowsName>
   <ColumnsName>DAY_lag_1\MONTH_lag_1\WEEKDAY_lag_1\PM2.5(microg/m3)_lag_1\PM10(microg/m3)_lag_1\O3(microg/m3)_lag_1\NO2(microg/m3)_lag_1\SO2(microg/m3)_lag_1\PRECIPITATIONS(mm)_lag_1\TAVG(C)_lag_1\TMAX(C)_lag_1\TMIN(C)_lag_1\PRESSURE(hPa)_lag_1\WINDSPEED(km/h)_lag_1\HUMIDITY(percentage)_lag_1\DAY_lag_0\MONTH_lag_0\WEEKDAY_lag_0\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_0\O3(microg/m3)_lag_0\NO2(microg/m3)_lag_0\SO2(microg/m3)_lag_0\PRECIPITATIONS(mm)_lag_0\TAVG(C)_lag_0\TMAX(C)_lag_0\TMIN(C)_lag_0\PRESSURE(hPa)_lag_0\WINDSPEED(km/h)_lag_0\HUMIDITY(percentage)_lag_0\DAY_ahead_1\MONTH_ahead_1\WEEKDAY_ahead_1\PRECIPITATIONS(mm)_ahead_1\TAVG(C)_ahead_1\TMAX(C)_ahead_1\TMIN(C)_ahead_1\PRESSURE(hPa)_ahead_1\WINDSPEED(km/h)_ahead_1\HUMIDITY(percentage)_ahead_1</ColumnsName>
   <RowHeadingsWidth>21</RowHeadingsWidth>
   <ColumnHeadingsWidth>21</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="shUDD1" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="2OLHRv" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="faCakm" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="1alMoN" Title="Quasi-Newton method errors history">
   <Caption Id="an5mRw">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.83113, and the final value after 206 epochs is 0.0831394.
The initial value of the selection error is 1.6377, and the final value after 206 epochs is 0.152137.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206</X2Data>
   <Y1Data>1.83\0.868\0.746\0.659\0.515\0.463\0.437\0.411\0.39\0.355\0.338\0.32\0.302\0.296\0.282\0.272\0.259\0.249\0.246\0.241\0.232\0.224\0.217\0.205\0.195\0.19\0.186\0.181\0.174\0.163\0.159\0.157\0.154\0.15\0.146\0.142\0.14\0.137\0.134\0.131\0.128\0.125\0.122\0.119\0.118\0.115\0.114\0.111\0.109\0.107\0.106\0.104\0.103\0.102\0.101\0.0998\0.0988\0.0976\0.0968\0.0962\0.0953\0.0946\0.094\0.0937\0.0933\0.0929\0.0924\0.0919\0.0915\0.091\0.0907\0.09\0.0896\0.0892\0.0888\0.0886\0.0883\0.0881\0.0879\0.0877\0.0874\0.0872\0.0868\0.0866\0.0864\0.0862\0.086\0.0858\0.0857\0.0855\0.0852\0.0851\0.085\0.0849\0.0848\0.0848\0.0847\0.0847\0.0846\0.0846\0.0846\0.0845\0.0845\0.0844\0.0844\0.0844\0.0843\0.0843\0.0842\0.0842\0.0842\0.0842\0.0841\0.0841\0.0841\0.0841\0.084\0.084\0.084\0.0841\0.084\0.084\0.084\0.084\0.084\0.084\0.084\0.084\0.084\0.084\0.084\0.084\0.0839\0.0839\0.0839\0.0839\0.0839\0.0839\0.0839\0.0839\0.0839\0.0839\0.0839\0.0839\0.0839\0.0838\0.0838\0.0838\0.0838\0.0838\0.0837\0.0837\0.0836\0.0836\0.0835\0.0835\0.0835\0.0835\0.0834\0.0834\0.0834\0.0834\0.0834\0.0833\0.0833\0.0833\0.0833\0.0833\0.0833\0.0833\0.0832\0.0832\0.0832\0.0832\0.0832\0.0832\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831\0.0831</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.64\0.925\0.774\0.715\0.572\0.497\0.491\0.418\0.43\0.374\0.361\0.368\0.37\0.357\0.351\0.33\0.305\0.299\0.303\0.298\0.313\0.298\0.282\0.265\0.276\0.27\0.263\0.26\0.245\0.239\0.236\0.233\0.227\0.214\0.206\0.197\0.193\0.196\0.197\0.183\0.182\0.182\0.181\0.177\0.176\0.178\0.182\0.187\0.189\0.183\0.181\0.179\0.181\0.185\0.189\0.185\0.176\0.168\0.166\0.163\0.16\0.161\0.163\0.162\0.164\0.16\0.157\0.16\0.159\0.159\0.156\0.154\0.155\0.156\0.156\0.156\0.155\0.156\0.155\0.156\0.157\0.157\0.158\0.158\0.157\0.157\0.158\0.157\0.157\0.157\0.156\0.156\0.157\0.157\0.156\0.155\0.156\0.156\0.155\0.155\0.155\0.155\0.155\0.155\0.155\0.154\0.154\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.153\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.151\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>207</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="3UaHW7" Title="Quasi-Newton method results">
   <Caption Id="bOSvvr">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0831
0.152
206
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="MO6BwW" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="Ly7ryJ" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="HtXL4J" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="GMJ0Kb">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0161018\95.5045\5.92672\7.42719
0.000103217\0.612208\0.0379918\0.0476102
0.0103217\61.2208\3.79918\4.76102</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="jR7S6z" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="JPTg8H" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="oxH3eY" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="yCVLrg" Title="Quasi-Newton method errors history">
   <Caption Id="uM5dD8">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.54502, and the final value after 121 epochs is 0.094442.
The initial value of the selection error is 2.67017, and the final value after 121 epochs is 0.167276.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121</X2Data>
   <Y1Data>2.55\0.95\0.829\0.633\0.559\0.49\0.421\0.379\0.324\0.304\0.287\0.269\0.257\0.248\0.242\0.231\0.221\0.217\0.209\0.202\0.189\0.179\0.171\0.166\0.159\0.153\0.149\0.145\0.139\0.135\0.132\0.128\0.123\0.12\0.117\0.114\0.112\0.11\0.108\0.106\0.105\0.104\0.104\0.103\0.102\0.101\0.101\0.1\0.0997\0.099\0.0987\0.0981\0.0976\0.0975\0.0972\0.097\0.0969\0.0968\0.0967\0.0966\0.0965\0.0964\0.0963\0.0962\0.0962\0.0961\0.0959\0.0959\0.0958\0.0958\0.0957\0.0956\0.0955\0.0954\0.0953\0.0953\0.0952\0.0951\0.0951\0.095\0.095\0.095\0.0949\0.0949\0.0949\0.0949\0.0949\0.0949\0.0949\0.0949\0.0949\0.0949\0.0949\0.0949\0.0948\0.0948\0.0948\0.0947\0.0947\0.0946\0.0946\0.0946\0.0946\0.0945\0.0945\0.0945\0.0945\0.0945\0.0944\0.0944\0.0944\0.0944\0.0944\0.0944\0.0944\0.0944\0.0944\0.0944\0.0944\0.0944\0.0944\0.0944</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.67\1.05\0.885\0.68\0.629\0.527\0.462\0.458\0.384\0.37\0.368\0.378\0.353\0.338\0.333\0.332\0.302\0.3\0.293\0.302\0.309\0.286\0.275\0.254\0.229\0.215\0.21\0.203\0.201\0.183\0.183\0.185\0.184\0.178\0.174\0.17\0.172\0.169\0.169\0.168\0.165\0.164\0.167\0.171\0.174\0.176\0.173\0.174\0.175\0.175\0.174\0.173\0.172\0.169\0.175\0.178\0.177\0.176\0.174\0.172\0.172\0.172\0.172\0.17\0.168\0.167\0.169\0.168\0.168\0.169\0.168\0.169\0.169\0.169\0.17\0.169\0.169\0.168\0.169\0.169\0.169\0.17\0.17\0.169\0.169\0.169\0.169\0.169\0.169\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.167\0.167\0.167\0.167\0.167\0.168\0.168\0.167\0.167\0.167\0.168\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>122</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="CqC7sd" Title="Quasi-Newton method results">
   <Caption Id="Yv1xdE">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0944
0.167
121
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="9m7IS8" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="OhvP3y" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="rugAQw" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="gYvoCl">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00705719\116.332\6.14336\8.32335
4.52384e-5\0.745721\0.0393805\0.0533548
0.00452384\74.5721\3.93805\5.33548</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="CGRXOU" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="tAX13f" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="ji3HnF" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="Ia5cmS" Title="Quasi-Newton method errors history">
   <Caption Id="NdtSOv">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.52386, and the final value after 229 epochs is 0.0780033.
The initial value of the selection error is 2.48665, and the final value after 229 epochs is 0.155582.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229</X2Data>
   <Y1Data>2.52\0.919\0.774\0.652\0.538\0.507\0.443\0.408\0.39\0.36\0.336\0.322\0.307\0.29\0.28\0.263\0.257\0.247\0.236\0.229\0.223\0.218\0.21\0.202\0.194\0.188\0.18\0.174\0.165\0.16\0.155\0.151\0.146\0.141\0.135\0.131\0.127\0.124\0.121\0.119\0.116\0.114\0.112\0.11\0.108\0.107\0.105\0.104\0.103\0.102\0.101\0.0992\0.0981\0.0969\0.0959\0.0954\0.0949\0.0946\0.0944\0.094\0.0937\0.0934\0.0931\0.0928\0.0926\0.0923\0.0921\0.092\0.0919\0.0918\0.0915\0.0913\0.091\0.0908\0.0905\0.0901\0.0899\0.0895\0.0894\0.0892\0.0891\0.0889\0.0889\0.0888\0.0886\0.0885\0.0883\0.0881\0.0879\0.0878\0.0876\0.0874\0.0873\0.087\0.0866\0.0864\0.0862\0.086\0.0858\0.0855\0.0854\0.0851\0.0847\0.0844\0.0841\0.0836\0.0833\0.0829\0.0827\0.0824\0.0822\0.082\0.0818\0.0817\0.0816\0.0814\0.0813\0.0812\0.0811\0.081\0.0809\0.0808\0.0807\0.0806\0.0805\0.0805\0.0804\0.0803\0.0802\0.08\0.08\0.0799\0.0798\0.0797\0.0796\0.0796\0.0795\0.0794\0.0794\0.0793\0.0793\0.0792\0.0792\0.0792\0.0791\0.0791\0.0791\0.079\0.079\0.079\0.079\0.0789\0.0788\0.0787\0.0786\0.0786\0.0786\0.0786\0.0785\0.0785\0.0785\0.0784\0.0783\0.0783\0.0783\0.0783\0.0784\0.0784\0.0784\0.0784\0.0784\0.0784\0.0784\0.0784\0.0785\0.0786\0.0786\0.0787\0.0787\0.0787\0.0788\0.0788\0.0789\0.079\0.0791\0.0791\0.0792\0.0793\0.0794\0.0794\0.0795\0.0795\0.0796\0.0797\0.0797\0.0797\0.0798\0.0798\0.0799\0.08\0.0799\0.08\0.08\0.08\0.0799\0.0798\0.0797\0.0796\0.0795\0.0794\0.0793\0.0792\0.0791\0.079\0.0788\0.0787\0.0787\0.0787\0.0786\0.0784\0.0783\0.0783\0.0782\0.0782\0.0782\0.0781\0.0781\0.0781\0.078\0.078</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.49\1.01\0.812\0.663\0.548\0.573\0.531\0.452\0.456\0.434\0.445\0.441\0.415\0.388\0.38\0.357\0.353\0.344\0.32\0.305\0.296\0.287\0.289\0.295\0.279\0.269\0.26\0.245\0.244\0.239\0.237\0.237\0.24\0.224\0.205\0.195\0.186\0.178\0.177\0.18\0.176\0.172\0.171\0.173\0.172\0.171\0.17\0.17\0.174\0.176\0.175\0.178\0.177\0.176\0.168\0.173\0.178\0.178\0.179\0.181\0.18\0.177\0.172\0.169\0.169\0.169\0.173\0.174\0.173\0.17\0.166\0.163\0.165\0.162\0.162\0.161\0.162\0.162\0.164\0.166\0.166\0.168\0.165\0.162\0.161\0.161\0.161\0.16\0.161\0.16\0.162\0.163\0.164\0.164\0.161\0.162\0.162\0.165\0.165\0.168\0.164\0.167\0.172\0.172\0.17\0.17\0.171\0.173\0.174\0.176\0.176\0.173\0.172\0.172\0.173\0.171\0.172\0.171\0.17\0.171\0.17\0.169\0.169\0.169\0.168\0.167\0.167\0.166\0.164\0.162\0.163\0.16\0.161\0.16\0.159\0.161\0.159\0.159\0.158\0.158\0.159\0.156\0.156\0.155\0.155\0.155\0.155\0.155\0.156\0.155\0.155\0.154\0.153\0.153\0.152\0.151\0.152\0.152\0.152\0.153\0.152\0.152\0.151\0.152\0.151\0.151\0.151\0.152\0.152\0.151\0.151\0.152\0.153\0.154\0.155\0.155\0.154\0.154\0.153\0.154\0.154\0.154\0.155\0.155\0.154\0.155\0.155\0.154\0.155\0.154\0.154\0.154\0.154\0.155\0.154\0.154\0.154\0.155\0.155\0.154\0.154\0.154\0.153\0.154\0.154\0.154\0.154\0.154\0.153\0.153\0.153\0.152\0.153\0.152\0.153\0.153\0.152\0.153\0.153\0.152\0.153\0.153\0.153\0.153\0.153\0.154\0.154\0.155\0.155\0.156</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>230</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="E2Kmz3" Title="Quasi-Newton method results">
   <Caption Id="hMot7m">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.078
0.156
229
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="dbuOle" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="zxzkyL" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="GeSsZ7" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="0wV57X">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00201035\89.2374\6.15766\7.724
1.28868e-5\0.572035\0.0394722\0.0495128
0.00128868\57.2035\3.94722\4.95128</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="z6mdHz" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="sT88HK" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="EFxrpK" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="hMKOmz" Title="Quasi-Newton method errors history">
   <Caption Id="k31ty9">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.08114, and the final value after 229 epochs is 0.0835336.
The initial value of the selection error is 1.81678, and the final value after 229 epochs is 0.162264.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229</X2Data>
   <Y1Data>2.08\0.886\0.846\0.691\0.63\0.512\0.502\0.436\0.386\0.37\0.345\0.337\0.327\0.308\0.3\0.287\0.276\0.271\0.268\0.265\0.253\0.245\0.24\0.221\0.216\0.21\0.206\0.198\0.186\0.181\0.177\0.17\0.166\0.161\0.159\0.156\0.154\0.151\0.147\0.145\0.142\0.14\0.137\0.134\0.132\0.129\0.127\0.124\0.123\0.122\0.12\0.119\0.118\0.117\0.115\0.114\0.113\0.112\0.111\0.109\0.109\0.108\0.107\0.106\0.105\0.104\0.104\0.103\0.103\0.102\0.102\0.102\0.101\0.101\0.101\0.1\0.0999\0.0994\0.0989\0.0983\0.098\0.0976\0.0973\0.0968\0.0964\0.0962\0.096\0.0956\0.0953\0.095\0.0947\0.0943\0.094\0.0938\0.0933\0.093\0.0927\0.0925\0.0922\0.0921\0.0918\0.0916\0.091\0.0907\0.0903\0.09\0.0897\0.0896\0.0893\0.089\0.0886\0.0882\0.0878\0.0877\0.0875\0.0874\0.0874\0.0872\0.0871\0.0869\0.0868\0.0868\0.0867\0.0866\0.0865\0.0865\0.0863\0.0863\0.0862\0.0861\0.086\0.0859\0.0859\0.0858\0.0857\0.0856\0.0856\0.0855\0.0854\0.0853\0.0852\0.0851\0.0851\0.085\0.085\0.085\0.0849\0.0849\0.0849\0.0849\0.0849\0.0848\0.0848\0.0849\0.0849\0.0849\0.0849\0.0848\0.0848\0.0848\0.0848\0.0848\0.0848\0.0847\0.0847\0.0847\0.0847\0.0846\0.0846\0.0845\0.0845\0.0845\0.0844\0.0844\0.0844\0.0843\0.0843\0.0842\0.0842\0.084\0.084\0.0839\0.084\0.0839\0.0839\0.0838\0.0839\0.0838\0.0838\0.0837\0.0838\0.0838\0.0838\0.0838\0.0837\0.0837\0.0838\0.0838\0.0838\0.0837\0.0837\0.0837\0.0837\0.0837\0.0837\0.0837\0.0837\0.0837\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0836\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835\0.0835</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.82\0.878\0.849\0.752\0.671\0.576\0.544\0.475\0.463\0.483\0.472\0.462\0.422\0.415\0.42\0.421\0.402\0.388\0.375\0.374\0.373\0.372\0.372\0.33\0.307\0.303\0.293\0.282\0.252\0.242\0.238\0.22\0.209\0.202\0.194\0.2\0.195\0.184\0.183\0.185\0.184\0.178\0.168\0.167\0.173\0.186\0.197\0.187\0.186\0.18\0.173\0.176\0.179\0.178\0.179\0.177\0.171\0.171\0.169\0.171\0.17\0.168\0.167\0.169\0.166\0.165\0.168\0.168\0.165\0.166\0.167\0.169\0.169\0.174\0.172\0.173\0.172\0.169\0.173\0.172\0.17\0.169\0.166\0.169\0.167\0.168\0.169\0.168\0.17\0.171\0.169\0.163\0.162\0.163\0.162\0.164\0.167\0.166\0.168\0.166\0.168\0.166\0.166\0.167\0.165\0.166\0.167\0.164\0.165\0.165\0.166\0.167\0.166\0.165\0.167\0.167\0.169\0.166\0.168\0.168\0.169\0.169\0.168\0.165\0.165\0.165\0.165\0.165\0.164\0.164\0.165\0.165\0.164\0.164\0.165\0.164\0.165\0.166\0.167\0.167\0.167\0.166\0.166\0.165\0.166\0.166\0.166\0.165\0.165\0.165\0.164\0.165\0.164\0.165\0.165\0.165\0.165\0.165\0.164\0.164\0.164\0.165\0.165\0.164\0.164\0.164\0.164\0.164\0.164\0.165\0.165\0.164\0.164\0.164\0.164\0.165\0.165\0.165\0.165\0.164\0.165\0.165\0.165\0.165\0.164\0.164\0.164\0.164\0.163\0.163\0.163\0.163\0.162\0.162\0.162\0.162\0.163\0.163\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162\0.162</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>230</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="kbdxOc" Title="Quasi-Newton method results">
   <Caption Id="XicmTG">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0835
0.162
229
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="GsOAa9" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="sugpLs" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="54aXVg" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Lytode">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00727844\91.9726\6.08738\7.4476
4.66567e-5\0.589568\0.0390217\0.047741
0.00466567\58.9568\3.90217\4.77411</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ZcHUCW" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="P6kKwH" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 118.</Text>
 </Task>
 <Task Id="qY85Br" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="ev98v9" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="v1922L" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="HpF4nk" Title="Quasi-Newton method errors history">
   <Caption Id="dtHayg">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.37356, and the final value after 171 epochs is 0.0824602.
The initial value of the selection error is 2.4416, and the final value after 171 epochs is 0.157179.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171</X2Data>
   <Y1Data>2.37\0.918\0.787\0.725\0.583\0.525\0.497\0.471\0.432\0.398\0.374\0.347\0.334\0.314\0.307\0.294\0.283\0.279\0.274\0.266\0.253\0.249\0.238\0.23\0.225\0.215\0.206\0.2\0.193\0.188\0.183\0.177\0.174\0.171\0.168\0.165\0.16\0.157\0.154\0.151\0.144\0.141\0.139\0.137\0.134\0.132\0.129\0.126\0.125\0.123\0.121\0.119\0.117\0.115\0.112\0.11\0.109\0.107\0.106\0.104\0.102\0.101\0.0998\0.0986\0.0974\0.0966\0.0961\0.0953\0.0943\0.0935\0.093\0.0922\0.0916\0.0909\0.0905\0.0901\0.0896\0.0891\0.0888\0.0884\0.088\0.0875\0.0872\0.087\0.0867\0.0863\0.0859\0.0857\0.0852\0.085\0.0849\0.0847\0.0846\0.0845\0.0844\0.0841\0.084\0.0839\0.0838\0.0837\0.0837\0.0836\0.0835\0.0834\0.0834\0.0833\0.0832\0.0832\0.0832\0.0831\0.083\0.083\0.083\0.0829\0.0829\0.0828\0.0828\0.0827\0.0827\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0826\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825\0.0825</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.44\0.893\0.812\0.74\0.616\0.58\0.525\0.499\0.475\0.416\0.43\0.413\0.393\0.408\0.424\0.385\0.378\0.353\0.357\0.376\0.363\0.362\0.349\0.338\0.329\0.327\0.32\0.321\0.31\0.297\0.298\0.294\0.295\0.292\0.293\0.278\0.262\0.261\0.258\0.242\0.22\0.22\0.225\0.215\0.208\0.206\0.2\0.19\0.185\0.186\0.191\0.191\0.197\0.193\0.191\0.182\0.184\0.183\0.188\0.188\0.183\0.191\0.19\0.187\0.183\0.18\0.178\0.179\0.179\0.181\0.183\0.181\0.176\0.177\0.175\0.173\0.171\0.169\0.165\0.165\0.165\0.166\0.165\0.165\0.165\0.164\0.162\0.162\0.16\0.16\0.16\0.16\0.161\0.159\0.162\0.161\0.161\0.161\0.16\0.16\0.161\0.16\0.158\0.159\0.159\0.16\0.16\0.159\0.159\0.158\0.158\0.158\0.158\0.159\0.159\0.159\0.159\0.159\0.158\0.157\0.157\0.158\0.158\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.158\0.158\0.158\0.158\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>172</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="1dYAff" Title="Quasi-Newton method results">
   <Caption Id="XlXVC8">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0825
0.157
171
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="AhNDKr" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="LdVgbP" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="KjiMkt" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Rpwfo4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00147629\93.5135\5.96236\7.36428
9.46338e-6\0.599446\0.0382202\0.0472069
0.000946338\59.9446\3.82202\4.72069</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="hEAXju" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="8PKgNz" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="oJEbac" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="MoUE8S" Title="Quasi-Newton method errors history">
   <Caption Id="ZMfvir">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.9871, and the final value after 114 epochs is 0.629274.
The initial value of the selection error is 1.73636, and the final value after 114 epochs is 0.577527.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114</X2Data>
   <Y1Data>1.99\1.26\1\0.957\0.893\0.86\0.844\0.815\0.799\0.791\0.784\0.763\0.759\0.743\0.732\0.731\0.723\0.708\0.707\0.699\0.692\0.688\0.685\0.682\0.677\0.672\0.669\0.665\0.663\0.662\0.657\0.655\0.653\0.65\0.648\0.647\0.645\0.644\0.643\0.641\0.639\0.638\0.638\0.636\0.635\0.635\0.634\0.634\0.633\0.633\0.632\0.632\0.631\0.631\0.631\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.629\0.63\0.63\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629\0.629</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.74\1.1\0.868\0.824\0.824\0.806\0.802\0.749\0.739\0.724\0.702\0.703\0.704\0.703\0.689\0.686\0.688\0.659\0.654\0.64\0.628\0.631\0.624\0.632\0.62\0.609\0.603\0.602\0.599\0.597\0.598\0.599\0.599\0.599\0.594\0.592\0.587\0.589\0.588\0.591\0.588\0.587\0.585\0.581\0.581\0.582\0.583\0.583\0.583\0.582\0.582\0.58\0.579\0.579\0.579\0.58\0.581\0.579\0.579\0.579\0.579\0.579\0.579\0.579\0.577\0.577\0.578\0.578\0.578\0.578\0.578\0.578\0.578\0.578\0.577\0.577\0.577\0.577\0.577\0.577\0.577\0.576\0.577\0.577\0.577\0.577\0.577\0.577\0.578\0.577\0.577\0.577\0.577\0.577\0.577\0.578\0.578\0.578\0.578\0.578\0.578\0.578\0.577\0.578\0.578\0.577\0.578\0.578\0.578\0.578\0.578\0.578\0.578\0.578\0.578</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>115</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="BzPwj3" Title="Quasi-Newton method results">
   <Caption Id="3miR3c">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.629
0.578
114
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="1NPftP" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="L7gD3F" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="D1nwI9" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="WoBq9F">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00879669\110.376\9.9078\10.4804
5.6389e-5\0.707538\0.0635115\0.0671818
0.0056389\70.7538\6.35115\6.71818</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="DXjnk3" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="5P1pUP">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00257111\316.898\7.17632\14.7107
7.56208e-6\0.932053\0.0211068\0.0432668
0.000756208\93.2053\2.11068\4.32668</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="zkqzni" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="uFm0Wj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0086174\26.8692\5.63588\4.69945
3.47476e-5\0.108343\0.0227253\0.0189494
0.00347476\10.8343\2.27253\1.89494</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="8v67aM" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="oTLxrB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0133896\39.9631\8.42645\5.33222
0.000185966\0.555043\0.117034\0.0740587
0.0185966\55.5043\11.7034\7.40587</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="LMQjlz" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="XKrEvq">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00486565\14.4123\1.02521\1.01116
0.000286214\0.847784\0.0603065\0.05948
0.0286215\84.7784\6.03065\5.948</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="TpAY9j" Title="PM2.5(microg/m3)_ahead_2 errors statistics">
   <Caption Id="KXkOYN">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0123978\107.829\12.5627\12.9065
7.94729e-5\0.69121\0.0805303\0.0827337
0.00794729\69.121\8.05303\8.27337</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="k93kUh" Title="PM10(microg/m3)_ahead_2 errors statistics">
   <Caption Id="6tUP5q">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0230484\326.89\8.4927\15.4034
6.77894e-5\0.961441\0.0249785\0.0453041
0.00677894\96.1441\2.49785\4.53041</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="D8xqPG" Title="O3(microg/m3)_ahead_2 errors statistics">
   <Caption Id="f2bRNi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.000869751\27.299\5.66617\4.62691
3.50706e-6\0.110077\0.0228475\0.0186569
0.000350706\11.0077\2.28475\1.86569</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="pTSE8f" Title="NO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="9REaO5">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00537491\38.3671\8.79277\5.63243
7.46515e-5\0.532877\0.122122\0.0782282
0.00746515\53.2877\12.2122\7.82282</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="oJZD0l" Title="SO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="4UPkoF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00342417\14.9943\1.16744\1.07265
0.000201422\0.882017\0.068673\0.0630969
0.0201422\88.2017\6.8673\6.30969</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ug5eAC" Title="PM2.5(microg/m3)_ahead_3 errors statistics">
   <Caption Id="GB4XbM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.180313\123.761\14.4443\13.9141
0.00115585\0.793341\0.092592\0.0891927
0.115585\79.3341\9.2592\8.91927</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="sfMZy2" Title="PM10(microg/m3)_ahead_3 errors statistics">
   <Caption Id="L7d04I">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00506401\325.533\9.03763\15.5515
1.48941e-5\0.957451\0.0265813\0.0457397
0.00148941\95.7451\2.65813\4.57397</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4ylSwc" Title="O3(microg/m3)_ahead_3 errors statistics">
   <Caption Id="yDvr9L">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.000789642\28.3082\5.67957\4.71299
3.18404e-6\0.114146\0.0229015\0.019004
0.000318404\11.4146\2.29015\1.9004</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="lTjqSH" Title="NO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="P27zps">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0478878\37.7106\8.99251\5.72819
0.000665108\0.523758\0.124896\0.0795581
0.0665108\52.3758\12.4896\7.95581</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="iuM2Hr" Title="SO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="9eD7aF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.000741005\14.9175\1.2363\1.07663
4.35885e-5\0.877502\0.0727238\0.063331
0.00435885\87.7502\7.27238\6.3331</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="3lxpsE" Title="PM2.5(microg/m3)_ahead_4 errors statistics">
   <Caption Id="Jx2Qxr">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0379181\120.917\15.2999\14.2407
0.000243065\0.775109\0.0980762\0.0912865
0.0243065\77.5109\9.80762\9.12865</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="QUX4cc" Title="PM10(microg/m3)_ahead_4 errors statistics">
   <Caption Id="RsXarI">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0117035\325.124\9.31878\15.5136
3.4422e-5\0.956248\0.0274082\0.0456282
0.0034422\95.6248\2.74082\4.56282</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Dd3inK" Title="O3(microg/m3)_ahead_4 errors statistics">
   <Caption Id="qzyURX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0435638\27.1247\5.7202\4.74992
0.000175661\0.109374\0.0230653\0.0191529
0.0175661\10.9374\2.30653\1.91529</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="zBxSj1" Title="NO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="jtk2Ut">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.012785\36.7913\9.15702\5.76976
0.000177569\0.51099\0.127181\0.0801355
0.0177569\51.099\12.7181\8.01355</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="fCGSpz" Title="SO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="CtSvCo">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00873494\14.9236\1.26757\1.06031
0.00051382\0.877857\0.0745631\0.0623715
0.051382\87.7857\7.45631\6.23714</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="yAqPbM" Title="PM2.5(microg/m3)_ahead_5 errors statistics">
   <Caption Id="sDMdvR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00509262\119.671\15.7866\14.3457
3.2645e-5\0.767125\0.101196\0.0919596
0.0032645\76.7125\10.1196\9.19596</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ePbTUo" Title="PM10(microg/m3)_ahead_5 errors statistics">
   <Caption Id="obgg5y">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.000873566\314.604\9.32535\15.142
2.56931e-6\0.925306\0.0274275\0.0445352
0.000256931\92.5306\2.74275\4.45352</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="naC16X" Title="O3(microg/m3)_ahead_5 errors statistics">
   <Caption Id="E9Dopv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>9.53674e-6\28.6064\5.6521\4.74408
3.84546e-8\0.115348\0.0227907\0.0191294
3.84546e-6\11.5348\2.27907\1.91294</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Kiu3L2" Title="NO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="zdAbqR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0378742\36.5578\9.25755\5.87518
0.000526031\0.507747\0.128577\0.0815998
0.0526031\50.7747\12.8577\8.15998</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ZgxPbn" Title="SO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="r3Zm8V">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00408602\14.7684\1.33374\1.05947
0.000240354\0.868732\0.0784553\0.0623215
0.0240354\86.8732\7.84553\6.23215</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="p3zLtM" Title="PM2.5(microg/m3)_ahead_6 errors statistics">
   <Caption Id="z5hfpl">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.000656128\103.009\15.7874\14.1666
4.20595e-6\0.660313\0.101202\0.0908114
0.000420595\66.0313\10.1202\9.08114</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ArQSGm" Title="PM10(microg/m3)_ahead_6 errors statistics">
   <Caption Id="rfi3sf">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0547657\322.649\9.41371\15.3899
0.000161076\0.948967\0.0276874\0.0452645
0.0161076\94.8967\2.76874\4.52645</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="VUmqxY" Title="O3(microg/m3)_ahead_6 errors statistics">
   <Caption Id="DGON0r">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00465775\31.1596\5.63157\4.73513
1.87812e-5\0.125644\0.022708\0.0190933
0.00187812\12.5644\2.2708\1.90933</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="fVyXF3" Title="NO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="hntavv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0295372\37.0446\9.32799\5.93939
0.000410239\0.514508\0.129555\0.0824916
0.0410239\51.4508\12.9555\8.24916</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="UTIHXq" Title="SO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="QNzYFj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00704288\14.5628\1.37237\1.06239
0.000414287\0.856635\0.0807275\0.0624934
0.0414287\85.6635\8.07275\6.24934</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="iQdwda" Title="PM2.5(microg/m3)_ahead_7 errors statistics">
   <Caption Id="35ztNd">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0031929\116.233\15.9322\14.2808
2.04673e-5\0.745085\0.10213\0.0915438
0.00204673\74.5085\10.213\9.15438</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="B1k7uA" Title="PM10(microg/m3)_ahead_7 errors statistics">
   <Caption Id="dCwu2t">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.0173607\322.757\9.45978\15.4263
5.10608e-5\0.949285\0.0278229\0.0453715
0.00510608\94.9285\2.78229\4.53715</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uywBHU" Title="O3(microg/m3)_ahead_7 errors statistics">
   <Caption Id="odsrVE">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00435638\30.4091\5.67713\4.80998
1.75661e-5\0.122617\0.0228916\0.0193951
0.00175661\12.2617\2.28916\1.93951</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="S6Zmfk" Title="NO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="HJFRuW">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>0.00973892\36.1167\9.3317\5.89604
0.000135263\0.501621\0.129607\0.0818894
0.0135263\50.1621\12.9607\8.18894</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="PAebqd" Title="SO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="6S3uhs">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391432 and its percentage error 3.91417</Caption>
   <Data>2.15769e-5\14.288\1.38343\1.04897
1.26923e-6\0.840471\0.081378\0.0617042
0.000126923\84.0471\8.1378\6.17042</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="yeTB6b" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="v1e9MW" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="S6oIbc" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="Z2dFuf" Title="Quasi-Newton method errors history">
   <Caption Id="whlMh2">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.1402, and the final value after 141 epochs is 0.668029.
The initial value of the selection error is 1.93219, and the final value after 141 epochs is 0.793397.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141</X2Data>
   <Y1Data>2.14\1.14\0.968\0.936\0.913\0.89\0.867\0.836\0.825\0.813\0.807\0.803\0.791\0.781\0.777\0.768\0.757\0.751\0.748\0.744\0.74\0.738\0.735\0.732\0.731\0.729\0.727\0.726\0.722\0.722\0.72\0.717\0.716\0.713\0.71\0.707\0.706\0.704\0.702\0.701\0.699\0.697\0.696\0.695\0.694\0.693\0.692\0.692\0.691\0.691\0.69\0.689\0.689\0.688\0.687\0.687\0.686\0.685\0.684\0.683\0.682\0.682\0.681\0.681\0.68\0.68\0.679\0.679\0.679\0.678\0.677\0.676\0.676\0.675\0.675\0.674\0.674\0.674\0.673\0.673\0.673\0.672\0.672\0.672\0.672\0.671\0.671\0.671\0.671\0.67\0.67\0.67\0.669\0.669\0.669\0.669\0.669\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.93\1.2\0.978\0.972\0.976\0.951\0.917\0.905\0.9\0.892\0.886\0.876\0.862\0.856\0.865\0.874\0.868\0.853\0.863\0.857\0.855\0.85\0.851\0.866\0.853\0.851\0.842\0.837\0.846\0.846\0.838\0.835\0.831\0.83\0.831\0.839\0.837\0.827\0.819\0.814\0.819\0.821\0.822\0.825\0.826\0.83\0.829\0.826\0.829\0.833\0.827\0.825\0.824\0.819\0.819\0.823\0.817\0.816\0.817\0.814\0.817\0.815\0.817\0.816\0.814\0.812\0.811\0.813\0.81\0.807\0.809\0.812\0.812\0.814\0.815\0.815\0.813\0.812\0.816\0.816\0.817\0.817\0.817\0.816\0.815\0.815\0.813\0.814\0.813\0.809\0.808\0.807\0.807\0.805\0.804\0.804\0.8\0.801\0.801\0.802\0.799\0.798\0.799\0.799\0.798\0.798\0.797\0.797\0.797\0.796\0.796\0.796\0.796\0.796\0.795\0.795\0.795\0.795\0.795\0.795\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.793\0.794\0.794\0.793\0.793\0.793\0.793</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>142</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="bYnr0v" Title="Quasi-Newton method results">
   <Caption Id="Za813l">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.668
0.793
141
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="5rDupG" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="KalLaw" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="ZLe4fS" Title="PM2.5(microg/m3)_ahead_3 errors statistics">
   <Caption Id="crM7kx">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0787188 and its percentage error 7.87184</Caption>
   <Data>0.0251541\120.468\14.6892\13.6264
0.000161244\0.772231\0.0941615\0.0873488
0.0161244\77.2231\9.41615\8.73489</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="UudJ36" Title="PM2.5(microg/m3)_ahead_5 errors statistics">
   <Caption Id="Prf0QC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0787188 and its percentage error 7.87184</Caption>
   <Data>0.0268517\118.214\16.041\14.6321
0.000172126\0.757782\0.102827\0.0937958
0.0172126\75.7782\10.2827\9.37958</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="bsIuGo" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="bmPLZg" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 101.</Text>
 </Task>
 <Task Id="pQnScg" Title="Inputs-targets correlations" Component="Data set" Name="Calculate inputs-targets correlations">
  <Text Id="HRyzss" Title="Task description">It might be interesting to look for dependencies between single inputs and single targets in the data set.
This task calculates the correlation coefficient values between all inputs and all targets.
Correlations close to 1 mean a strong relationship between input and target variables.
Correlations close to 0 mean no relationship between that variables.
Note that, in general, the targets depend on many inputs simultaneously.
</Text>
  <BarsChart Id="gybULQ" Title="PM2.5(microg/m3)_ahead_3 correlations chart">
   <Caption Id="XXFmm6">The following chart illustrates the target 'PM2.5(microg/m3)_ahead_3' dependency with the 16 input columns with greatest correlation in the data set.
 </Caption>
   <Data>-0.226\-0.177\0.143\0.151\0.157\0.158\0.159\0.164\0.182\0.188\0.227\0.252\0.304\0.32\0.355\0.427</Data>
   <XTitle>Correlation</XTitle>
   <Names>WINDSPEED(km/h)_lag_0\WINDSPEED(km/h)_lag_1\SO2(microg/m3)_lag_1\TMAX(C)_lag_1\MONTH_lag_0\NO2(microg/m3)_lag_1\MONTH_lag_1\TMAX(C)_lag_0\PRESSURE(hPa)_lag_1\SO2(microg/m3)_lag_0\PRESSURE(hPa)_lag_0\NO2(microg/m3)_lag_0\PM2.5(microg/m3)_lag_1\PM10(microg/m3)_lag_1\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_0</Names>
   <Minimum>-1</Minimum>
   <Maximum>1</Maximum>
  </BarsChart>
  <Table Id="hUwsVQ" Title="PM2.5(microg/m3)_ahead_3 correlations table">
   <Caption Id="UIWXLD">The following table shows the value of all the correlations between input and target variables.
</Caption>
   <Data>power\0.427246
power\0.354514
power\0.320052
logarithmic\0.304339
linear\0.251839
linear\0.227495
linear\0.187773
linear\0.182152
exponential\0.164024
linear\0.159473
linear\0.158406
linear\0.157062
exponential\0.150705
linear\0.142786
exponential\0.119208
exponential\0.118684
exponential\0.088059
exponential\0.079393
power\0.049542
linear\0.044345
power\0.037928
power\-0.025081
logarithmic\-0.082388
linear\-0.095446
power\-0.100066
power\-0.102799
exponential\-0.113049
logarithmic\-0.117975
linear\-0.177411
exponential\-0.226136</Data>
   <RowsName>PM10(microg/m3)_lag_0\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_1\PM2.5(microg/m3)_lag_1\NO2(microg/m3)_lag_0\PRESSURE(hPa)_lag_0\SO2(microg/m3)_lag_0\PRESSURE(hPa)_lag_1\TMAX(C)_lag_0\MONTH_lag_1\NO2(microg/m3)_lag_1\MONTH_lag_0\TMAX(C)_lag_1\SO2(microg/m3)_lag_1\TAVG(C)_lag_0\TAVG(C)_lag_1\TMIN(C)_lag_1\TMIN(C)_lag_0\DAY_lag_0\WEEKDAY_lag_1\DAY_lag_1\WEEKDAY_lag_0\O3(microg/m3)_lag_1\PRECIPITATIONS(mm)_lag_1\HUMIDITY(percentage)_lag_0\HUMIDITY(percentage)_lag_1\PRECIPITATIONS(mm)_lag_0\O3(microg/m3)_lag_0\WINDSPEED(km/h)_lag_1\WINDSPEED(km/h)_lag_0</RowsName>
   <ColumnsName>type\PM2.5(microg/m3)_ahead_3</ColumnsName>
   <RowHeadingsWidth>38</RowHeadingsWidth>
   <ColumnHeadingsWidth>18</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <BarsChart Id="wb5YSu" Title="PM2.5(microg/m3)_ahead_5 correlations chart">
   <Caption Id="JWGGay">The following chart illustrates the target 'PM2.5(microg/m3)_ahead_5' dependency with the 16 input columns with greatest correlation in the data set.
 </Caption>
   <Data>-0.147\0.108\0.115\0.119\0.12\0.121\0.124\0.133\0.144\0.153\0.16\0.164\0.231\0.232\0.261\0.268</Data>
   <XTitle>Correlation</XTitle>
   <Names>WINDSPEED(km/h)_lag_0\NO2(microg/m3)_lag_0\PRESSURE(hPa)_lag_1\TAVG(C)_lag_1\SO2(microg/m3)_lag_1\TAVG(C)_lag_0\SO2(microg/m3)_lag_0\TMAX(C)_lag_1\TMAX(C)_lag_0\PRESSURE(hPa)_lag_0\MONTH_lag_0\MONTH_lag_1\PM2.5(microg/m3)_lag_1\PM10(microg/m3)_lag_1\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_0</Names>
   <Minimum>-1</Minimum>
   <Maximum>1</Maximum>
  </BarsChart>
  <Table Id="QER23A" Title="PM2.5(microg/m3)_ahead_5 correlations table">
   <Caption Id="nMktLJ">The following table shows the value of all the correlations between input and target variables.
</Caption>
   <Data>power\0.267842
linear\0.261282
power\0.232307
linear\0.231153
linear\0.163998
linear\0.160279
linear\0.152967
exponential\0.143813
exponential\0.133121
linear\0.124320
exponential\0.121279
linear\0.119977
exponential\0.119105
linear\0.114684
linear\0.108399
exponential\0.106673
exponential\0.097595
linear\0.087685
exponential\0.075688
power\0.065863
power\0.014454
exponential\-0.014423
logarithmic\-0.055621
logarithmic\-0.058495
exponential\-0.067605
power\-0.079162
exponential\-0.089721
power\-0.090938
linear\-0.103256
linear\-0.146829</Data>
   <RowsName>PM10(microg/m3)_lag_0\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_1\PM2.5(microg/m3)_lag_1\MONTH_lag_1\MONTH_lag_0\PRESSURE(hPa)_lag_0\TMAX(C)_lag_0\TMAX(C)_lag_1\SO2(microg/m3)_lag_0\TAVG(C)_lag_0\SO2(microg/m3)_lag_1\TAVG(C)_lag_1\PRESSURE(hPa)_lag_1\NO2(microg/m3)_lag_0\TMIN(C)_lag_1\TMIN(C)_lag_0\NO2(microg/m3)_lag_1\WEEKDAY_lag_0\WEEKDAY_lag_1\DAY_lag_0\DAY_lag_1\O3(microg/m3)_lag_1\O3(microg/m3)_lag_0\PRECIPITATIONS(mm)_lag_1\HUMIDITY(percentage)_lag_1\PRECIPITATIONS(mm)_lag_0\HUMIDITY(percentage)_lag_0\WINDSPEED(km/h)_lag_1\WINDSPEED(km/h)_lag_0</RowsName>
   <ColumnsName>type\PM2.5(microg/m3)_ahead_5</ColumnsName>
   <RowHeadingsWidth>38</RowHeadingsWidth>
   <ColumnHeadingsWidth>18</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="IFa2Ri" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="LC2qJk" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="5jNiUb" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="rvJ3Dt" Title="Quasi-Newton method errors history">
   <Caption Id="AmEigh">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.27425, and the final value after 132 epochs is 0.376647.
The initial value of the selection error is 2.07483, and the final value after 132 epochs is 0.363038.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132</X2Data>
   <Y1Data>2.27\0.865\0.821\0.794\0.77\0.744\0.719\0.698\0.686\0.68\0.672\0.664\0.651\0.64\0.624\0.615\0.603\0.597\0.589\0.578\0.569\0.559\0.551\0.546\0.541\0.535\0.531\0.521\0.514\0.508\0.503\0.496\0.484\0.479\0.469\0.463\0.459\0.453\0.448\0.443\0.439\0.436\0.431\0.427\0.424\0.419\0.416\0.411\0.407\0.404\0.401\0.398\0.396\0.393\0.391\0.389\0.388\0.386\0.385\0.384\0.384\0.383\0.383\0.382\0.381\0.381\0.38\0.38\0.379\0.379\0.379\0.378\0.378\0.378\0.378\0.378\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.376\0.376\0.376\0.376\0.376\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.07\0.768\0.808\0.745\0.684\0.678\0.656\0.623\0.599\0.592\0.596\0.602\0.569\0.574\0.578\0.57\0.545\0.542\0.553\0.572\0.544\0.532\0.527\0.524\0.52\0.518\0.516\0.505\0.502\0.504\0.502\0.482\0.462\0.456\0.46\0.458\0.45\0.444\0.434\0.424\0.421\0.42\0.419\0.418\0.412\0.402\0.395\0.389\0.392\0.394\0.397\0.39\0.38\0.376\0.376\0.376\0.375\0.374\0.374\0.373\0.371\0.369\0.365\0.363\0.363\0.364\0.365\0.367\0.368\0.367\0.365\0.364\0.363\0.364\0.365\0.366\0.365\0.364\0.363\0.363\0.362\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>133</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="WyBDSd" Title="Quasi-Newton method results">
   <Caption Id="62t0wS">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.377
0.363
132
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="SoTL1i" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="x4xMAs" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="Iq99ad" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="RBA5im">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0284233\113.87\8.22188\9.18554
0.000182201\0.729936\0.0527043\0.0588817
0.0182201\72.9936\5.27043\5.88817</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="FMmPhZ" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="jgE8KY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0177402\308.621\6.84973\14.2817
5.21772e-5\0.907708\0.0201463\0.0420051
0.00521772\90.7708\2.01463\4.20051</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ZWSHoG" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Y1bExT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0263824\25.7432\5.61804\4.68497
0.000106381\0.103803\0.0226534\0.018891
0.0106381\10.3803\2.26534\1.8891</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="pKfsXX" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ayOnIv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0116062\42.5975\7.61535\5.07006
0.000161197\0.591632\0.105769\0.0704175
0.0161197\59.1632\10.5769\7.04175</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="7cr646" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="l6vjrM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000118256\14.4339\0.696653\0.972976
6.95621e-6\0.849055\0.0409796\0.0572339
0.000695621\84.9055\4.09796\5.72339</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Af4LZg" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="laBlNy" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 158.</Text>
 </Task>
 <Task Id="mJZ6vP" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="9VkC5c" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="iV2pTX" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="fMDoTZ" Title="Quasi-Newton method errors history">
   <Caption Id="ABChbf">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.70354, and the final value after 124 epochs is 0.463077.
The initial value of the selection error is 1.52463, and the final value after 124 epochs is 0.446997.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124</X2Data>
   <Y1Data>1.7\1.01\0.919\0.866\0.817\0.79\0.765\0.748\0.719\0.705\0.694\0.683\0.677\0.674\0.668\0.66\0.646\0.639\0.636\0.628\0.621\0.615\0.601\0.595\0.59\0.58\0.571\0.563\0.556\0.551\0.546\0.544\0.538\0.533\0.528\0.525\0.521\0.516\0.512\0.507\0.505\0.501\0.498\0.496\0.493\0.49\0.488\0.485\0.483\0.481\0.479\0.478\0.477\0.476\0.475\0.474\0.473\0.472\0.471\0.47\0.469\0.469\0.468\0.467\0.467\0.466\0.466\0.466\0.465\0.465\0.465\0.464\0.464\0.464\0.464\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463\0.463</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.52\0.948\0.79\0.76\0.73\0.706\0.681\0.683\0.627\0.616\0.604\0.602\0.604\0.595\0.588\0.577\0.576\0.574\0.58\0.582\0.574\0.566\0.533\0.523\0.519\0.521\0.53\0.531\0.521\0.511\0.508\0.512\0.515\0.514\0.506\0.501\0.494\0.486\0.487\0.487\0.486\0.481\0.471\0.466\0.465\0.461\0.46\0.456\0.455\0.456\0.453\0.451\0.451\0.452\0.452\0.454\0.453\0.452\0.45\0.449\0.449\0.448\0.448\0.447\0.447\0.447\0.447\0.447\0.447\0.448\0.448\0.448\0.449\0.448\0.448\0.447\0.447\0.447\0.447\0.447\0.447\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.446\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447\0.447</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>125</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="xSFRJO" Title="Quasi-Newton method results">
   <Caption Id="myzhZ6">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.463
0.447
124
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="wsvQUP" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="FrBEvk" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="u6oGZ3" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Fc4rxz">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00595856\121.347\8.08341\9.67431
3.81959e-5\0.777862\0.0518167\0.0620148
0.00381959\77.7862\5.18167\6.20148</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="jlEsc0" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9AotCJ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0157852\307.564\6.89595\14.28
4.64271e-5\0.904599\0.0202822\0.0420001
0.00464271\90.4599\2.02822\4.20001</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="FvYyRO" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QWTbpT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0349922\36.2728\10.7229\6.4371
0.000141098\0.146261\0.0432375\0.0259561
0.0141098\14.6261\4.32375\2.59561</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uQzCSw" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="EEIJLV">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00860786\46.3009\6.94792\4.71604
0.000119554\0.643069\0.0964988\0.0655006
0.0119554\64.3069\9.64988\6.55006</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="5QVnDH" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="JXPa3I">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00404191\10.706\1.62851\1.11435
0.000237759\0.629766\0.0957948\0.0655498
0.0237759\62.9766\9.57948\6.55498</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="MIGKfL" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="y02qXQ" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="62CL00" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="VnGt0L" Title="Quasi-Newton method errors history">
   <Caption Id="Jf053d">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.28916, and the final value after 153 epochs is 0.34355.
The initial value of the selection error is 2.04732, and the final value after 153 epochs is 0.276237.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153</X2Data>
   <Y1Data>2.29\0.989\0.787\0.737\0.711\0.694\0.671\0.657\0.644\0.635\0.622\0.61\0.601\0.588\0.575\0.561\0.55\0.545\0.53\0.521\0.517\0.509\0.502\0.496\0.49\0.483\0.476\0.472\0.466\0.46\0.457\0.453\0.45\0.446\0.441\0.437\0.433\0.43\0.426\0.423\0.418\0.415\0.411\0.408\0.405\0.403\0.399\0.396\0.394\0.391\0.388\0.386\0.384\0.381\0.378\0.376\0.373\0.371\0.367\0.365\0.362\0.36\0.358\0.356\0.355\0.354\0.353\0.352\0.351\0.35\0.35\0.349\0.348\0.348\0.347\0.347\0.347\0.346\0.346\0.346\0.345\0.345\0.345\0.345\0.345\0.345\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.05\0.934\0.693\0.697\0.648\0.606\0.612\0.613\0.596\0.592\0.604\0.563\0.556\0.55\0.535\0.537\0.511\0.49\0.464\0.464\0.461\0.46\0.457\0.469\0.474\0.485\0.476\0.483\0.48\0.486\0.474\0.467\0.46\0.45\0.453\0.454\0.457\0.455\0.445\0.435\0.425\0.423\0.417\0.411\0.409\0.405\0.4\0.396\0.394\0.392\0.388\0.378\0.373\0.363\0.353\0.35\0.349\0.347\0.334\0.326\0.321\0.315\0.311\0.307\0.304\0.302\0.295\0.295\0.292\0.291\0.291\0.288\0.284\0.285\0.284\0.283\0.282\0.281\0.28\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>154</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="pAX3MV" Title="Quasi-Newton method results">
   <Caption Id="shBGsG">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.344
0.276
153
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Wr8eEp" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="aMxMNY" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="IdymcV" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Z37wVF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00626373\117.372\6.81197\8.44576
4.01521e-5\0.752387\0.0436665\0.0541395
0.00401521\75.2386\4.36665\5.41395</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="OF1maz" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="2t6zFP">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00729752\309.38\6.80162\14.3101
2.14633e-5\0.909941\0.0200048\0.0420885
0.00214633\90.9941\2.00048\4.20885</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="LAkYkY" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="2HYk1w">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0347805\24.2905\4.83973\3.99611
0.000140244\0.0979456\0.019515\0.0161134
0.0140244\9.79456\1.9515\1.61134</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="MK5Cxs" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="1dru5G">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0152569\46.7041\5.70668\4.29055
0.000211901\0.648668\0.0792594\0.059591
0.0211901\64.8668\7.92594\5.9591</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="OnemBk" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="etYcPi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00429058\14.1993\0.887499\0.978757
0.000252387\0.835251\0.0522058\0.057574
0.0252387\83.5251\5.22058\5.7574</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="DeARR3" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="qaBbFj" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="MGSeFR" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="AoRCdF" Title="Quasi-Newton method errors history">
   <Caption Id="PunOyG">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.49764, and the final value after 219 epochs is 0.0930912.
The initial value of the selection error is 1.25528, and the final value after 219 epochs is 0.181244.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219</X2Data>
   <Y1Data>1.5\0.791\0.693\0.654\0.573\0.54\0.499\0.398\0.383\0.368\0.352\0.325\0.313\0.299\0.295\0.278\0.257\0.248\0.237\0.231\0.229\0.224\0.219\0.207\0.196\0.193\0.188\0.184\0.175\0.172\0.169\0.164\0.161\0.155\0.151\0.147\0.144\0.142\0.141\0.139\0.138\0.137\0.135\0.133\0.132\0.13\0.129\0.128\0.127\0.126\0.125\0.125\0.124\0.122\0.121\0.121\0.119\0.118\0.117\0.117\0.116\0.115\0.115\0.114\0.113\0.112\0.112\0.111\0.11\0.11\0.11\0.109\0.109\0.108\0.108\0.108\0.107\0.107\0.106\0.106\0.105\0.105\0.104\0.104\0.103\0.103\0.102\0.102\0.101\0.101\0.101\0.1\0.0997\0.0994\0.0989\0.0986\0.0985\0.0981\0.0979\0.0976\0.0974\0.0972\0.0971\0.0969\0.0967\0.0965\0.0962\0.096\0.0959\0.0958\0.0956\0.0955\0.0953\0.0952\0.095\0.0949\0.0948\0.0947\0.0946\0.0945\0.0944\0.0943\0.0943\0.0941\0.094\0.094\0.0939\0.0939\0.0938\0.0938\0.0938\0.0938\0.0938\0.0937\0.0938\0.0938\0.0938\0.0938\0.0938\0.0938\0.0938\0.0938\0.0938\0.0938\0.0938\0.0938\0.0938\0.0938\0.0938\0.0937\0.0937\0.0937\0.0936\0.0936\0.0936\0.0936\0.0936\0.0935\0.0935\0.0935\0.0934\0.0934\0.0933\0.0932\0.0931\0.0932\0.0931\0.0931\0.093\0.093\0.0931\0.093\0.093\0.093\0.093\0.093\0.093\0.0931\0.0931\0.0931\0.093\0.093\0.093\0.093\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931\0.0931</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.26\0.835\0.734\0.677\0.617\0.618\0.602\0.476\0.439\0.401\0.404\0.397\0.392\0.42\0.425\0.365\0.342\0.354\0.322\0.311\0.32\0.315\0.3\0.303\0.31\0.304\0.294\0.304\0.301\0.285\0.273\0.255\0.247\0.229\0.23\0.211\0.205\0.205\0.206\0.205\0.213\0.213\0.204\0.202\0.193\0.197\0.195\0.202\0.204\0.211\0.208\0.208\0.203\0.206\0.195\0.194\0.192\0.193\0.193\0.19\0.192\0.191\0.185\0.187\0.195\0.198\0.196\0.191\0.191\0.192\0.196\0.195\0.196\0.195\0.193\0.191\0.19\0.188\0.193\0.194\0.193\0.2\0.204\0.203\0.197\0.194\0.192\0.194\0.193\0.192\0.192\0.192\0.193\0.192\0.191\0.191\0.193\0.193\0.192\0.193\0.191\0.191\0.191\0.189\0.188\0.186\0.186\0.185\0.186\0.184\0.183\0.185\0.184\0.185\0.184\0.185\0.184\0.184\0.183\0.184\0.184\0.184\0.184\0.185\0.184\0.184\0.184\0.184\0.184\0.184\0.184\0.185\0.185\0.185\0.186\0.185\0.185\0.185\0.185\0.184\0.184\0.184\0.184\0.183\0.183\0.183\0.183\0.183\0.184\0.184\0.184\0.183\0.184\0.184\0.184\0.183\0.184\0.183\0.183\0.183\0.183\0.183\0.182\0.181\0.181\0.182\0.181\0.182\0.183\0.182\0.183\0.182\0.182\0.182\0.182\0.182\0.182\0.182\0.181\0.182\0.182\0.182\0.182\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181\0.181</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>220</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="9wSXrL" Title="Quasi-Newton method results">
   <Caption Id="4EwrJA">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0931
0.181
219
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="XmPZaX" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="XJX9v0" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="ly1yxS" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="LKco72">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0176105\121.37\6.56734\8.78356
0.000112888\0.778013\0.0420984\0.0563048
0.0112888\77.8013\4.20984\5.63048</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="4d8B8o" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="t39Scb" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="o78FIU" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="dNrgp6" Title="Quasi-Newton method errors history">
   <Caption Id="ipdb0o">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.0930911, and the final value after 1 epochs is 0.0930911.
The initial value of the selection error is 0.181242, and the final value after 1 epochs is 0.181243.
</Caption>
   <X1Data>0\1</X1Data>
   <X2Data>0\1</X2Data>
   <Y1Data>0.0931\0.0931</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.181\0.181</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>2</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="PU8ITV" Title="Quasi-Newton method results">
   <Caption Id="UGg8sQ">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0931
0.181
1
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="KPFiRy" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="NVhAg9" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="exGvyq" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="HSQ7fH">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0176392\121.37\6.56734\8.78356
0.000113072\0.778012\0.0420984\0.0563048
0.0113072\77.8012\4.20984\5.63049</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="4tUCx8" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="f1VRI6" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="acFMSF" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="9J3C1f" Title="Quasi-Newton method errors history">
   <Caption Id="BCxRmf">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.3642, and the final value after 194 epochs is 0.322857.
The initial value of the selection error is 2.07074, and the final value after 194 epochs is 0.308876.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194</X2Data>
   <Y1Data>2.36\0.879\0.731\0.693\0.622\0.584\0.571\0.555\0.541\0.525\0.493\0.486\0.478\0.476\0.471\0.459\0.447\0.443\0.438\0.433\0.427\0.421\0.416\0.409\0.405\0.402\0.399\0.395\0.392\0.389\0.385\0.382\0.38\0.377\0.376\0.374\0.371\0.37\0.367\0.365\0.363\0.362\0.36\0.359\0.357\0.356\0.355\0.355\0.353\0.352\0.351\0.349\0.348\0.348\0.347\0.346\0.346\0.345\0.344\0.343\0.343\0.342\0.341\0.34\0.34\0.339\0.338\0.338\0.337\0.336\0.336\0.336\0.336\0.335\0.335\0.334\0.334\0.334\0.333\0.333\0.333\0.333\0.333\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.327\0.327\0.327\0.327\0.327\0.327\0.326\0.327\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.07\0.681\0.58\0.53\0.503\0.408\0.42\0.412\0.38\0.373\0.348\0.333\0.329\0.329\0.336\0.327\0.325\0.316\0.31\0.303\0.31\0.304\0.309\0.311\0.313\0.309\0.31\0.308\0.309\0.318\0.33\0.329\0.32\0.317\0.325\0.32\0.325\0.325\0.319\0.317\0.315\0.31\0.302\0.304\0.295\0.289\0.292\0.291\0.29\0.294\0.288\0.29\0.287\0.286\0.285\0.29\0.292\0.29\0.293\0.298\0.299\0.302\0.3\0.308\0.306\0.302\0.302\0.303\0.299\0.298\0.301\0.3\0.3\0.298\0.298\0.297\0.299\0.299\0.301\0.298\0.3\0.299\0.301\0.302\0.299\0.299\0.299\0.298\0.299\0.299\0.301\0.3\0.299\0.299\0.299\0.298\0.298\0.298\0.299\0.301\0.301\0.302\0.3\0.302\0.302\0.301\0.302\0.303\0.302\0.305\0.307\0.308\0.307\0.309\0.309\0.308\0.309\0.311\0.31\0.312\0.313\0.315\0.316\0.315\0.315\0.313\0.315\0.315\0.315\0.315\0.314\0.315\0.316\0.314\0.315\0.313\0.312\0.312\0.312\0.312\0.312\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.31\0.31\0.309\0.31\0.309\0.309\0.31\0.309\0.309\0.309\0.31\0.31\0.31\0.31\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.309\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>195</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="1gCDD9" Title="Quasi-Newton method results">
   <Caption Id="RltOFK">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.323
0.309
194
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="71Rp9H" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="elfGZI" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="SyIcAC" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="BhsPLN">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228121 and its percentage error 2.28121</Caption>
   <Data>0.00164413\301.315\7.14314\14.0338
4.83569e-6\0.886219\0.0210092\0.0412758
0.000483569\88.6219\2.10092\4.12758</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="E01YBG" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="qJduTD" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="MhlWun" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="kzHnNi" Title="Quasi-Newton method errors history">
   <Caption Id="hYfoKC">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.90828, and the final value after 178 epochs is 0.364033.
The initial value of the selection error is 1.4613, and the final value after 178 epochs is 0.285114.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178</X2Data>
   <Y1Data>1.91\0.807\0.797\0.72\0.695\0.608\0.581\0.565\0.55\0.526\0.501\0.494\0.493\0.477\0.466\0.458\0.455\0.452\0.448\0.444\0.439\0.435\0.43\0.425\0.419\0.415\0.409\0.407\0.404\0.402\0.401\0.399\0.397\0.395\0.393\0.39\0.389\0.387\0.385\0.382\0.38\0.379\0.377\0.376\0.375\0.374\0.373\0.372\0.371\0.371\0.37\0.369\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.366\0.366\0.367\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.46\0.582\0.584\0.528\0.534\0.442\0.438\0.41\0.37\0.334\0.32\0.328\0.329\0.323\0.314\0.314\0.312\0.32\0.306\0.299\0.31\0.315\0.323\0.339\0.343\0.344\0.312\0.311\0.31\0.307\0.31\0.311\0.305\0.303\0.299\0.296\0.3\0.293\0.292\0.288\0.282\0.275\0.276\0.279\0.28\0.281\0.281\0.28\0.277\0.278\0.281\0.284\0.285\0.279\0.276\0.276\0.274\0.274\0.274\0.274\0.273\0.273\0.271\0.271\0.271\0.271\0.271\0.27\0.271\0.27\0.269\0.269\0.267\0.266\0.266\0.265\0.265\0.265\0.264\0.264\0.264\0.265\0.264\0.264\0.264\0.265\0.264\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.264\0.264\0.264\0.265\0.265\0.264\0.264\0.264\0.264\0.266\0.267\0.267\0.269\0.271\0.272\0.271\0.272\0.272\0.272\0.273\0.273\0.274\0.275\0.274\0.276\0.275\0.276\0.276\0.276\0.277\0.276\0.277\0.278\0.278\0.278\0.278\0.278\0.278\0.279\0.279\0.279\0.279\0.279\0.278\0.279\0.279\0.279\0.279\0.279\0.279\0.28\0.28\0.28\0.28\0.28\0.28\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.283\0.284\0.284\0.284\0.284\0.284\0.284\0.285</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>179</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="SnsUaq" Title="Quasi-Newton method results">
   <Caption Id="rFrCKu">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.364
0.285
178
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="X8GCKl" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="31ySAU" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="lPKrcp" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="OVyOr2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228121 and its percentage error 2.28121</Caption>
   <Data>0.0119915\306.657\6.78112\14.0847
3.52691e-5\0.901932\0.0199445\0.0414255
0.00352691\90.1932\1.99445\4.14255</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="e7LQsk" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="EYR64P" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="CeIkMw" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="yHumNM" Title="Quasi-Newton method errors history">
   <Caption Id="rpu0yn">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.22959, and the final value after 74 epochs is 0.374528.
The initial value of the selection error is 1.02849, and the final value after 74 epochs is 0.273385.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74</X2Data>
   <Y1Data>1.23\0.818\0.8\0.685\0.654\0.65\0.62\0.608\0.598\0.576\0.558\0.55\0.541\0.535\0.519\0.517\0.495\0.481\0.472\0.469\0.466\0.454\0.451\0.445\0.438\0.432\0.422\0.414\0.413\0.41\0.407\0.404\0.402\0.401\0.398\0.395\0.393\0.391\0.39\0.388\0.386\0.384\0.382\0.381\0.381\0.379\0.378\0.378\0.377\0.376\0.376\0.375\0.375\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.03\0.569\0.601\0.581\0.511\0.478\0.417\0.422\0.449\0.404\0.382\0.354\0.363\0.372\0.348\0.353\0.338\0.341\0.33\0.326\0.321\0.32\0.324\0.317\0.314\0.309\0.317\0.307\0.312\0.309\0.303\0.304\0.295\0.294\0.293\0.293\0.298\0.294\0.29\0.289\0.283\0.275\0.279\0.279\0.28\0.278\0.279\0.277\0.274\0.274\0.275\0.277\0.275\0.274\0.273\0.273\0.272\0.273\0.272\0.273\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>75</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="IDQuWb" Title="Quasi-Newton method results">
   <Caption Id="4VsiAb">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.375
0.273
74
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="BBZzy3" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="jv1WDV" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="NpZ7Bf" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Fy373i">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228121 and its percentage error 2.28121</Caption>
   <Data>0.00234413\304.571\6.80378\14.0518
6.8945e-6\0.895796\0.0200111\0.0413288
0.00068945\89.5796\2.00111\4.13288</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="7amOR2" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="J7Y8HI" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="axk7pl" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="E08S49" Title="Quasi-Newton method errors history">
   <Caption Id="3xqPGB">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.95942, and the final value after 77 epochs is 0.132019.
The initial value of the selection error is 1.16592, and the final value after 77 epochs is 0.168234.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77</X2Data>
   <Y1Data>1.96\0.919\0.691\0.664\0.63\0.566\0.559\0.444\0.427\0.418\0.411\0.404\0.398\0.38\0.364\0.347\0.34\0.331\0.315\0.299\0.27\0.254\0.24\0.23\0.216\0.205\0.199\0.192\0.189\0.184\0.175\0.17\0.167\0.164\0.16\0.157\0.154\0.152\0.15\0.146\0.144\0.141\0.14\0.138\0.137\0.136\0.136\0.136\0.135\0.135\0.135\0.135\0.135\0.134\0.134\0.133\0.133\0.133\0.133\0.133\0.133\0.132\0.132\0.132\0.132\0.132\0.132\0.132\0.132\0.132\0.132\0.132\0.132\0.132\0.132\0.132\0.132\0.132</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.17\0.882\0.538\0.518\0.535\0.509\0.513\0.487\0.509\0.498\0.468\0.437\0.43\0.412\0.413\0.403\0.385\0.366\0.361\0.357\0.315\0.281\0.27\0.269\0.256\0.242\0.242\0.234\0.22\0.203\0.197\0.185\0.183\0.182\0.181\0.195\0.2\0.207\0.206\0.192\0.185\0.184\0.185\0.185\0.176\0.173\0.17\0.169\0.168\0.17\0.173\0.173\0.171\0.17\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.168\0.168\0.169\0.168\0.168\0.168\0.168\0.168\0.168</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>78</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="0YBdOW" Title="Quasi-Newton method results">
   <Caption Id="RvlJ9O">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.132
0.168
77
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Rj1Wa3" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="zpugxY" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="0fJSXI" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="sWp4ZS">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0098114\118.545\6.48101\8.42602
6.28936e-5\0.759905\0.041545\0.054013
0.00628936\75.9905\4.1545\5.4013</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="EZscT2" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="U81nBD" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="1gE1ZM" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="HI4qZz" Title="Quasi-Newton method errors history">
   <Caption Id="Z41CtZ">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.1626, and the final value after 117 epochs is 0.107243.
The initial value of the selection error is 2.42371, and the final value after 117 epochs is 0.187355.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117</X2Data>
   <Y1Data>2.16\0.877\0.828\0.755\0.653\0.604\0.554\0.519\0.49\0.451\0.415\0.404\0.383\0.364\0.352\0.337\0.324\0.311\0.304\0.29\0.281\0.268\0.259\0.252\0.249\0.238\0.234\0.223\0.214\0.203\0.192\0.182\0.172\0.167\0.162\0.159\0.156\0.153\0.151\0.149\0.145\0.141\0.138\0.134\0.128\0.124\0.12\0.118\0.116\0.115\0.114\0.113\0.113\0.112\0.112\0.111\0.111\0.11\0.11\0.11\0.109\0.109\0.109\0.109\0.109\0.108\0.108\0.108\0.108\0.108\0.108\0.108\0.108\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.42\0.977\0.891\0.829\0.649\0.606\0.532\0.507\0.471\0.504\0.511\0.485\0.45\0.444\0.439\0.428\0.402\0.413\0.424\0.423\0.396\0.379\0.365\0.357\0.365\0.361\0.357\0.341\0.357\0.333\0.28\0.256\0.232\0.243\0.238\0.235\0.231\0.23\0.231\0.222\0.224\0.223\0.226\0.226\0.227\0.23\0.224\0.216\0.204\0.199\0.195\0.194\0.189\0.19\0.188\0.189\0.186\0.188\0.189\0.188\0.188\0.184\0.184\0.186\0.186\0.187\0.186\0.186\0.186\0.187\0.187\0.188\0.187\0.19\0.19\0.189\0.19\0.189\0.19\0.19\0.188\0.188\0.188\0.188\0.188\0.188\0.187\0.187\0.187\0.187\0.188\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.188\0.188\0.188\0.188\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>118</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="Q36A4v" Title="Quasi-Newton method results">
   <Caption Id="EFAllz">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.107
0.187
117
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Yh7GcW" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="BMNLrT" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="QOnLHu" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="RolS4J">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00583649\123.219\6.39017\8.59359
3.74134e-5\0.789864\0.0409627\0.0550871
0.00374134\78.9864\4.09627\5.50871</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="5YP60X" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="qzjFAY" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="GxARdQ" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="sesYRt" Title="Quasi-Newton method errors history">
   <Caption Id="O9x5vJ">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.16723, and the final value after 151 epochs is 0.329007.
The initial value of the selection error is 1.77809, and the final value after 151 epochs is 0.287317.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151</X2Data>
   <Y1Data>2.17\0.935\0.846\0.777\0.719\0.692\0.658\0.642\0.635\0.63\0.62\0.611\0.603\0.583\0.571\0.559\0.55\0.539\0.532\0.528\0.523\0.517\0.51\0.504\0.498\0.492\0.486\0.481\0.476\0.469\0.464\0.459\0.454\0.449\0.443\0.438\0.431\0.424\0.416\0.411\0.406\0.4\0.394\0.39\0.384\0.38\0.376\0.374\0.371\0.368\0.365\0.362\0.36\0.358\0.357\0.354\0.351\0.349\0.347\0.346\0.344\0.343\0.342\0.34\0.339\0.338\0.337\0.336\0.335\0.335\0.334\0.333\0.333\0.333\0.332\0.332\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.78\0.9\0.796\0.751\0.653\0.608\0.608\0.614\0.609\0.604\0.573\0.576\0.613\0.597\0.561\0.534\0.523\0.513\0.511\0.507\0.513\0.51\0.504\0.507\0.505\0.503\0.494\0.502\0.503\0.485\0.471\0.458\0.447\0.445\0.448\0.457\0.44\0.423\0.411\0.412\0.406\0.41\0.405\0.4\0.38\0.378\0.377\0.373\0.372\0.37\0.363\0.358\0.352\0.344\0.343\0.337\0.329\0.326\0.323\0.316\0.311\0.311\0.307\0.306\0.308\0.307\0.309\0.308\0.307\0.306\0.304\0.304\0.304\0.304\0.303\0.303\0.302\0.301\0.3\0.298\0.298\0.298\0.297\0.298\0.298\0.297\0.296\0.296\0.295\0.295\0.295\0.294\0.294\0.293\0.293\0.293\0.292\0.291\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>152</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="QUcDrG" Title="Quasi-Newton method results">
   <Caption Id="AHU0h8">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.329
0.287
151
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="aOx9sk" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="5WTc1M" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 189.</Text>
 </Task>
 <Task Id="F7mQa1" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="Rs9H0g" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="T4wsjw" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="SujbwJ" Title="Quasi-Newton method errors history">
   <Caption Id="Tjzmwe">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.1342, and the final value after 173 epochs is 0.32903.
The initial value of the selection error is 0.761664, and the final value after 173 epochs is 0.286891.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173</X2Data>
   <Y1Data>1.13\0.765\0.674\0.616\0.589\0.564\0.531\0.51\0.492\0.475\0.459\0.446\0.436\0.425\0.415\0.409\0.403\0.397\0.39\0.387\0.383\0.38\0.378\0.376\0.375\0.372\0.371\0.369\0.367\0.364\0.361\0.36\0.357\0.355\0.352\0.35\0.348\0.347\0.346\0.344\0.344\0.343\0.341\0.34\0.339\0.338\0.337\0.337\0.336\0.335\0.335\0.335\0.334\0.333\0.333\0.333\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.762\0.692\0.654\0.561\0.507\0.464\0.461\0.461\0.451\0.43\0.417\0.403\0.396\0.394\0.389\0.375\0.376\0.377\0.378\0.372\0.366\0.362\0.365\0.362\0.36\0.359\0.355\0.351\0.343\0.333\0.325\0.317\0.311\0.302\0.294\0.29\0.286\0.281\0.281\0.275\0.279\0.277\0.277\0.279\0.281\0.281\0.281\0.28\0.28\0.283\0.282\0.284\0.283\0.285\0.287\0.288\0.289\0.29\0.291\0.289\0.29\0.289\0.289\0.289\0.288\0.287\0.286\0.285\0.286\0.286\0.287\0.286\0.286\0.286\0.286\0.287\0.286\0.287\0.286\0.287\0.287\0.288\0.287\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.286\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.286\0.285\0.286\0.286\0.286\0.286\0.286\0.286\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>174</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="dCBeFB" Title="Quasi-Newton method results">
   <Caption Id="7F3HZQ">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.329
0.287
173
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="LKW9nh" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="KRHwF6" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="7Inenn" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="WFSSI4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0209236\106.533\6.57342\8.07645
0.000134126\0.682901\0.0421373\0.0517721
0.0134126\68.2901\4.21373\5.17721</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="1xKkmi" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Rz3HfS">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0012455\309.989\6.76535\14.2588
3.66323e-6\0.911732\0.0198981\0.0419377
0.000366323\91.1732\1.98981\4.19377</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="QZlxo8" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="jNWdfR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0116577\33.6898\5.09494\4.48169
4.70069e-5\0.135846\0.0205441\0.0180713
0.00470069\13.5846\2.05441\1.80713</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="NX2V4a" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="05tg9u">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0129814\36.4477\6.24232\4.61561
0.000180297\0.506217\0.0866989\0.0641057
0.0180297\50.6217\8.66989\6.41057</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="zCCXDU" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="qKtbNC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00427175\14.2281\0.752836\0.995104
0.000251279\0.836949\0.0442845\0.0585355
0.0251279\83.6949\4.42845\5.85355</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="6W1GJI" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="if7fPA" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 235.</Text>
 </Task>
 <Task Id="mz128I" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="HcmaBH" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="WD3c4S" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="lxyz9r" Title="Quasi-Newton method errors history">
   <Caption Id="c0Gbxz">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.03787, and the final value after 211 epochs is 0.315861.
The initial value of the selection error is 0.746445, and the final value after 211 epochs is 0.278184.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211</X2Data>
   <Y1Data>1.04\0.754\0.627\0.567\0.532\0.498\0.472\0.443\0.427\0.42\0.411\0.398\0.388\0.38\0.373\0.369\0.361\0.356\0.351\0.348\0.346\0.344\0.342\0.34\0.339\0.337\0.336\0.334\0.333\0.332\0.331\0.331\0.33\0.329\0.329\0.328\0.328\0.328\0.327\0.327\0.327\0.326\0.326\0.326\0.325\0.325\0.325\0.325\0.324\0.324\0.324\0.324\0.324\0.324\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.318\0.318\0.318\0.318\0.318\0.318\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.746\0.62\0.585\0.549\0.482\0.439\0.435\0.419\0.4\0.396\0.381\0.373\0.363\0.352\0.341\0.34\0.333\0.334\0.326\0.32\0.313\0.308\0.305\0.301\0.3\0.301\0.297\0.295\0.294\0.294\0.294\0.292\0.293\0.289\0.288\0.288\0.289\0.29\0.29\0.289\0.289\0.289\0.288\0.287\0.286\0.286\0.286\0.284\0.284\0.284\0.284\0.285\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.282\0.282\0.281\0.281\0.28\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.277\0.277\0.277\0.275\0.275\0.275\0.275\0.274\0.275\0.276\0.276\0.278\0.279\0.278\0.279\0.278\0.279\0.279\0.28\0.281\0.281\0.281\0.282\0.281\0.282\0.281\0.281\0.282\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.282\0.283\0.282\0.282\0.282\0.282\0.282\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.279\0.28\0.279\0.279\0.28\0.28\0.28\0.279\0.28\0.279\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>212</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="UoFWaV" Title="Quasi-Newton method results">
   <Caption Id="aKhUs7">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.316
0.278
211
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ppdezd" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="UP6Qnz" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="4azG6d" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="HrZAo0">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00208664\105.786\6.54\8.11844
1.33759e-5\0.678117\0.0419231\0.0520413
0.00133759\67.8117\4.19231\5.20413</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="9rQL8m" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="tiQL29">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00324821\307.31\6.88482\14.0396
9.55357e-6\0.903853\0.0202495\0.0412929
0.000955357\90.3853\2.02495\4.12929</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="kz25Vh" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="RN2u7z">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0346203\36.8622\4.98558\4.40516
0.000139598\0.148638\0.0201031\0.0177628
0.0139598\14.8638\2.01031\1.77628</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="QUjZG2" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="R9aIOz">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00782394\38.0287\5.91809\4.47591
0.000108666\0.528176\0.0821958\0.0621654
0.0108666\52.8176\8.21958\6.21654</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Cbcy9x" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="goIZTk">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000297308\14.7554\0.745263\1.01464
1.74887e-5\0.867967\0.043839\0.0596844
0.00174887\86.7967\4.3839\5.96844</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="zJM1B9" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="ju0VOK" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 465.</Text>
 </Task>
 <Task Id="zV13UW" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="IWcmCa" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="VeSuDk" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="wq5otJ" Title="Quasi-Newton method errors history">
   <Caption Id="XiU6Eh">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.07128, and the final value after 217 epochs is 0.275606.
The initial value of the selection error is 0.794562, and the final value after 217 epochs is 0.27459.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217</X2Data>
   <Y1Data>1.07\0.748\0.599\0.559\0.509\0.474\0.442\0.415\0.401\0.39\0.379\0.371\0.365\0.358\0.353\0.347\0.344\0.34\0.337\0.334\0.332\0.329\0.327\0.325\0.323\0.322\0.321\0.319\0.318\0.317\0.316\0.315\0.314\0.314\0.313\0.312\0.312\0.311\0.31\0.309\0.309\0.308\0.308\0.307\0.306\0.306\0.306\0.305\0.304\0.304\0.303\0.303\0.302\0.302\0.301\0.301\0.3\0.299\0.299\0.298\0.298\0.297\0.296\0.295\0.295\0.294\0.293\0.293\0.292\0.291\0.291\0.29\0.29\0.289\0.289\0.289\0.288\0.288\0.288\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.278\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.795\0.606\0.548\0.512\0.457\0.433\0.404\0.375\0.355\0.352\0.345\0.334\0.328\0.311\0.298\0.296\0.294\0.291\0.287\0.284\0.276\0.271\0.272\0.271\0.272\0.276\0.275\0.274\0.275\0.277\0.279\0.277\0.276\0.275\0.275\0.276\0.277\0.278\0.279\0.279\0.28\0.28\0.278\0.281\0.279\0.278\0.278\0.278\0.279\0.28\0.278\0.276\0.275\0.274\0.276\0.275\0.278\0.277\0.278\0.278\0.279\0.278\0.276\0.276\0.274\0.275\0.271\0.273\0.271\0.269\0.268\0.266\0.266\0.264\0.265\0.264\0.264\0.263\0.264\0.264\0.264\0.265\0.266\0.266\0.265\0.266\0.266\0.265\0.265\0.266\0.266\0.265\0.265\0.265\0.266\0.266\0.267\0.267\0.267\0.266\0.267\0.266\0.267\0.267\0.268\0.268\0.268\0.268\0.267\0.267\0.267\0.268\0.269\0.268\0.268\0.268\0.267\0.268\0.268\0.267\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.269\0.269\0.269\0.269\0.269\0.27\0.269\0.269\0.269\0.269\0.269\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.271\0.271\0.271\0.271\0.271\0.272\0.272\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.272\0.271\0.272\0.271\0.272\0.272\0.272\0.273\0.272\0.273\0.273\0.273\0.273\0.273\0.273\0.272\0.273\0.273\0.273\0.273\0.273\0.274\0.274\0.274\0.275\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.275\0.274\0.274\0.274\0.274\0.275\0.275\0.275\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.275</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>218</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="IaKJuR" Title="Quasi-Newton method results">
   <Caption Id="WSoS1M">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.276
0.275
217
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ZyEIES" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="e1OQeP" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="tAcQlD" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="GGcuRZ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00649643\112.895\6.63726\8.34509
4.16438e-5\0.723685\0.0425465\0.0534941
0.00416438\72.3685\4.25465\5.34941</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="gCIVCg" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="AIcvxX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0219097\306.502\6.82379\14.0331
6.44403e-5\0.901476\0.02007\0.0412738
0.00644403\90.1476\2.007\4.12738</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="6h12pX" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="1Og9xf">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0114346\32.6486\5.29418\4.57132
4.61071e-5\0.131648\0.0213475\0.0184327
0.00461071\13.1648\2.13475\1.84327</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="FSGWMO" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="EPnt4H">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0400467\42.9939\5.78853\4.44924
0.000556204\0.597138\0.0803963\0.061795
0.0556204\59.7138\8.03963\6.1795</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="3SrsV1" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="xvXwbw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00336146\14.3555\0.780132\1.0306
0.000197733\0.844439\0.0458901\0.0606233
0.0197733\84.4439\4.58901\6.06233</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="fl3dt9" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="O5JPJZ" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 243.</Text>
 </Task>
 <Task Id="jPxEYf" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="F5kFj1" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="cFjD2b" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="uBp3BT" Title="Quasi-Newton method errors history">
   <Caption Id="WQgcZQ">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.08208, and the final value after 226 epochs is 0.352983.
The initial value of the selection error is 0.748401, and the final value after 226 epochs is 0.37341.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226</X2Data>
   <Y1Data>1.08\0.975\0.849\0.793\0.703\0.627\0.605\0.585\0.554\0.541\0.528\0.515\0.502\0.495\0.49\0.484\0.48\0.475\0.471\0.467\0.463\0.455\0.449\0.443\0.435\0.428\0.417\0.405\0.4\0.395\0.392\0.389\0.386\0.383\0.381\0.379\0.377\0.375\0.374\0.372\0.371\0.37\0.37\0.369\0.369\0.368\0.366\0.366\0.365\0.365\0.364\0.364\0.364\0.364\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.362\0.361\0.361\0.361\0.361\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.353\0.353\0.354\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.748\0.872\0.873\0.814\0.636\0.533\0.537\0.538\0.516\0.502\0.482\0.477\0.48\0.485\0.483\0.487\0.478\0.473\0.466\0.462\0.465\0.459\0.444\0.438\0.425\0.415\0.417\0.415\0.409\0.4\0.4\0.394\0.392\0.391\0.392\0.389\0.387\0.384\0.383\0.38\0.38\0.378\0.376\0.376\0.375\0.373\0.373\0.373\0.374\0.373\0.373\0.372\0.371\0.371\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.374\0.375\0.375\0.375\0.375\0.375\0.374\0.375\0.376\0.376\0.376\0.376\0.376\0.376\0.377\0.378\0.379\0.379\0.379\0.379\0.378\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.375\0.375\0.375\0.375\0.375\0.374\0.374\0.374\0.374\0.374\0.375\0.375\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.375\0.375\0.375\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.375\0.375\0.375\0.375\0.376\0.375\0.375\0.375\0.376\0.376\0.375\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.376\0.377\0.377\0.377\0.376\0.376\0.376\0.376\0.376\0.376\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>227</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="gr5nXT" Title="Quasi-Newton method results">
   <Caption Id="MgVeD5">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.353
0.373
226
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="tyK289" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="qSBHZ9" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="pqV7e7" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="URf6gs">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00236511\102.955\8.27126\9.05566
1.5161e-5\0.659967\0.0530209\0.0580491
0.0015161\65.9967\5.30209\5.80491</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="b0in6m" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ta8iQO">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00155258\309.015\6.68403\14.2176
4.56642e-6\0.908869\0.0196589\0.0418165
0.000456642\90.8869\1.96589\4.18165</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="p9Yv5N" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="t9Ep5N">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0605621\27.8515\5.79298\4.63822
0.000244202\0.112304\0.0233588\0.0187025
0.0244202\11.2304\2.33588\1.87025</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="S0b01m" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QddHtK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0159187\39.0549\7.62596\5.09906
0.000221093\0.54243\0.105916\0.0708202
0.0221094\54.243\10.5916\7.08202</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ANmms8" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9VHZuN">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>2.6226e-6\14.0192\0.937123\1.03616
1.54271e-7\0.824659\0.0551249\0.0609509
1.54271e-5\82.4659\5.51249\6.09509</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="pWetwK" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="Ns46rJ" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 221.</Text>
 </Task>
 <Task Id="QItc3I" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="7nb9Ok" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="m1u8A7" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="H51x2I" Title="Quasi-Newton method errors history">
   <Caption Id="J1oQaQ">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.05725, and the final value after 192 epochs is 0.639621.
The initial value of the selection error is 0.748566, and the final value after 192 epochs is 0.584052.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192</X2Data>
   <Y1Data>1.06\1.01\0.912\0.909\0.86\0.777\0.756\0.738\0.725\0.712\0.702\0.689\0.684\0.681\0.677\0.674\0.672\0.67\0.668\0.667\0.665\0.665\0.664\0.663\0.662\0.661\0.66\0.66\0.659\0.658\0.658\0.657\0.657\0.656\0.656\0.656\0.656\0.655\0.655\0.654\0.654\0.653\0.653\0.653\0.653\0.652\0.652\0.652\0.652\0.651\0.651\0.651\0.651\0.65\0.65\0.65\0.65\0.65\0.649\0.649\0.649\0.648\0.648\0.648\0.648\0.648\0.648\0.648\0.648\0.647\0.647\0.647\0.647\0.647\0.647\0.646\0.646\0.646\0.646\0.646\0.646\0.646\0.646\0.646\0.646\0.645\0.645\0.645\0.645\0.645\0.645\0.645\0.645\0.645\0.645\0.645\0.644\0.644\0.644\0.644\0.644\0.644\0.644\0.644\0.644\0.644\0.644\0.643\0.643\0.643\0.643\0.643\0.643\0.643\0.643\0.643\0.643\0.643\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.749\0.865\0.767\0.757\0.718\0.579\0.596\0.59\0.585\0.582\0.591\0.582\0.582\0.585\0.585\0.587\0.58\0.574\0.582\0.585\0.582\0.576\0.573\0.572\0.576\0.577\0.576\0.576\0.573\0.574\0.574\0.576\0.578\0.577\0.577\0.576\0.575\0.576\0.578\0.576\0.575\0.573\0.573\0.575\0.576\0.576\0.572\0.572\0.572\0.575\0.576\0.576\0.574\0.572\0.573\0.575\0.575\0.574\0.574\0.574\0.574\0.575\0.575\0.574\0.575\0.575\0.576\0.577\0.576\0.576\0.576\0.576\0.577\0.576\0.576\0.575\0.575\0.575\0.575\0.575\0.576\0.575\0.575\0.574\0.574\0.574\0.575\0.575\0.575\0.575\0.575\0.575\0.574\0.574\0.575\0.575\0.575\0.575\0.575\0.575\0.575\0.575\0.575\0.575\0.575\0.575\0.576\0.577\0.577\0.578\0.578\0.578\0.577\0.578\0.578\0.578\0.578\0.578\0.578\0.578\0.577\0.577\0.577\0.577\0.578\0.578\0.577\0.577\0.577\0.578\0.579\0.578\0.578\0.578\0.578\0.579\0.579\0.579\0.579\0.579\0.579\0.579\0.58\0.58\0.58\0.581\0.581\0.581\0.582\0.582\0.582\0.582\0.582\0.582\0.582\0.581\0.582\0.582\0.582\0.582\0.582\0.583\0.583\0.583\0.583\0.582\0.582\0.582\0.582\0.582\0.582\0.583\0.582\0.582\0.583\0.583\0.583\0.583\0.583\0.583\0.583\0.583\0.583\0.583\0.583\0.583\0.583\0.583\0.583\0.583\0.584\0.584\0.584</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>193</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="1J4GmQ" Title="Quasi-Newton method results">
   <Caption Id="MEH7gl">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.64
0.584
192
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Tf1EPd" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="KZvz0S" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="LNKGCm" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ozdehQ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00516891\102.193\12.98\12.1492
3.31341e-5\0.655085\0.0832052\0.0778798
0.00331341\65.5085\8.32052\7.78798</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="qsV9FE" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QilZE4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0123653\314.647\8.49571\14.8684
3.63687e-5\0.925434\0.0249874\0.0437306
0.00363686\92.5434\2.49874\4.37306</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="WO8iJs" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="G9hHZc">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00216675\37.4951\10.5514\6.92072
8.73689e-6\0.15119\0.0425461\0.0279061
0.000873689\15.119\4.25461\2.79061</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="O9xw6k" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="AcfhY6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0347557\42.5235\7.50645\5.03743
0.000482718\0.590604\0.104256\0.0699643
0.0482718\59.0604\10.4256\6.99643</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="OxkCIo" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="iOkqJB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00798583\10.4375\1.90851\1.15156
0.000469755\0.613973\0.112265\0.0677389
0.0469755\61.3973\11.2265\6.77389</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="CypBYX" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="0crTZV" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 327.</Text>
 </Task>
 <Task Id="adbxR6" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="V7sduN" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="lcnAwN" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="rHY2iH" Title="Quasi-Newton method errors history">
   <Caption Id="rygxrc">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.0644, and the final value after 237 epochs is 0.295115.
The initial value of the selection error is 0.837957, and the final value after 237 epochs is 0.260106.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237</X2Data>
   <Y1Data>1.06\0.814\0.688\0.602\0.523\0.491\0.449\0.431\0.412\0.402\0.391\0.381\0.375\0.37\0.364\0.359\0.355\0.352\0.35\0.346\0.343\0.341\0.338\0.337\0.335\0.332\0.33\0.329\0.327\0.326\0.325\0.324\0.323\0.322\0.322\0.321\0.321\0.32\0.319\0.319\0.318\0.318\0.318\0.317\0.317\0.316\0.316\0.316\0.315\0.315\0.314\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.302\0.302\0.302\0.301\0.301\0.301\0.301\0.3\0.3\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.838\0.737\0.692\0.566\0.463\0.424\0.378\0.365\0.37\0.377\0.374\0.354\0.342\0.336\0.327\0.319\0.308\0.306\0.304\0.296\0.287\0.286\0.28\0.282\0.279\0.277\0.274\0.278\0.273\0.275\0.274\0.275\0.274\0.276\0.275\0.275\0.275\0.276\0.274\0.273\0.273\0.273\0.274\0.273\0.273\0.272\0.273\0.274\0.275\0.273\0.274\0.272\0.271\0.271\0.27\0.271\0.271\0.27\0.27\0.27\0.271\0.272\0.272\0.271\0.272\0.273\0.274\0.273\0.272\0.273\0.274\0.275\0.275\0.275\0.272\0.274\0.274\0.273\0.272\0.271\0.272\0.272\0.275\0.275\0.273\0.274\0.274\0.272\0.272\0.273\0.273\0.274\0.274\0.275\0.274\0.274\0.273\0.272\0.27\0.27\0.269\0.271\0.271\0.268\0.267\0.265\0.266\0.265\0.263\0.262\0.262\0.262\0.261\0.261\0.26\0.26\0.261\0.26\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.262\0.261\0.261\0.261\0.26\0.26\0.261\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.259\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.261\0.26\0.26\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.262\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.26\0.261\0.26\0.26\0.26\0.26</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>238</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="pEh6uz" Title="Quasi-Newton method results">
   <Caption Id="nhYAPS">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.295
0.26
237
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="dzjZoR" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="itCzW5" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="KzDoyZ" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="OigjNP">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00643158\112.989\6.37392\8.30095
4.12281e-5\0.724288\0.0408585\0.0532112
0.00412281\72.4288\4.08585\5.32112</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="OLNg2P" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="EcsvMd">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00681877\309.742\6.71723\14.18
2.00552e-5\0.911005\0.0197566\0.0417058
0.00200552\91.1005\1.97566\4.17058</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="wMPl13" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="A9hM3d">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00243378\33.5706\5.07852\4.48371
9.81362e-6\0.135365\0.0204779\0.0180795
0.000981362\13.5365\2.04779\1.80795</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="E8rxBE" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="pTu4y8">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00227356\37.9077\5.4863\4.26209
3.15772e-5\0.526496\0.0761986\0.0591957
0.00315772\52.6496\7.61986\5.91957</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="SQ3AHn" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="zjseLs">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00435257\13.9674\0.797805\1.00152
0.000256033\0.821614\0.0469297\0.058913
0.0256034\82.1614\4.69297\5.8913</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="BkmNDl" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="EqHaUH" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 373.</Text>
 </Task>
 <Task Id="A57AXO" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="lgSq0C" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="aejQ9J" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="gIi6PO" Title="Quasi-Newton method errors history">
   <Caption Id="Yjnfmu">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.11059, and the final value after 225 epochs is 0.288634.
The initial value of the selection error is 0.777588, and the final value after 225 epochs is 0.265515.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225</X2Data>
   <Y1Data>1.11\0.79\0.642\0.57\0.526\0.47\0.438\0.423\0.404\0.39\0.38\0.376\0.368\0.361\0.354\0.35\0.346\0.344\0.339\0.337\0.336\0.333\0.331\0.328\0.326\0.325\0.323\0.321\0.32\0.319\0.319\0.318\0.317\0.317\0.316\0.315\0.315\0.314\0.313\0.313\0.312\0.312\0.312\0.311\0.311\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.302\0.302\0.302\0.301\0.301\0.3\0.299\0.299\0.299\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.294\0.295\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.778\0.688\0.643\0.536\0.46\0.411\0.396\0.392\0.386\0.366\0.339\0.331\0.333\0.322\0.306\0.305\0.3\0.296\0.285\0.283\0.281\0.281\0.281\0.276\0.275\0.277\0.279\0.277\0.276\0.274\0.274\0.273\0.275\0.274\0.275\0.274\0.276\0.275\0.276\0.278\0.279\0.277\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.275\0.276\0.275\0.276\0.276\0.277\0.277\0.278\0.279\0.278\0.278\0.279\0.28\0.279\0.279\0.28\0.279\0.279\0.279\0.279\0.28\0.28\0.279\0.28\0.28\0.281\0.28\0.28\0.28\0.281\0.281\0.282\0.28\0.281\0.281\0.281\0.279\0.279\0.278\0.278\0.278\0.276\0.275\0.276\0.274\0.273\0.272\0.27\0.27\0.269\0.269\0.269\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.267\0.267\0.267\0.265\0.265\0.265\0.265\0.265\0.265\0.264\0.264\0.265\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.263\0.264\0.263\0.263\0.263\0.264\0.264\0.264\0.264\0.264\0.264\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.264\0.264\0.264\0.263\0.263\0.263\0.263\0.263\0.263\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.261\0.261\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.264\0.264\0.264\0.264\0.264\0.265\0.265\0.265\0.265\0.266</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>226</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="yYrsSJ" Title="Quasi-Newton method results">
   <Caption Id="wwpi0K">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.289
0.266
225
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="DRFlOj" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="yTmgcu" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="9094Bz" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="DjqHZX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00144958\109.73\6.59662\8.444
9.29221e-6\0.7034\0.0422861\0.0541282
0.000929221\70.34\4.22861\5.41282</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="5ZTtxB" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Nx2BQw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00641251\309.237\6.7907\14.1527
1.88603e-5\0.90952\0.0199727\0.0416255
0.00188603\90.952\1.99727\4.16255</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="6bgJcR" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QuMhk7">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00814438\36.8735\5.03398\4.40902
3.28402e-5\0.148683\0.0202983\0.0177783
0.00328402\14.8683\2.02983\1.77783</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="fD7bZi" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Ek8hot">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0229177\38.0692\5.4465\4.27124
0.000318302\0.528739\0.0756458\0.0593228
0.0318302\52.8739\7.56458\5.93228</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="tb5HNu" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="U6EVuO">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00437236\13.1533\0.853208\0.976905
0.000257198\0.773722\0.0501887\0.057465
0.0257198\77.3722\5.01887\5.7465</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="bsq4FC" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="43dQf6" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 281.</Text>
 </Task>
 <Task Id="TmDrze" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="6GDGOS" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="eplGXQ" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="N7flcz">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0580902\110.638\16.0432\14.1658
0.000372373\0.709216\0.102841\0.0908067
0.0372373\70.9216\10.2841\9.08067</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="aPivNT" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="LFiRCD">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0662975\319.462\10.2061\15.4703
0.000194993\0.939595\0.030018\0.0455008
0.0194993\93.9595\3.0018\4.55008</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="GtHpBV" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="3AMSR2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0148087\32.073\7.32571\6.05219
5.97123e-5\0.129326\0.0295392\0.024404
0.00597123\12.9326\2.95392\2.4404</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="RyjOY9" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Z2bWe6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0259075\43.5738\6.73248\4.65381
0.000359827\0.605192\0.0935067\0.0646362
0.0359827\60.5192\9.35067\6.46362</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="RHEgFi" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="cIs6TM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0123224\13.3735\2.33848\0.99319
0.000724849\0.786677\0.137558\0.058423
0.0724849\78.6677\13.7558\5.8423</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="X9pTLz" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="J2W60A" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="DhBOZ5" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="uYnSH8" Title="Quasi-Newton method errors history">
   <Caption Id="bls9Yl">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.07507, and the final value after 173 epochs is 0.311317.
The initial value of the selection error is 0.69947, and the final value after 173 epochs is 0.274366.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173</X2Data>
   <Y1Data>1.08\0.734\0.637\0.57\0.526\0.502\0.47\0.436\0.423\0.411\0.397\0.389\0.382\0.371\0.365\0.359\0.353\0.351\0.347\0.344\0.343\0.34\0.339\0.337\0.336\0.334\0.333\0.332\0.331\0.33\0.329\0.329\0.328\0.327\0.327\0.326\0.325\0.324\0.324\0.324\0.323\0.323\0.322\0.322\0.321\0.321\0.32\0.32\0.319\0.319\0.319\0.319\0.318\0.318\0.317\0.317\0.316\0.316\0.316\0.315\0.315\0.315\0.315\0.315\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.699\0.644\0.625\0.526\0.424\0.4\0.389\0.407\0.39\0.369\0.353\0.351\0.357\0.335\0.321\0.323\0.317\0.311\0.296\0.29\0.289\0.285\0.279\0.281\0.279\0.28\0.282\0.281\0.279\0.279\0.279\0.28\0.279\0.277\0.279\0.279\0.278\0.278\0.278\0.279\0.281\0.281\0.281\0.28\0.279\0.28\0.28\0.276\0.277\0.276\0.278\0.277\0.278\0.277\0.276\0.276\0.275\0.274\0.275\0.276\0.275\0.273\0.274\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.275\0.275\0.275\0.276\0.276\0.275\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.276\0.275\0.276\0.275\0.276\0.276\0.276\0.276\0.276\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>174</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="kAPker" Title="Quasi-Newton method results">
   <Caption Id="P38Qnb">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.311
0.274
173
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="azy4CJ" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="VpKwcs" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="m472bG" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QSudSk">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00790024\105.969\6.46048\8.06537
5.06426e-5\0.679287\0.0414134\0.0517011
0.00506426\67.9287\4.14134\5.17011</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="8cZtDY" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9Za3cU">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0110464\305.475\6.81476\13.9687
3.24894e-5\0.898456\0.0200434\0.0410844
0.00324894\89.8456\2.00434\4.10844</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Ziz1QC" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="3Ajuwi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00540543\34.3126\4.97924\4.29676
2.17961e-5\0.138357\0.0200776\0.0173257
0.00217961\13.8357\2.00776\1.73257</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="pXE83e" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="D4IHSH">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0156994\37.5806\5.82768\4.46969
0.000218047\0.521953\0.08094\0.062079
0.0218047\52.1953\8.094\6.2079</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="sgOY7G" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="BgD65V">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000231504\13.4918\0.861925\1.00439
1.36179e-5\0.793636\0.0507015\0.0590816
0.00136179\79.3636\5.07015\5.90816</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="5PfcoC" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="Y3D0DD" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="u1EqFf" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="HBJktc" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="HDEynh" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="g3LNYg" Title="Quasi-Newton method errors history">
   <Caption Id="VCgNdq">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.0966, and the final value after 207 epochs is 0.276746.
The initial value of the selection error is 0.785344, and the final value after 207 epochs is 0.267462.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207</X2Data>
   <Y1Data>1.1\0.794\0.628\0.563\0.511\0.455\0.426\0.414\0.404\0.395\0.384\0.375\0.367\0.356\0.351\0.347\0.343\0.34\0.337\0.334\0.332\0.33\0.328\0.326\0.324\0.323\0.321\0.32\0.318\0.317\0.316\0.316\0.315\0.314\0.314\0.313\0.312\0.311\0.311\0.31\0.309\0.309\0.309\0.308\0.308\0.307\0.307\0.307\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.302\0.302\0.301\0.301\0.301\0.3\0.3\0.299\0.299\0.299\0.299\0.299\0.298\0.298\0.298\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.295\0.295\0.294\0.294\0.294\0.293\0.293\0.292\0.292\0.292\0.291\0.291\0.291\0.29\0.29\0.289\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.281\0.281\0.281\0.281\0.281\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.278\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.785\0.684\0.613\0.5\0.424\0.41\0.387\0.367\0.351\0.347\0.345\0.336\0.332\0.317\0.306\0.296\0.289\0.284\0.277\0.275\0.275\0.272\0.273\0.269\0.273\0.275\0.275\0.273\0.269\0.269\0.269\0.273\0.272\0.271\0.269\0.27\0.269\0.27\0.27\0.272\0.273\0.274\0.274\0.273\0.272\0.272\0.271\0.272\0.271\0.271\0.271\0.272\0.271\0.272\0.272\0.272\0.271\0.272\0.272\0.272\0.272\0.272\0.271\0.269\0.269\0.269\0.269\0.269\0.27\0.27\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.267\0.267\0.268\0.268\0.269\0.269\0.271\0.271\0.271\0.272\0.272\0.273\0.271\0.271\0.271\0.272\0.271\0.272\0.271\0.271\0.269\0.269\0.268\0.268\0.268\0.267\0.268\0.269\0.268\0.268\0.268\0.269\0.268\0.268\0.267\0.266\0.266\0.266\0.264\0.264\0.263\0.263\0.263\0.262\0.263\0.263\0.263\0.263\0.262\0.262\0.261\0.261\0.262\0.262\0.261\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.263\0.263\0.263\0.264\0.264\0.265\0.265\0.266\0.265\0.265\0.264\0.265\0.265\0.265\0.265\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.267\0.267\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.268\0.267\0.267\0.267\0.267\0.267\0.268\0.267\0.267</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>208</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="aijCa8" Title="Quasi-Newton method results">
   <Caption Id="jARStL">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.277
0.267
207
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="0kyZTK" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="glQQ72" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="PS4kNX" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="js4EOx">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00961304\107.839\6.36936\8.21192
6.1622e-5\0.691274\0.0408292\0.0526405
0.0061622\69.1274\4.08292\5.26405</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="3Q1KzO" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="OWbL1P">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.025177\304.011\6.65716\13.9398
7.405e-5\0.894151\0.0195799\0.0409994
0.007405\89.4151\1.95799\4.09994</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="2y9882" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="kQ3qXa">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0101013\40.6576\5.05205\4.72932
4.07311e-5\0.163942\0.0203712\0.0190699
0.00407311\16.3942\2.03712\1.90699</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="SxNhKW" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="tShOVW">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0126667\36.3888\5.26577\4.09463
0.000175926\0.5054\0.0731356\0.0568699
0.0175926\50.54\7.31356\5.68699</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="mjbBCl" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="I8ZPlW">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00169158\13.88\0.819244\1.00799
9.95047e-5\0.816469\0.0481908\0.0592935
0.00995047\81.6469\4.81908\5.92935</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ZnybC5" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="gLh88l" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 1155.</Text>
 </Task>
 <Task Id="KRYHua" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="1oBr1m" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="S9bZle" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="QDXt62" Title="Quasi-Newton method errors history">
   <Caption Id="46w4ay">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.02322, and the final value after 191 epochs is 0.213521.
The initial value of the selection error is 0.694735, and the final value after 191 epochs is 0.318361.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191</X2Data>
   <Y1Data>1.02\0.626\0.522\0.477\0.424\0.395\0.38\0.368\0.359\0.353\0.347\0.342\0.336\0.332\0.329\0.326\0.323\0.321\0.319\0.317\0.316\0.314\0.312\0.31\0.309\0.307\0.305\0.304\0.303\0.301\0.3\0.298\0.297\0.296\0.294\0.292\0.291\0.29\0.289\0.288\0.286\0.285\0.283\0.282\0.281\0.279\0.277\0.275\0.274\0.272\0.271\0.269\0.268\0.266\0.266\0.264\0.263\0.262\0.261\0.26\0.259\0.258\0.257\0.256\0.255\0.254\0.254\0.253\0.252\0.252\0.251\0.25\0.249\0.249\0.248\0.247\0.246\0.245\0.244\0.244\0.243\0.242\0.242\0.242\0.241\0.24\0.24\0.239\0.239\0.238\0.238\0.237\0.237\0.237\0.236\0.236\0.235\0.235\0.235\0.234\0.234\0.234\0.233\0.233\0.233\0.232\0.232\0.231\0.231\0.23\0.23\0.23\0.229\0.229\0.229\0.228\0.228\0.228\0.227\0.227\0.227\0.227\0.226\0.226\0.226\0.226\0.225\0.225\0.225\0.225\0.224\0.224\0.224\0.224\0.224\0.223\0.223\0.223\0.223\0.222\0.222\0.222\0.222\0.221\0.221\0.221\0.221\0.22\0.22\0.22\0.22\0.22\0.22\0.219\0.219\0.219\0.219\0.219\0.219\0.219\0.219\0.218\0.218\0.218\0.218\0.218\0.218\0.218\0.217\0.217\0.217\0.217\0.217\0.217\0.217\0.216\0.216\0.216\0.216\0.216\0.216\0.216\0.215\0.215\0.215\0.215\0.214\0.214\0.214\0.214\0.214\0.214</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.695\0.58\0.458\0.432\0.404\0.383\0.344\0.325\0.312\0.311\0.3\0.294\0.287\0.28\0.282\0.276\0.277\0.274\0.272\0.273\0.275\0.274\0.275\0.274\0.273\0.277\0.275\0.274\0.274\0.271\0.27\0.271\0.271\0.274\0.272\0.277\0.276\0.278\0.278\0.279\0.277\0.279\0.279\0.281\0.276\0.28\0.278\0.278\0.275\0.278\0.277\0.283\0.284\0.282\0.28\0.279\0.279\0.276\0.279\0.279\0.283\0.281\0.284\0.284\0.284\0.285\0.287\0.287\0.286\0.285\0.284\0.284\0.288\0.288\0.287\0.288\0.291\0.289\0.291\0.289\0.29\0.291\0.292\0.292\0.292\0.294\0.295\0.298\0.295\0.296\0.294\0.295\0.293\0.295\0.297\0.296\0.296\0.298\0.297\0.297\0.296\0.297\0.297\0.298\0.296\0.298\0.298\0.301\0.301\0.301\0.3\0.302\0.302\0.302\0.301\0.301\0.302\0.302\0.303\0.304\0.303\0.303\0.303\0.303\0.304\0.303\0.304\0.304\0.305\0.305\0.307\0.306\0.306\0.307\0.307\0.308\0.308\0.309\0.308\0.31\0.311\0.312\0.312\0.313\0.314\0.314\0.316\0.316\0.316\0.317\0.316\0.316\0.316\0.317\0.316\0.316\0.316\0.317\0.318\0.318\0.318\0.319\0.319\0.319\0.319\0.32\0.319\0.32\0.319\0.319\0.319\0.319\0.32\0.319\0.319\0.318\0.319\0.319\0.319\0.318\0.318\0.317\0.317\0.317\0.318\0.318\0.318\0.318\0.318\0.317\0.318\0.318</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>192</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="mjSNlp" Title="Quasi-Newton method results">
   <Caption Id="Lv4kJh">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.214
0.318
191
00:00:05
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="o9Ajd4" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="pPRQYA" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="4SxoZh" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="BT54bF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00937271\111.971\6.77369\8.51233
6.00815e-5\0.717763\0.0434211\0.0545662
0.00600815\71.7763\4.34211\5.45662</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="nr1ddV" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ssdBuZ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\306.335\7.59117\14.2913
0\0.900985\0.022327\0.0420333
0\90.0985\2.2327\4.20333</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="FqFaR5" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="LgsQDv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0370827\42.2408\6.43495\5.43347
0.000149527\0.170326\0.0259474\0.0219092
0.0149527\17.0326\2.59474\2.19092</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="hRwArw" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="xOBzNA">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0148582\38.6341\5.57788\4.40916
0.000206365\0.536585\0.0774706\0.0612383
0.0206365\53.6585\7.74706\6.12383</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ww4tPo" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="nHNvJG">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\13.4159\0.749822\0.996933
0\0.789173\0.0441072\0.0586431
0\78.9173\4.41072\5.86431</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="vArMSs" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="l7t11W" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 329.</Text>
 </Task>
 <Task Id="YkaLEG" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="ubXTX6" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="KYMjOE" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="8TFFND" Title="Quasi-Newton method errors history">
   <Caption Id="txCbfC">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.0935, and the final value after 208 epochs is 0.318629.
The initial value of the selection error is 0.804555, and the final value after 208 epochs is 0.280428.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208</X2Data>
   <Y1Data>1.09\0.673\0.61\0.554\0.495\0.46\0.443\0.43\0.417\0.405\0.395\0.384\0.378\0.371\0.366\0.363\0.359\0.356\0.353\0.351\0.349\0.348\0.346\0.344\0.343\0.342\0.341\0.34\0.339\0.338\0.337\0.337\0.336\0.335\0.334\0.334\0.333\0.333\0.332\0.332\0.331\0.331\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.329\0.328\0.328\0.328\0.328\0.328\0.328\0.327\0.327\0.327\0.327\0.327\0.327\0.326\0.326\0.326\0.326\0.326\0.325\0.325\0.325\0.325\0.325\0.324\0.324\0.324\0.324\0.323\0.324\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.805\0.591\0.546\0.461\0.413\0.413\0.403\0.398\0.378\0.36\0.344\0.344\0.338\0.32\0.307\0.298\0.292\0.286\0.284\0.282\0.277\0.28\0.281\0.279\0.283\0.282\0.281\0.279\0.278\0.274\0.275\0.276\0.274\0.274\0.275\0.274\0.273\0.273\0.273\0.275\0.274\0.275\0.273\0.272\0.271\0.272\0.274\0.274\0.274\0.274\0.274\0.273\0.273\0.273\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.276\0.275\0.274\0.273\0.274\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.274\0.273\0.273\0.273\0.273\0.273\0.274\0.273\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.276\0.276\0.276\0.276\0.277\0.277\0.277\0.276\0.276\0.277\0.276\0.276\0.276\0.277\0.277\0.278\0.278\0.278\0.277\0.277\0.278\0.278\0.278\0.278\0.278\0.279\0.279\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.281\0.28\0.28\0.28\0.28\0.28</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>209</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="vZjuJB" Title="Quasi-Newton method results">
   <Caption Id="BZKV28">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.319
0.28
208
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="4cKxnX" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="kMgIDh" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="qizzjG" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QWTe4H">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0110359\109.881\6.30005\8.07179
7.07431e-5\0.704365\0.040385\0.0517422
0.00707431\70.4365\4.0385\5.17422</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="2DzwB3" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="mBGme0">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0200891\303.488\6.73938\14.0306
5.90857e-5\0.892613\0.0198217\0.0412665
0.00590857\89.2613\1.98217\4.12665</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="eeSirI" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Iw3rmk">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00185204\22.9876\4.98252\4.18262
7.46789e-6\0.0926919\0.0200908\0.0168654
0.000746789\9.26919\2.00908\1.68654</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="V2Lwnl" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="gdadms">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0112572\43.335\5.57105\4.34761
0.00015635\0.601874\0.0773757\0.0603835
0.015635\60.1874\7.73757\6.03835</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="YuhIl1" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="JMfI2Q">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00269246\13.3819\0.745887\0.977931
0.00015838\0.78717\0.0438757\0.0575254
0.015838\78.717\4.38757\5.75254</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="oac0J0" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="J7zGJ1" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 289.</Text>
 </Task>
 <Task Id="oAtV6w" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="hDNSWd" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="xLvQPE" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="LgrncQ" Title="Quasi-Newton method errors history">
   <Caption Id="GHIVAN">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.834036, and the final value after 208 epochs is 0.0771518.
The initial value of the selection error is 0.779883, and the final value after 208 epochs is 0.176248.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208</X2Data>
   <Y1Data>0.834\0.417\0.326\0.249\0.21\0.189\0.171\0.157\0.146\0.138\0.134\0.13\0.126\0.123\0.121\0.12\0.118\0.116\0.114\0.112\0.111\0.11\0.109\0.108\0.106\0.104\0.103\0.102\0.101\0.0999\0.0994\0.0986\0.0979\0.0968\0.0958\0.0951\0.0946\0.0939\0.0933\0.0927\0.0922\0.0914\0.0913\0.0912\0.0907\0.09\0.0896\0.0894\0.0888\0.0885\0.0883\0.088\0.0876\0.0873\0.0872\0.087\0.0865\0.0863\0.0858\0.0858\0.0855\0.0851\0.0849\0.0847\0.0845\0.0842\0.0842\0.0838\0.0838\0.0836\0.0834\0.0834\0.083\0.083\0.0826\0.0827\0.0826\0.0824\0.0822\0.0821\0.0819\0.0819\0.0817\0.0816\0.0814\0.0812\0.0813\0.0811\0.0807\0.0808\0.0807\0.0806\0.0805\0.0805\0.0803\0.0802\0.0802\0.0801\0.0799\0.0798\0.0797\0.0797\0.0796\0.0794\0.0794\0.0793\0.0791\0.0791\0.0791\0.079\0.0789\0.0788\0.0787\0.0786\0.0786\0.0786\0.0784\0.0784\0.0784\0.0783\0.0783\0.0782\0.0781\0.0781\0.0781\0.078\0.078\0.078\0.0779\0.0779\0.0779\0.0779\0.0778\0.0778\0.0777\0.0778\0.0777\0.0777\0.0776\0.0776\0.0776\0.0776\0.0775\0.0775\0.0775\0.0775\0.0775\0.0775\0.0774\0.0774\0.0774\0.0774\0.0774\0.0774\0.0773\0.0774\0.0773\0.0773\0.0773\0.0773\0.0773\0.0773\0.0773\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0771\0.0771\0.0771\0.0771\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772\0.0772</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.78\0.444\0.41\0.425\0.395\0.335\0.272\0.232\0.221\0.196\0.193\0.19\0.187\0.18\0.174\0.171\0.168\0.171\0.18\0.181\0.183\0.18\0.181\0.179\0.176\0.177\0.182\0.184\0.184\0.177\0.179\0.184\0.182\0.18\0.181\0.179\0.178\0.178\0.18\0.177\0.178\0.177\0.178\0.176\0.176\0.18\0.18\0.179\0.177\0.178\0.179\0.179\0.177\0.178\0.179\0.18\0.181\0.18\0.18\0.179\0.179\0.177\0.178\0.178\0.179\0.179\0.177\0.178\0.177\0.177\0.177\0.177\0.178\0.179\0.179\0.177\0.177\0.177\0.179\0.177\0.177\0.176\0.175\0.176\0.177\0.179\0.178\0.177\0.177\0.177\0.178\0.178\0.178\0.177\0.178\0.179\0.179\0.179\0.178\0.178\0.179\0.177\0.176\0.177\0.178\0.178\0.177\0.178\0.178\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.177\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.175\0.176\0.175\0.175\0.175\0.175\0.175\0.175\0.175\0.175\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176\0.176</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>209</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="Lc7qZX" Title="Quasi-Newton method results">
   <Caption Id="iqDZ1H">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0772
0.176
208
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="vNeiKc" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="6mU8k3" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="5aICT4" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="O1afSi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.024929\117.142\6.80423\8.55635
0.000159802\0.750907\0.0436169\0.0548484
0.0159802\75.0907\4.36169\5.48484</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="oBl7hy" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="FdoXWm" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 289.</Text>
 </Task>
 <Task Id="IPBzTZ" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="YDLxO1" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="88Vu2J" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="bLIzzm" Title="Quasi-Newton method errors history">
   <Caption Id="QIYHjb">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.856825, and the final value after 151 epochs is 0.268343.
The initial value of the selection error is 0.666621, and the final value after 151 epochs is 0.418586.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151</X2Data>
   <Y1Data>0.857\0.549\0.488\0.424\0.399\0.392\0.389\0.385\0.38\0.375\0.37\0.367\0.364\0.362\0.36\0.358\0.354\0.352\0.349\0.348\0.345\0.343\0.34\0.338\0.336\0.334\0.332\0.331\0.33\0.328\0.327\0.325\0.325\0.323\0.322\0.321\0.319\0.318\0.317\0.315\0.313\0.312\0.311\0.31\0.309\0.308\0.307\0.306\0.305\0.304\0.304\0.303\0.302\0.301\0.299\0.299\0.298\0.297\0.296\0.295\0.295\0.294\0.293\0.293\0.292\0.291\0.29\0.289\0.288\0.288\0.287\0.286\0.286\0.285\0.285\0.284\0.283\0.283\0.282\0.282\0.281\0.281\0.281\0.281\0.28\0.28\0.279\0.279\0.278\0.278\0.278\0.278\0.277\0.277\0.276\0.276\0.276\0.276\0.275\0.275\0.274\0.274\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.268\0.268</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.667\0.447\0.336\0.309\0.28\0.276\0.27\0.268\0.266\0.277\0.275\0.274\0.273\0.277\0.279\0.286\0.285\0.281\0.285\0.281\0.287\0.286\0.283\0.29\0.294\0.293\0.295\0.292\0.293\0.302\0.305\0.302\0.3\0.301\0.3\0.305\0.308\0.31\0.311\0.31\0.315\0.315\0.32\0.318\0.316\0.315\0.317\0.319\0.324\0.326\0.323\0.325\0.327\0.325\0.334\0.333\0.338\0.335\0.336\0.341\0.341\0.343\0.345\0.344\0.347\0.349\0.349\0.348\0.353\0.349\0.351\0.353\0.354\0.356\0.358\0.357\0.361\0.363\0.362\0.363\0.364\0.366\0.365\0.366\0.37\0.366\0.367\0.364\0.367\0.368\0.368\0.368\0.37\0.37\0.372\0.373\0.376\0.376\0.377\0.379\0.382\0.38\0.382\0.383\0.385\0.384\0.386\0.388\0.39\0.392\0.393\0.391\0.392\0.392\0.394\0.395\0.395\0.396\0.395\0.396\0.397\0.397\0.396\0.398\0.399\0.4\0.4\0.401\0.402\0.402\0.404\0.405\0.405\0.405\0.407\0.408\0.408\0.409\0.409\0.409\0.41\0.411\0.411\0.412\0.412\0.414\0.414\0.415\0.416\0.417\0.419\0.419</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>152</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="C9PTDZ" Title="Quasi-Newton method results">
   <Caption Id="YHnvrR">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.268
0.419
151
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="OcwhhE" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="xmT9cS" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="GNrgcA" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="eh1rp4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228121 and its percentage error 2.28121</Caption>
   <Data>0.0069809\303.713\8.05724\14.378
2.0532e-5\0.893273\0.0236978\0.0422884
0.0020532\89.3273\2.36978\4.22884</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="s73CsA" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="7kmchL" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="kAYIwV" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="Qp8ZLz" Title="Quasi-Newton method errors history">
   <Caption Id="aPVpQE">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 3.41298, and the final value after 135 epochs is 0.44946.
The initial value of the selection error is 2.51712, and the final value after 135 epochs is 0.27579.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135</X2Data>
   <Y1Data>3.41\1.14\0.942\0.822\0.746\0.719\0.71\0.683\0.672\0.669\0.657\0.635\0.614\0.605\0.587\0.57\0.564\0.55\0.545\0.54\0.535\0.527\0.521\0.516\0.51\0.504\0.499\0.495\0.489\0.486\0.484\0.48\0.475\0.472\0.47\0.468\0.466\0.464\0.462\0.462\0.461\0.46\0.46\0.459\0.459\0.458\0.457\0.457\0.457\0.456\0.456\0.456\0.456\0.455\0.455\0.455\0.455\0.454\0.454\0.454\0.453\0.453\0.453\0.453\0.453\0.453\0.453\0.452\0.452\0.452\0.452\0.452\0.452\0.452\0.452\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.45\0.45\0.45\0.45\0.45\0.45\0.45\0.45\0.45\0.45\0.45\0.45\0.45\0.45\0.45\0.45\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449\0.449</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.52\0.7\0.619\0.523\0.517\0.46\0.467\0.433\0.435\0.421\0.384\0.369\0.395\0.401\0.389\0.376\0.369\0.367\0.365\0.343\0.338\0.322\0.326\0.332\0.318\0.306\0.307\0.306\0.292\0.296\0.303\0.305\0.292\0.281\0.277\0.276\0.275\0.271\0.274\0.272\0.275\0.28\0.283\0.285\0.279\0.277\0.277\0.276\0.279\0.278\0.273\0.269\0.271\0.269\0.271\0.271\0.272\0.275\0.278\0.277\0.277\0.278\0.274\0.273\0.274\0.273\0.272\0.273\0.273\0.274\0.273\0.274\0.272\0.272\0.271\0.271\0.273\0.273\0.272\0.272\0.271\0.271\0.272\0.271\0.271\0.271\0.271\0.271\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.274\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.276\0.276\0.276\0.276\0.275\0.275\0.275\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>136</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>4</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="Sgee1X" Title="Quasi-Newton method results">
   <Caption Id="BVIGJi">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.449
0.276
135
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Avn9t2" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="5ObDTV" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 65.</Text>
 </Task>
 <Task Id="2gPWyA" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="i7CeXT" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="xlKi2W" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="0ZzbMt" Title="Quasi-Newton method errors history">
   <Caption Id="fzX0RZ">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.43087, and the final value after 91 epochs is 0.450637.
The initial value of the selection error is 0.902681, and the final value after 91 epochs is 0.280497.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91</X2Data>
   <Y1Data>1.43\1.18\0.687\0.645\0.619\0.599\0.557\0.532\0.517\0.508\0.503\0.498\0.495\0.487\0.48\0.478\0.476\0.475\0.473\0.471\0.47\0.47\0.469\0.469\0.468\0.468\0.468\0.468\0.467\0.467\0.467\0.467\0.467\0.467\0.467\0.466\0.465\0.464\0.464\0.463\0.463\0.461\0.461\0.459\0.458\0.457\0.457\0.456\0.455\0.454\0.454\0.453\0.452\0.452\0.452\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.45\0.45\0.451\0.451\0.451\0.45\0.45\0.45\0.45\0.45\0.45\0.451\0.451\0.451\0.451\0.45\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451\0.451</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.903\0.731\0.423\0.468\0.433\0.386\0.311\0.286\0.3\0.29\0.294\0.287\0.281\0.277\0.269\0.274\0.28\0.283\0.279\0.279\0.274\0.279\0.276\0.273\0.274\0.275\0.276\0.276\0.276\0.273\0.273\0.275\0.274\0.273\0.272\0.27\0.27\0.269\0.269\0.269\0.273\0.279\0.28\0.283\0.278\0.276\0.281\0.279\0.276\0.275\0.274\0.276\0.278\0.278\0.279\0.278\0.278\0.278\0.277\0.278\0.279\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.278\0.279\0.279\0.279\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>92</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="fOD6tS" Title="Quasi-Newton method results">
   <Caption Id="ezLXBW">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.451
0.28
91
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="52Y7pV" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="ut0U9B" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="Velq4C" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="X06IX1">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0796335 and its percentage error 7.96336</Caption>
   <Data>0.00832558\30.0956\4.82812\4.13296
3.35709e-5\0.121353\0.0194682\0.0166652
0.00335709\12.1353\1.94682\1.66652</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="pm1Gyz" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="aWKmSW" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 65.</Text>
 </Task>
 <Task Id="mho28S" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="ZYDNUP" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="yu3try" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="tqzE3Q" Title="Quasi-Newton method errors history">
   <Caption Id="RKBuNC">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.0126, and the final value after 73 epochs is 0.441027.
The initial value of the selection error is 1.02854, and the final value after 73 epochs is 0.482081.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73</X2Data>
   <Y1Data>1.01\0.794\0.679\0.637\0.577\0.546\0.519\0.502\0.493\0.481\0.471\0.464\0.461\0.459\0.457\0.455\0.453\0.451\0.45\0.449\0.448\0.447\0.446\0.446\0.446\0.445\0.445\0.444\0.444\0.444\0.443\0.443\0.443\0.443\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.03\1.35\0.841\0.726\0.589\0.585\0.563\0.513\0.485\0.497\0.523\0.541\0.556\0.558\0.561\0.527\0.534\0.523\0.514\0.499\0.502\0.502\0.496\0.497\0.497\0.496\0.484\0.483\0.48\0.483\0.483\0.478\0.477\0.477\0.482\0.482\0.485\0.487\0.488\0.487\0.487\0.487\0.488\0.489\0.487\0.487\0.486\0.486\0.485\0.486\0.486\0.486\0.486\0.486\0.487\0.487\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.485\0.482\0.482</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>74</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="0wwnph" Title="Quasi-Newton method results">
   <Caption Id="lCCM7C">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.441
0.482
73
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="sQZFrc" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="y8K2ip" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="69PfvI" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="xBi6th">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.082324 and its percentage error 8.2324</Caption>
   <Data>0.0315018\42.1795\6.03663\4.74224
0.000437525\0.585826\0.0838421\0.0658645
0.0437525\58.5826\8.3842\6.58645</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="QzaS0W" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="PLold0" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 65.</Text>
 </Task>
 <Task Id="CoduqQ" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="SoWZ4a" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="oL57Jb" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="debEZ4" Title="Quasi-Newton method errors history">
   <Caption Id="cbQZVn">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.21981, and the final value after 112 epochs is 0.255819.
The initial value of the selection error is 0.386325, and the final value after 112 epochs is 0.212491.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112</X2Data>
   <Y1Data>1.22\0.887\0.625\0.478\0.412\0.368\0.347\0.322\0.313\0.303\0.299\0.294\0.29\0.287\0.284\0.278\0.277\0.276\0.275\0.273\0.272\0.271\0.27\0.27\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.268\0.268\0.267\0.267\0.266\0.266\0.266\0.266\0.265\0.265\0.265\0.264\0.264\0.263\0.262\0.262\0.261\0.26\0.26\0.259\0.259\0.259\0.258\0.258\0.258\0.258\0.258\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.386\0.556\0.605\0.329\0.294\0.241\0.234\0.219\0.221\0.215\0.219\0.221\0.217\0.218\0.208\0.211\0.209\0.212\0.215\0.212\0.208\0.208\0.211\0.214\0.215\0.214\0.212\0.212\0.213\0.215\0.213\0.215\0.214\0.218\0.224\0.226\0.229\0.229\0.224\0.225\0.223\0.221\0.22\0.218\0.22\0.224\0.225\0.222\0.222\0.221\0.22\0.221\0.222\0.222\0.221\0.221\0.221\0.221\0.22\0.22\0.222\0.22\0.22\0.219\0.217\0.216\0.216\0.217\0.218\0.217\0.216\0.214\0.214\0.215\0.215\0.215\0.214\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.213\0.212\0.212\0.212\0.212\0.212\0.212\0.212\0.212\0.212\0.212\0.212\0.212\0.212\0.212\0.212</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>113</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="JNUDFS" Title="Quasi-Newton method results">
   <Caption Id="USDMUt">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.256
0.212
112
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="FBFVgl" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="e7qPgI" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="NDVGLm" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9ZDuED">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222382 and its percentage error 2.22381</Caption>
   <Data>0.00122833\12.7532\0.764054\0.949944
7.22549e-5\0.750186\0.0449444\0.055879
0.00722549\75.0186\4.49444\5.5879</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="9ABylb" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="c02M4Z" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 289.</Text>
 </Task>
 <Task Id="5POkZi" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="ucSnrf" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="Wb4iFY" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="6fwtHV" Title="Quasi-Newton method errors history">
   <Caption Id="x0aAym">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.34553, and the final value after 150 epochs is 0.289588.
The initial value of the selection error is 0.889252, and the final value after 150 epochs is 0.502627.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150</X2Data>
   <Y1Data>1.35\0.623\0.597\0.552\0.518\0.495\0.485\0.476\0.469\0.465\0.463\0.459\0.457\0.454\0.451\0.446\0.444\0.441\0.439\0.437\0.436\0.434\0.431\0.428\0.425\0.423\0.42\0.416\0.414\0.411\0.407\0.404\0.403\0.401\0.398\0.397\0.395\0.393\0.392\0.389\0.388\0.385\0.382\0.379\0.377\0.375\0.372\0.37\0.367\0.365\0.363\0.36\0.358\0.356\0.354\0.352\0.351\0.349\0.348\0.347\0.346\0.345\0.343\0.342\0.341\0.341\0.339\0.338\0.337\0.336\0.335\0.334\0.333\0.332\0.331\0.331\0.33\0.329\0.328\0.328\0.327\0.326\0.325\0.325\0.324\0.323\0.323\0.322\0.321\0.32\0.32\0.319\0.318\0.318\0.317\0.317\0.316\0.315\0.314\0.313\0.313\0.312\0.311\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.308\0.308\0.307\0.307\0.306\0.306\0.306\0.305\0.305\0.304\0.304\0.304\0.303\0.303\0.303\0.302\0.301\0.301\0.3\0.3\0.299\0.298\0.298\0.298\0.297\0.297\0.296\0.296\0.295\0.294\0.294\0.294\0.293\0.293\0.293\0.292\0.292\0.291\0.291\0.29\0.29</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.889\0.434\0.436\0.375\0.306\0.279\0.29\0.279\0.274\0.275\0.274\0.284\0.281\0.289\0.298\0.3\0.29\0.283\0.288\0.29\0.288\0.287\0.285\0.283\0.289\0.294\0.303\0.313\0.307\0.316\0.318\0.317\0.315\0.32\0.332\0.33\0.33\0.337\0.336\0.345\0.348\0.347\0.349\0.346\0.346\0.354\0.348\0.349\0.354\0.354\0.356\0.359\0.357\0.352\0.359\0.354\0.359\0.36\0.365\0.366\0.369\0.365\0.368\0.372\0.373\0.374\0.378\0.385\0.384\0.386\0.387\0.387\0.389\0.391\0.394\0.396\0.402\0.397\0.397\0.397\0.4\0.399\0.398\0.399\0.402\0.407\0.409\0.41\0.413\0.415\0.414\0.416\0.417\0.42\0.423\0.421\0.423\0.422\0.424\0.427\0.429\0.431\0.434\0.433\0.433\0.431\0.433\0.434\0.431\0.43\0.432\0.432\0.434\0.438\0.443\0.442\0.439\0.44\0.443\0.444\0.449\0.446\0.447\0.445\0.448\0.45\0.456\0.457\0.461\0.459\0.466\0.467\0.468\0.471\0.475\0.476\0.475\0.478\0.479\0.486\0.489\0.49\0.496\0.491\0.492\0.49\0.496\0.496\0.499\0.501\0.503</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>151</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="rdnMab" Title="Quasi-Newton method results">
   <Caption Id="49dTIr">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.29
0.503
150
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="C79xIO" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="JhZT65" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="AqRVJP" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="A0Bl6m">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0796335 and its percentage error 7.96336</Caption>
   <Data>0.000701904\45.4376\6.65091\5.55277
2.83026e-6\0.183216\0.0268182\0.0223902
0.000283026\18.3216\2.68182\2.23902</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="7ZNJET" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="9eSeD5" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 289.</Text>
 </Task>
 <Task Id="5wZmDq" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="UXeEDf" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="bZoyZb" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="84pvce" Title="Quasi-Newton method errors history">
   <Caption Id="OGlMVS">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.26145, and the final value after 209 epochs is 0.29682.
The initial value of the selection error is 1.40203, and the final value after 209 epochs is 0.51314.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209</X2Data>
   <Y1Data>1.26\0.783\0.683\0.633\0.57\0.523\0.493\0.487\0.478\0.469\0.464\0.459\0.454\0.448\0.446\0.443\0.439\0.436\0.432\0.427\0.425\0.422\0.419\0.415\0.41\0.407\0.405\0.401\0.397\0.392\0.39\0.389\0.385\0.381\0.378\0.374\0.371\0.368\0.366\0.363\0.36\0.358\0.356\0.353\0.351\0.349\0.347\0.345\0.343\0.34\0.339\0.337\0.336\0.334\0.333\0.332\0.331\0.33\0.329\0.328\0.327\0.326\0.326\0.325\0.324\0.323\0.322\0.322\0.321\0.321\0.32\0.32\0.319\0.319\0.318\0.318\0.317\0.317\0.316\0.316\0.315\0.315\0.315\0.314\0.314\0.313\0.313\0.313\0.312\0.312\0.312\0.311\0.311\0.311\0.31\0.31\0.309\0.309\0.309\0.308\0.308\0.308\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.301\0.301\0.301\0.301\0.301\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.4\1.14\0.828\0.704\0.543\0.533\0.428\0.448\0.451\0.422\0.431\0.474\0.47\0.482\0.459\0.465\0.488\0.473\0.483\0.507\0.49\0.484\0.483\0.507\0.537\0.545\0.532\0.562\0.575\0.585\0.565\0.543\0.545\0.542\0.545\0.547\0.52\0.506\0.506\0.5\0.509\0.485\0.497\0.497\0.47\0.462\0.452\0.463\0.452\0.439\0.441\0.436\0.436\0.439\0.451\0.456\0.454\0.461\0.459\0.455\0.459\0.459\0.458\0.464\0.471\0.473\0.475\0.483\0.482\0.485\0.494\0.485\0.49\0.494\0.496\0.499\0.492\0.501\0.5\0.498\0.497\0.497\0.499\0.505\0.499\0.497\0.492\0.498\0.498\0.497\0.496\0.499\0.502\0.502\0.505\0.504\0.51\0.509\0.507\0.508\0.51\0.508\0.514\0.513\0.516\0.519\0.519\0.515\0.517\0.519\0.516\0.519\0.518\0.517\0.517\0.518\0.515\0.519\0.518\0.521\0.521\0.521\0.521\0.519\0.517\0.517\0.519\0.52\0.522\0.522\0.519\0.521\0.521\0.519\0.523\0.523\0.519\0.518\0.517\0.516\0.516\0.513\0.512\0.515\0.514\0.513\0.513\0.516\0.515\0.513\0.515\0.516\0.516\0.515\0.515\0.516\0.514\0.513\0.513\0.513\0.515\0.515\0.514\0.514\0.515\0.515\0.515\0.514\0.514\0.514\0.513\0.513\0.513\0.512\0.512\0.511\0.511\0.51\0.509\0.509\0.509\0.509\0.509\0.509\0.509\0.51\0.51\0.511\0.511\0.511\0.511\0.511\0.511\0.511\0.511\0.511\0.511\0.511\0.511\0.512\0.512\0.512\0.512\0.513\0.513\0.514\0.513\0.512\0.512\0.513</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>210</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="bWuCZu" Title="Quasi-Newton method results">
   <Caption Id="1910W9">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.297
0.513
209
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Is4h9B" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="Hgs6fV" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="N31WCI" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="iFneIg">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.082324 and its percentage error 8.2324</Caption>
   <Data>0.00120163\43.8279\6.1053\4.98557
1.66893e-5\0.608721\0.0847959\0.069244
0.00166893\60.8721\8.47959\6.9244</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="xWKMdG" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="W0JmYM" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 289.</Text>
 </Task>
 <Task Id="CgU1a9" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="SWysJ5" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="oJP0yU" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="gJSoJ3" Title="Quasi-Newton method errors history">
   <Caption Id="tFnzNj">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.964867, and the final value after 183 epochs is 0.185909.
The initial value of the selection error is 0.441533, and the final value after 183 epochs is 0.271989.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183</X2Data>
   <Y1Data>0.965\0.631\0.438\0.357\0.337\0.325\0.311\0.299\0.291\0.285\0.28\0.277\0.274\0.27\0.268\0.266\0.264\0.262\0.26\0.257\0.255\0.254\0.252\0.251\0.249\0.248\0.246\0.245\0.243\0.24\0.238\0.236\0.235\0.233\0.231\0.229\0.228\0.226\0.226\0.225\0.224\0.223\0.222\0.222\0.221\0.22\0.219\0.218\0.217\0.216\0.215\0.214\0.214\0.213\0.212\0.212\0.211\0.209\0.209\0.208\0.207\0.206\0.206\0.205\0.204\0.204\0.203\0.203\0.202\0.201\0.201\0.201\0.2\0.199\0.199\0.198\0.198\0.198\0.197\0.197\0.197\0.196\0.196\0.196\0.196\0.195\0.195\0.194\0.194\0.194\0.193\0.193\0.192\0.193\0.192\0.192\0.192\0.192\0.191\0.191\0.191\0.191\0.191\0.19\0.19\0.19\0.19\0.19\0.19\0.19\0.19\0.19\0.189\0.189\0.189\0.189\0.189\0.189\0.189\0.189\0.188\0.188\0.188\0.188\0.188\0.188\0.188\0.188\0.188\0.188\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.187\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186\0.186</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.442\0.592\0.312\0.263\0.241\0.235\0.251\0.229\0.242\0.233\0.235\0.233\0.231\0.223\0.222\0.221\0.231\0.229\0.225\0.224\0.218\0.219\0.224\0.228\0.228\0.224\0.221\0.221\0.226\0.232\0.23\0.23\0.23\0.232\0.235\0.232\0.232\0.233\0.232\0.231\0.235\0.235\0.236\0.237\0.237\0.238\0.238\0.238\0.237\0.238\0.238\0.241\0.238\0.24\0.24\0.242\0.243\0.247\0.247\0.246\0.246\0.243\0.244\0.244\0.247\0.244\0.243\0.244\0.245\0.245\0.247\0.246\0.244\0.245\0.245\0.246\0.246\0.247\0.249\0.249\0.25\0.251\0.251\0.251\0.25\0.251\0.252\0.252\0.252\0.252\0.253\0.256\0.257\0.255\0.255\0.258\0.259\0.258\0.259\0.259\0.258\0.259\0.258\0.258\0.26\0.259\0.26\0.26\0.262\0.261\0.261\0.26\0.261\0.261\0.262\0.263\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.263\0.263\0.262\0.262\0.262\0.262\0.261\0.261\0.26\0.26\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.26\0.26\0.259\0.26\0.261\0.261\0.262\0.262\0.263\0.263\0.264\0.264\0.264\0.265\0.265\0.267\0.266\0.268\0.267\0.268\0.268\0.269\0.269\0.27\0.27\0.271\0.271\0.271\0.27\0.271\0.271\0.272\0.272\0.272\0.272</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>184</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="dDuAyx" Title="Quasi-Newton method results">
   <Caption Id="lTeDzd">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.186
0.272
183
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="JtEjnJ" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="q65YXV" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="wXFG2o" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="keBg9p">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222382 and its percentage error 2.22381</Caption>
   <Data>0\11.6898\0.79512\0.976871
0\0.687633\0.0467718\0.057463
0\68.7633\4.67718\5.7463</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="3e36iC" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="mgFjei" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 734.</Text>
 </Task>
 <Task Id="FBXLJG" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="8XAgzj" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="mLYqvW" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="xLr8HV" Title="Quasi-Newton method errors history">
   <Caption Id="qJ7rM7">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.06032, and the final value after 226 epochs is 0.278766.
The initial value of the selection error is 0.761252, and the final value after 226 epochs is 0.281907.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226</X2Data>
   <Y1Data>1.06\0.787\0.697\0.634\0.551\0.519\0.489\0.466\0.447\0.434\0.423\0.411\0.398\0.386\0.379\0.37\0.364\0.359\0.352\0.349\0.344\0.34\0.337\0.335\0.332\0.329\0.327\0.325\0.323\0.322\0.321\0.319\0.318\0.317\0.316\0.315\0.314\0.314\0.313\0.312\0.311\0.31\0.31\0.309\0.309\0.308\0.308\0.307\0.307\0.306\0.306\0.306\0.305\0.305\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.302\0.302\0.301\0.301\0.301\0.3\0.3\0.3\0.299\0.299\0.299\0.298\0.298\0.298\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.289\0.289\0.289\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.761\0.723\0.615\0.562\0.547\0.517\0.459\0.418\0.413\0.415\0.415\0.403\0.375\0.353\0.343\0.334\0.327\0.332\0.321\0.315\0.3\0.29\0.288\0.285\0.283\0.285\0.283\0.278\0.276\0.275\0.277\0.278\0.278\0.274\0.271\0.271\0.271\0.273\0.272\0.272\0.274\0.272\0.274\0.273\0.273\0.272\0.272\0.273\0.273\0.274\0.275\0.274\0.274\0.274\0.274\0.272\0.271\0.271\0.271\0.271\0.271\0.271\0.272\0.272\0.272\0.271\0.272\0.271\0.271\0.273\0.273\0.274\0.275\0.275\0.275\0.274\0.275\0.275\0.275\0.276\0.277\0.276\0.276\0.277\0.277\0.278\0.278\0.278\0.278\0.279\0.278\0.28\0.28\0.28\0.28\0.281\0.282\0.282\0.282\0.283\0.283\0.283\0.285\0.284\0.284\0.284\0.284\0.284\0.283\0.284\0.284\0.285\0.285\0.286\0.285\0.285\0.284\0.284\0.285\0.285\0.286\0.286\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.287\0.286\0.287\0.287\0.287\0.286\0.286\0.287\0.286\0.287\0.287\0.286\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.285\0.286\0.286\0.286\0.286\0.287\0.286\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.285\0.286\0.286\0.286\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.285\0.284\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.284\0.284\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>227</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="fxV7IY" Title="Quasi-Newton method results">
   <Caption Id="dGLWP6">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.279
0.282
226
00:00:02
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="S71Hj5" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="wgPYRZ" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="3okOZC" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="3WpsxY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.0223007\114.627\6.49163\8.11881
0.000142953\0.734786\0.041613\0.0520437
0.0142953\73.4786\4.1613\5.20437</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="wGJGYj" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="HaFs86">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.00990105\304.707\7.31749\14.2215
2.91207e-5\0.896197\0.021522\0.0418279
0.00291207\89.6197\2.1522\4.18279</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ktFPuV" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="gB07Iw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.021534\25.0639\5.50884\4.31771
8.68305e-5\0.101064\0.0222131\0.0174101
0.00868305\10.1064\2.22131\1.74101</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="9k4l6C" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ERkNMX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.0220718\52.5998\5.9175\4.83992
0.000306553\0.730553\0.0821875\0.0672211
0.0306553\73.0553\8.21875\6.72211</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="kFSxZ8" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="otDA8t">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0\15.4386\0.763964\1.06442
0\0.908154\0.0449391\0.0626132
0\90.8155\4.49391\6.26132</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="xTUbGC" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="Fy3TdB" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 1625.</Text>
 </Task>
 <Task Id="FV3Omh" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="0uj70E" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="eKtDvJ" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="UiRgoD" Title="Quasi-Newton method errors history">
   <Caption Id="GS8tfe">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.10586, and the final value after 163 epochs is 0.199283.
The initial value of the selection error is 0.779003, and the final value after 163 epochs is 0.360586.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163</X2Data>
   <Y1Data>1.11\0.758\0.625\0.52\0.488\0.464\0.439\0.425\0.406\0.39\0.379\0.37\0.358\0.349\0.342\0.337\0.332\0.329\0.325\0.322\0.317\0.313\0.311\0.308\0.304\0.302\0.299\0.297\0.295\0.293\0.291\0.289\0.287\0.286\0.283\0.282\0.28\0.279\0.277\0.276\0.274\0.273\0.271\0.269\0.268\0.267\0.265\0.264\0.263\0.261\0.26\0.259\0.257\0.256\0.255\0.254\0.252\0.251\0.25\0.25\0.248\0.247\0.246\0.245\0.244\0.243\0.242\0.242\0.24\0.24\0.239\0.239\0.237\0.236\0.235\0.235\0.234\0.233\0.233\0.232\0.231\0.231\0.23\0.229\0.228\0.227\0.227\0.226\0.225\0.225\0.224\0.223\0.223\0.222\0.221\0.221\0.22\0.22\0.219\0.219\0.218\0.218\0.217\0.217\0.217\0.216\0.216\0.215\0.215\0.214\0.214\0.214\0.213\0.213\0.212\0.212\0.212\0.211\0.211\0.21\0.21\0.209\0.209\0.209\0.208\0.208\0.208\0.207\0.207\0.207\0.206\0.206\0.206\0.206\0.205\0.205\0.205\0.204\0.204\0.204\0.204\0.203\0.203\0.203\0.203\0.202\0.202\0.202\0.202\0.202\0.202\0.201\0.201\0.201\0.201\0.201\0.201\0.2\0.2\0.2\0.2\0.2\0.199\0.199</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.779\0.701\0.545\0.485\0.486\0.471\0.422\0.382\0.368\0.375\0.36\0.349\0.324\0.298\0.292\0.278\0.279\0.281\0.278\0.276\0.273\0.275\0.278\0.274\0.273\0.272\0.272\0.269\0.266\0.267\0.268\0.269\0.273\0.273\0.273\0.277\0.278\0.28\0.279\0.277\0.279\0.28\0.28\0.286\0.285\0.288\0.288\0.292\0.292\0.294\0.295\0.295\0.3\0.299\0.305\0.303\0.304\0.302\0.304\0.302\0.303\0.306\0.309\0.311\0.315\0.311\0.312\0.312\0.316\0.314\0.317\0.317\0.318\0.318\0.317\0.315\0.317\0.318\0.319\0.317\0.319\0.319\0.321\0.32\0.323\0.323\0.324\0.326\0.328\0.329\0.329\0.331\0.331\0.33\0.332\0.331\0.332\0.331\0.331\0.331\0.332\0.333\0.334\0.336\0.335\0.336\0.336\0.338\0.339\0.338\0.338\0.338\0.339\0.339\0.341\0.342\0.342\0.341\0.341\0.343\0.343\0.343\0.344\0.342\0.341\0.34\0.342\0.342\0.344\0.344\0.344\0.345\0.343\0.343\0.345\0.347\0.346\0.347\0.347\0.348\0.348\0.349\0.35\0.351\0.352\0.352\0.352\0.353\0.354\0.354\0.354\0.355\0.355\0.356\0.356\0.356\0.357\0.357\0.357\0.359\0.36\0.359\0.36\0.361</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>164</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="1XgsWy" Title="Quasi-Newton method results">
   <Caption Id="XlwJTm">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.199
0.361
163
00:00:06
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="jCeMVC" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="IUzgXc" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="G8ESFM" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="iyzqKR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.0214844\114.846\6.71285\8.32135
0.00013772\0.73619\0.0430311\0.053342
0.013772\73.619\4.30311\5.3342</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="zDDwiB" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="7grgBj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.00131989\308.533\7.92647\14.6088
3.88202e-6\0.90745\0.0233131\0.0429672
0.000388202\90.745\2.33131\4.29672</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Qg8f5A" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="OCITj8">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.0188141\33.7478\7.02568\5.65651
7.58633e-5\0.13608\0.0283293\0.0228085
0.00758633\13.608\2.83293\2.28085</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="P7mOnd" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Nv2SUF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.0471029\50.5509\6.38827\5.11933
0.000654207\0.702095\0.0887259\0.0711018
0.0654207\70.2095\8.87259\7.11018</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="co3BGV" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="1ayA49">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0\12.6946\0.849488\1.03998
0\0.746743\0.0499699\0.0611755
0\74.6743\4.99699\6.11755</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="apAR1W" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="w9YARg" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 248.</Text>
 </Task>
 <Task Id="cnQgly" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="lIvuKh" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="Usj3AK" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="CVpW1s" Title="Quasi-Newton method errors history">
   <Caption Id="K8NXfe">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.0665, and the final value after 132 epochs is 0.366339.
The initial value of the selection error is 0.77584, and the final value after 132 epochs is 0.361434.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132</X2Data>
   <Y1Data>1.07\0.874\0.759\0.718\0.669\0.629\0.589\0.571\0.553\0.531\0.515\0.491\0.48\0.47\0.457\0.448\0.437\0.429\0.421\0.415\0.41\0.406\0.402\0.397\0.394\0.393\0.391\0.388\0.386\0.384\0.382\0.381\0.379\0.378\0.377\0.376\0.375\0.374\0.373\0.372\0.372\0.372\0.371\0.371\0.37\0.37\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.776\0.727\0.698\0.662\0.569\0.554\0.549\0.526\0.504\0.487\0.489\0.469\0.461\0.435\0.428\0.42\0.409\0.406\0.404\0.404\0.403\0.399\0.394\0.39\0.388\0.383\0.38\0.378\0.377\0.378\0.375\0.376\0.374\0.373\0.369\0.368\0.367\0.366\0.364\0.364\0.365\0.365\0.364\0.365\0.364\0.365\0.364\0.364\0.363\0.363\0.364\0.364\0.364\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.362\0.363\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>133</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="fRepzO" Title="Quasi-Newton method results">
   <Caption Id="WN1FQe">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.366
0.361
132
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="jzAPyd" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="As2aZ7" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="c7apEA" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="FqYNr9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.000465393\113.77\8.32905\9.23432
2.98329e-6\0.729295\0.0533913\0.0591943
0.000298329\72.9295\5.33913\5.91943</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="sLEU1i" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Tk2RPf">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.0125351\309.395\6.85429\14.3032
3.68679e-5\0.909984\0.0201597\0.0420682
0.00368679\90.9984\2.01597\4.20682</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="qBjRgL" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="8xvfRp">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.014904\27.2354\5.96537\4.84832
6.00969e-5\0.10982\0.0240539\0.0195497
0.00600969\10.982\2.40539\1.95497</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="vL0whb" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="OLZ9aF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.0261021\43.3099\7.46536\4.97486
0.000362529\0.601527\0.103686\0.0690952
0.0362529\60.1527\10.3686\6.90952</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="i8DkJI" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="o3eTVI">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392044 and its percentage error 3.92042</Caption>
   <Data>0.00232077\14.846\0.670002\0.97962
0.000136516\0.873292\0.0394119\0.0576247
0.0136516\87.3292\3.94119\5.76247</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="XTDcdu" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="Edst9l" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 694.</Text>
 </Task>
 <Task Id="X5PkVg" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="E1Fkbk" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="jXLsQX" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="IstNDl" Title="Quasi-Newton method errors history">
   <Caption Id="FonSMy">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.941859, and the final value after 134 epochs is 0.161702.
The initial value of the selection error is 0.627125, and the final value after 134 epochs is 0.662373.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134</X2Data>
   <Y1Data>0.942\0.716\0.64\0.582\0.495\0.455\0.43\0.408\0.392\0.381\0.374\0.365\0.359\0.353\0.35\0.344\0.34\0.334\0.33\0.326\0.323\0.319\0.315\0.312\0.308\0.305\0.302\0.297\0.294\0.291\0.287\0.283\0.279\0.276\0.272\0.268\0.265\0.262\0.259\0.256\0.253\0.251\0.249\0.246\0.244\0.242\0.24\0.239\0.237\0.234\0.233\0.231\0.229\0.227\0.226\0.223\0.222\0.221\0.219\0.217\0.216\0.214\0.213\0.211\0.209\0.208\0.206\0.205\0.203\0.202\0.201\0.199\0.199\0.197\0.196\0.195\0.194\0.193\0.192\0.191\0.189\0.189\0.188\0.187\0.186\0.185\0.184\0.184\0.183\0.183\0.182\0.181\0.181\0.18\0.179\0.179\0.178\0.178\0.177\0.177\0.176\0.176\0.175\0.175\0.174\0.174\0.173\0.173\0.172\0.172\0.172\0.171\0.171\0.17\0.17\0.169\0.169\0.169\0.168\0.167\0.167\0.167\0.166\0.166\0.165\0.165\0.165\0.164\0.164\0.163\0.163\0.162\0.163\0.162\0.162</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.627\0.557\0.451\0.416\0.353\0.316\0.325\0.317\0.317\0.299\0.278\0.282\0.288\0.288\0.282\0.285\0.288\0.306\0.306\0.309\0.309\0.306\0.312\0.321\0.326\0.324\0.334\0.342\0.341\0.345\0.342\0.349\0.362\0.355\0.365\0.378\0.381\0.384\0.38\0.39\0.388\0.395\0.396\0.407\0.406\0.416\0.414\0.419\0.422\0.43\0.43\0.437\0.447\0.446\0.444\0.452\0.452\0.459\0.46\0.465\0.468\0.475\0.474\0.476\0.485\0.488\0.491\0.494\0.501\0.505\0.498\0.506\0.506\0.512\0.513\0.516\0.524\0.528\0.527\0.528\0.534\0.534\0.535\0.539\0.541\0.544\0.549\0.553\0.553\0.557\0.562\0.566\0.568\0.571\0.568\0.569\0.579\0.58\0.58\0.58\0.589\0.584\0.587\0.589\0.59\0.592\0.595\0.599\0.606\0.604\0.606\0.611\0.611\0.614\0.614\0.612\0.614\0.617\0.621\0.628\0.627\0.627\0.63\0.636\0.64\0.644\0.646\0.648\0.651\0.654\0.653\0.658\0.655\0.659\0.662</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>135</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="rNGLF4" Title="Quasi-Newton method results">
   <Caption Id="gYHZlc">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.162
0.662
134
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="do9TiM" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="d7MwpA" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="rmie0N" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="MhcE22">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228467 and its percentage error 2.28467</Caption>
   <Data>0.0190582\304.804\9.59444\14.7574
5.60536e-5\0.896482\0.0282189\0.043404
0.00560536\89.6482\2.82189\4.3404</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="am6NS1" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="xfg7Z6" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 232.</Text>
 </Task>
 <Task Id="hCS6io" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="OJMEms" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="VcM2s5" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="u1cxZK" Title="Quasi-Newton method errors history">
   <Caption Id="leVLt0">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.837825, and the final value after 169 epochs is 0.294721.
The initial value of the selection error is 0.694024, and the final value after 169 epochs is 0.373766.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169</X2Data>
   <Y1Data>0.838\0.718\0.628\0.578\0.518\0.479\0.449\0.436\0.422\0.41\0.403\0.393\0.385\0.379\0.373\0.366\0.363\0.36\0.356\0.354\0.35\0.348\0.346\0.344\0.343\0.34\0.338\0.337\0.336\0.334\0.333\0.332\0.331\0.33\0.329\0.328\0.327\0.326\0.326\0.325\0.324\0.324\0.323\0.322\0.322\0.322\0.321\0.32\0.32\0.319\0.318\0.318\0.317\0.317\0.316\0.316\0.315\0.315\0.315\0.314\0.314\0.314\0.313\0.313\0.312\0.312\0.311\0.311\0.31\0.31\0.31\0.309\0.309\0.309\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.302\0.302\0.302\0.302\0.301\0.301\0.301\0.301\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.694\0.476\0.412\0.366\0.371\0.347\0.318\0.328\0.319\0.31\0.297\0.296\0.286\0.291\0.287\0.282\0.284\0.284\0.28\0.281\0.284\0.284\0.286\0.284\0.281\0.28\0.282\0.286\0.285\0.287\0.287\0.289\0.287\0.289\0.293\0.293\0.295\0.297\0.296\0.301\0.303\0.303\0.302\0.304\0.304\0.304\0.305\0.308\0.307\0.31\0.311\0.313\0.313\0.312\0.317\0.319\0.321\0.321\0.323\0.324\0.324\0.327\0.329\0.329\0.335\0.335\0.341\0.341\0.34\0.341\0.342\0.343\0.343\0.344\0.347\0.348\0.349\0.351\0.352\0.353\0.353\0.354\0.353\0.354\0.353\0.354\0.354\0.355\0.356\0.355\0.355\0.355\0.356\0.356\0.357\0.357\0.358\0.359\0.36\0.362\0.364\0.366\0.368\0.367\0.371\0.369\0.369\0.37\0.372\0.372\0.375\0.374\0.374\0.376\0.379\0.381\0.381\0.378\0.377\0.378\0.378\0.379\0.378\0.379\0.377\0.377\0.376\0.376\0.375\0.375\0.375\0.374\0.374\0.375\0.376\0.377\0.376\0.375\0.374\0.375\0.375\0.375\0.375\0.376\0.375\0.375\0.375\0.375\0.375\0.375\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>170</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="HUwccl" Title="Quasi-Newton method results">
   <Caption Id="Z2FNrC">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.295
0.374
169
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="kY4DZ9" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="b6A4Oa" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="4VIguk" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="RwE8Dw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228467 and its percentage error 2.28467</Caption>
   <Data>0.00920486\298.227\7.3308\14.0108
2.70731e-5\0.877138\0.0215612\0.0412081
0.00270731\87.7138\2.15612\4.12081</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="U9b2Ad" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="StI8ih" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 232.</Text>
 </Task>
 <Task Id="RuOKjs" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="CG4bLH" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="yCThCD" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="PjwEOB" Title="Quasi-Newton method errors history">
   <Caption Id="A1DsXm">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.949116, and the final value after 215 epochs is 0.0871073.
The initial value of the selection error is 0.908206, and the final value after 215 epochs is 0.168613.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215</X2Data>
   <Y1Data>0.949\0.847\0.712\0.578\0.423\0.35\0.313\0.276\0.241\0.223\0.21\0.197\0.181\0.171\0.16\0.154\0.15\0.146\0.141\0.137\0.132\0.129\0.126\0.123\0.121\0.118\0.117\0.115\0.113\0.111\0.108\0.107\0.106\0.105\0.103\0.102\0.101\0.0999\0.099\0.0983\0.0974\0.0966\0.0961\0.0952\0.0947\0.0944\0.0938\0.0932\0.0929\0.0925\0.0921\0.0919\0.0918\0.0916\0.0913\0.091\0.0908\0.0907\0.0907\0.0905\0.0904\0.0903\0.0901\0.0901\0.09\0.0899\0.0897\0.0897\0.0897\0.0896\0.0895\0.0894\0.0893\0.0892\0.0892\0.0891\0.0891\0.0891\0.0891\0.089\0.089\0.0889\0.0889\0.0889\0.0889\0.0888\0.0888\0.0888\0.0888\0.0888\0.0887\0.0887\0.0888\0.0888\0.0888\0.0887\0.0887\0.0887\0.0887\0.0887\0.0887\0.0887\0.0887\0.0886\0.0886\0.0885\0.0885\0.0885\0.0884\0.0884\0.0884\0.0884\0.0884\0.0884\0.0883\0.0883\0.0883\0.0883\0.0883\0.0882\0.0882\0.0882\0.0881\0.0881\0.0881\0.0881\0.0881\0.0881\0.0881\0.0881\0.088\0.088\0.088\0.088\0.088\0.088\0.0879\0.0879\0.0879\0.0879\0.0879\0.0879\0.0879\0.0879\0.0879\0.0878\0.0878\0.0878\0.0878\0.0878\0.0878\0.0877\0.0877\0.0877\0.0877\0.0877\0.0877\0.0876\0.0876\0.0876\0.0876\0.0875\0.0875\0.0875\0.0874\0.0874\0.0875\0.0874\0.0874\0.0874\0.0873\0.0874\0.0874\0.0874\0.0874\0.0873\0.0873\0.0873\0.0873\0.0872\0.0872\0.0872\0.0872\0.0872\0.0872\0.0872\0.0872\0.0872\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871\0.0871</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.908\0.881\0.735\0.538\0.432\0.367\0.333\0.303\0.274\0.268\0.271\0.241\0.22\0.211\0.202\0.207\0.206\0.19\0.18\0.172\0.174\0.174\0.172\0.175\0.173\0.176\0.175\0.175\0.175\0.174\0.172\0.172\0.176\0.179\0.178\0.178\0.179\0.184\0.183\0.184\0.184\0.184\0.185\0.182\0.181\0.178\0.178\0.18\0.182\0.179\0.177\0.175\0.171\0.171\0.172\0.173\0.172\0.171\0.171\0.171\0.171\0.172\0.171\0.171\0.171\0.171\0.17\0.17\0.169\0.169\0.169\0.169\0.169\0.17\0.17\0.17\0.17\0.17\0.171\0.17\0.17\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.17\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.17\0.17\0.17\0.17\0.169\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.17\0.171\0.17\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.171\0.172\0.172\0.172\0.172\0.172\0.172\0.172\0.172\0.172\0.173\0.173\0.173\0.173\0.173\0.173\0.173\0.173\0.173\0.173\0.173\0.173\0.173\0.173\0.172\0.172\0.172\0.171\0.171\0.17\0.171\0.171\0.171\0.171\0.17\0.17\0.17\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169\0.169</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>216</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="WKLrUE" Title="Quasi-Newton method results">
   <Caption Id="pkXs7a">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0871
0.169
215
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="UGWv6D" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="P3rxxz" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="XYuLEP" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="we3v3o">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788808 and its percentage error 7.88807</Caption>
   <Data>0.00893402\120.398\6.29562\8.27671
5.72694e-5\0.77178\0.0403565\0.0530558
0.00572694\77.178\4.03565\5.30558</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="aLVmnm" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="eYCgQ8" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 232.</Text>
 </Task>
 <Task Id="EHPSmV" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="GUs9uv" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="YhcWXq" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="qu31kL" Title="Quasi-Newton method errors history">
   <Caption Id="xC208g">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.51748, and the final value after 183 epochs is 0.367138.
The initial value of the selection error is 1.03258, and the final value after 183 epochs is 0.38824.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183</X2Data>
   <Y1Data>1.52\0.916\0.701\0.613\0.575\0.556\0.54\0.524\0.506\0.496\0.487\0.477\0.472\0.469\0.462\0.458\0.453\0.449\0.446\0.444\0.441\0.438\0.436\0.434\0.432\0.431\0.429\0.427\0.426\0.424\0.423\0.422\0.42\0.419\0.418\0.417\0.416\0.415\0.415\0.414\0.413\0.412\0.412\0.411\0.409\0.406\0.405\0.404\0.403\0.403\0.402\0.401\0.401\0.4\0.4\0.4\0.399\0.399\0.398\0.397\0.397\0.396\0.395\0.395\0.395\0.394\0.393\0.393\0.392\0.391\0.391\0.391\0.39\0.39\0.389\0.389\0.389\0.389\0.388\0.388\0.388\0.387\0.387\0.387\0.387\0.386\0.386\0.386\0.386\0.385\0.385\0.385\0.384\0.384\0.383\0.383\0.383\0.382\0.382\0.382\0.382\0.382\0.381\0.381\0.381\0.381\0.381\0.38\0.38\0.38\0.38\0.38\0.38\0.379\0.379\0.379\0.379\0.379\0.378\0.378\0.378\0.378\0.377\0.377\0.377\0.377\0.377\0.377\0.376\0.376\0.376\0.376\0.376\0.376\0.375\0.375\0.375\0.375\0.375\0.374\0.374\0.374\0.374\0.374\0.373\0.373\0.373\0.372\0.372\0.371\0.371\0.371\0.371\0.371\0.371\0.37\0.37\0.37\0.37\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.03\0.577\0.5\0.484\0.36\0.367\0.359\0.351\0.312\0.299\0.295\0.29\0.287\0.275\0.293\0.307\0.32\0.318\0.308\0.307\0.311\0.312\0.314\0.321\0.317\0.322\0.324\0.327\0.315\0.311\0.31\0.313\0.315\0.319\0.322\0.317\0.313\0.31\0.309\0.307\0.31\0.309\0.31\0.307\0.309\0.318\0.32\0.318\0.318\0.313\0.31\0.311\0.313\0.314\0.314\0.315\0.32\0.32\0.318\0.323\0.325\0.326\0.332\0.333\0.335\0.336\0.339\0.338\0.336\0.339\0.338\0.342\0.342\0.342\0.342\0.343\0.342\0.343\0.343\0.341\0.34\0.34\0.338\0.336\0.336\0.337\0.34\0.341\0.339\0.338\0.337\0.337\0.335\0.337\0.341\0.343\0.34\0.341\0.34\0.341\0.344\0.342\0.343\0.343\0.345\0.344\0.344\0.343\0.345\0.345\0.346\0.347\0.346\0.346\0.346\0.344\0.343\0.345\0.348\0.349\0.35\0.351\0.351\0.352\0.351\0.351\0.352\0.353\0.353\0.352\0.351\0.35\0.351\0.352\0.351\0.351\0.352\0.352\0.354\0.354\0.354\0.354\0.355\0.356\0.357\0.358\0.36\0.362\0.364\0.368\0.369\0.373\0.373\0.371\0.372\0.372\0.373\0.373\0.375\0.375\0.379\0.378\0.378\0.382\0.381\0.378\0.379\0.381\0.382\0.382\0.382\0.382\0.383\0.385\0.386\0.386\0.386\0.387\0.387\0.388\0.387\0.387\0.387\0.388</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>184</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="GImsPc" Title="Quasi-Newton method results">
   <Caption Id="6cFmgN">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.367
0.388
183
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="H7X3cq" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="THBVWd" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="732AZV" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="MpfivT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0796598 and its percentage error 7.96599</Caption>
   <Data>0.0206985\37.2105\6.23826\5.53027
8.34619e-5\0.150042\0.0251543\0.0222995
0.00834619\15.0042\2.51543\2.22995</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="MWTjpz" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="8XKBzM" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 232.</Text>
 </Task>
 <Task Id="ox6dXz" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="kccNKF" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="bFRfbV" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="9S1aHP" Title="Quasi-Newton method errors history">
   <Caption Id="C4IMmc">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.1547, and the final value after 241 epochs is 0.323571.
The initial value of the selection error is 0.944133, and the final value after 241 epochs is 0.39429.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241</X2Data>
   <Y1Data>1.15\0.719\0.637\0.612\0.554\0.504\0.481\0.464\0.451\0.439\0.434\0.426\0.418\0.411\0.407\0.402\0.397\0.392\0.389\0.386\0.383\0.381\0.379\0.377\0.375\0.373\0.371\0.369\0.367\0.366\0.364\0.363\0.362\0.361\0.36\0.359\0.357\0.356\0.355\0.354\0.353\0.352\0.351\0.35\0.35\0.349\0.348\0.348\0.347\0.346\0.345\0.345\0.345\0.345\0.344\0.343\0.343\0.343\0.342\0.342\0.342\0.342\0.341\0.341\0.341\0.341\0.341\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.335\0.335\0.336\0.336\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.334\0.334\0.334\0.334\0.334\0.334\0.333\0.333\0.333\0.333\0.332\0.333\0.332\0.332\0.332\0.332\0.331\0.331\0.33\0.33\0.329\0.329\0.329\0.328\0.327\0.327\0.326\0.325\0.325\0.325\0.324\0.324\0.324</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.944\1.11\0.834\0.815\0.702\0.539\0.477\0.418\0.407\0.418\0.422\0.403\0.388\0.389\0.403\0.421\0.416\0.405\0.417\0.409\0.406\0.404\0.395\0.402\0.401\0.401\0.413\0.408\0.413\0.413\0.416\0.412\0.409\0.413\0.404\0.41\0.404\0.411\0.406\0.409\0.406\0.407\0.403\0.404\0.408\0.411\0.41\0.41\0.417\0.414\0.419\0.417\0.42\0.419\0.419\0.42\0.42\0.419\0.422\0.422\0.422\0.422\0.425\0.429\0.429\0.431\0.43\0.429\0.428\0.43\0.429\0.432\0.432\0.435\0.436\0.435\0.435\0.436\0.435\0.437\0.438\0.438\0.437\0.437\0.438\0.439\0.44\0.44\0.441\0.441\0.44\0.438\0.437\0.436\0.436\0.435\0.436\0.435\0.436\0.436\0.437\0.437\0.436\0.437\0.438\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.438\0.438\0.438\0.438\0.438\0.438\0.438\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.439\0.44\0.44\0.44\0.44\0.44\0.44\0.439\0.441\0.441\0.44\0.439\0.439\0.44\0.439\0.439\0.439\0.438\0.438\0.438\0.437\0.437\0.437\0.437\0.436\0.437\0.437\0.436\0.436\0.436\0.436\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.433\0.433\0.433\0.433\0.433\0.433\0.433\0.432\0.432\0.432\0.431\0.431\0.43\0.43\0.43\0.43\0.429\0.428\0.428\0.427\0.427\0.425\0.424\0.423\0.422\0.421\0.42\0.42\0.42\0.419\0.417\0.414\0.412\0.409\0.406\0.406\0.405\0.405\0.403\0.402\0.398\0.396\0.392\0.392\0.395\0.395\0.392\0.392\0.393\0.394</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>242</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="IwtjDI" Title="Quasi-Newton method results">
   <Caption Id="mrqZuM">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.324
0.394
241
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="vAqqG9" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="9Cj3Zf" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="iTHRfL" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="DoGR6A">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0823567 and its percentage error 8.23568</Caption>
   <Data>0.0109978\52.6767\5.84949\5.09362
0.000152747\0.73162\0.0812429\0.0707448
0.0152747\73.162\8.1243\7.07448</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Oeap8N" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="ho6XyB" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 232.</Text>
 </Task>
 <Task Id="pMXeEy" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="v0IL53" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 232.</Text>
 </Task>
 <Task Id="5baGaM" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="kG5OdH" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="v2NDwE" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="qjwbL3" Title="Quasi-Newton method errors history">
   <Caption Id="bW4RVR">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.50779, and the final value after 163 epochs is 0.205828.
The initial value of the selection error is 0.421255, and the final value after 163 epochs is 0.249699.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163</X2Data>
   <Y1Data>1.51\1.12\0.989\0.919\0.619\0.436\0.386\0.372\0.347\0.326\0.307\0.295\0.283\0.272\0.266\0.26\0.256\0.252\0.247\0.244\0.241\0.238\0.237\0.235\0.234\0.232\0.231\0.23\0.228\0.227\0.225\0.224\0.223\0.222\0.222\0.22\0.22\0.219\0.218\0.217\0.217\0.216\0.216\0.215\0.215\0.214\0.214\0.213\0.213\0.213\0.212\0.212\0.212\0.212\0.212\0.211\0.211\0.211\0.211\0.211\0.211\0.211\0.21\0.21\0.21\0.21\0.21\0.21\0.21\0.21\0.21\0.21\0.21\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.209\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.208\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.207\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206\0.206</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.421\0.431\0.672\0.808\0.502\0.31\0.274\0.267\0.293\0.265\0.238\0.236\0.24\0.232\0.234\0.235\0.233\0.226\0.222\0.221\0.225\0.224\0.23\0.227\0.227\0.223\0.223\0.224\0.226\0.227\0.223\0.222\0.224\0.223\0.223\0.223\0.223\0.223\0.226\0.226\0.225\0.227\0.228\0.228\0.23\0.229\0.228\0.227\0.229\0.229\0.228\0.228\0.227\0.228\0.228\0.228\0.227\0.228\0.228\0.228\0.23\0.23\0.231\0.23\0.231\0.231\0.231\0.231\0.231\0.231\0.232\0.233\0.232\0.233\0.232\0.233\0.233\0.233\0.232\0.232\0.232\0.232\0.232\0.232\0.233\0.233\0.232\0.232\0.233\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.233\0.234\0.235\0.235\0.236\0.235\0.235\0.235\0.235\0.235\0.234\0.236\0.236\0.237\0.237\0.237\0.237\0.238\0.238\0.239\0.239\0.239\0.24\0.239\0.239\0.239\0.24\0.24\0.241\0.24\0.241\0.241\0.242\0.242\0.242\0.242\0.242\0.243\0.243\0.243\0.244\0.244\0.245\0.245\0.245\0.245\0.245\0.246\0.246\0.246\0.246\0.247\0.246\0.247\0.247\0.247\0.247\0.247\0.248\0.248\0.248\0.249\0.249\0.249\0.249\0.25</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>164</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="7JjjEz" Title="Quasi-Newton method results">
   <Caption Id="L9WO3b">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.206
0.25
163
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="BoKzn1" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="9qC4sh" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="ZGkz04" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="u4fTBK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.022277 and its percentage error 2.22769</Caption>
   <Data>0\12.4727\0.857847\1.004
0\0.733687\0.0504616\0.0590589
0\73.3687\5.04616\5.90589</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="DA8ocg" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="sA82Br" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 694.</Text>
 </Task>
 <Task Id="IntiLV" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="LNgimc" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="0zDtkg" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="TMbrCf" Title="Quasi-Newton method errors history">
   <Caption Id="fM7BxQ">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.41796, and the final value after 151 epochs is 0.102017.
The initial value of the selection error is 0.374302, and the final value after 151 epochs is 0.367118.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151</X2Data>
   <Y1Data>1.42\0.789\0.594\0.498\0.414\0.367\0.331\0.321\0.305\0.295\0.282\0.271\0.265\0.259\0.253\0.248\0.242\0.239\0.237\0.234\0.23\0.228\0.226\0.223\0.221\0.219\0.217\0.215\0.212\0.21\0.208\0.205\0.202\0.2\0.197\0.193\0.191\0.189\0.187\0.185\0.183\0.181\0.179\0.177\0.175\0.173\0.171\0.17\0.167\0.165\0.163\0.162\0.16\0.158\0.156\0.154\0.153\0.151\0.149\0.148\0.146\0.145\0.144\0.143\0.142\0.141\0.14\0.139\0.138\0.136\0.136\0.135\0.134\0.133\0.132\0.131\0.131\0.129\0.129\0.128\0.128\0.127\0.126\0.125\0.125\0.124\0.124\0.123\0.123\0.122\0.121\0.121\0.121\0.12\0.12\0.119\0.119\0.118\0.118\0.118\0.117\0.117\0.116\0.116\0.116\0.115\0.115\0.114\0.114\0.114\0.113\0.113\0.113\0.112\0.112\0.112\0.111\0.111\0.111\0.111\0.11\0.11\0.11\0.109\0.109\0.109\0.109\0.108\0.108\0.108\0.107\0.107\0.107\0.106\0.106\0.106\0.105\0.105\0.105\0.105\0.104\0.104\0.104\0.104\0.104\0.103\0.103\0.103\0.103\0.102\0.102\0.102</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.374\0.531\0.503\0.385\0.346\0.244\0.264\0.255\0.256\0.25\0.242\0.229\0.234\0.226\0.225\0.22\0.221\0.218\0.217\0.221\0.221\0.221\0.221\0.219\0.221\0.221\0.221\0.222\0.22\0.221\0.227\0.232\0.237\0.238\0.238\0.246\0.249\0.248\0.247\0.245\0.248\0.249\0.257\0.255\0.256\0.261\0.256\0.258\0.26\0.26\0.26\0.262\0.269\0.269\0.27\0.271\0.271\0.27\0.278\0.278\0.281\0.283\0.281\0.283\0.284\0.286\0.289\0.292\0.294\0.296\0.298\0.299\0.302\0.301\0.302\0.301\0.301\0.303\0.304\0.305\0.305\0.309\0.306\0.31\0.313\0.314\0.315\0.314\0.315\0.317\0.319\0.322\0.32\0.32\0.321\0.322\0.324\0.325\0.327\0.329\0.33\0.334\0.336\0.338\0.34\0.34\0.341\0.343\0.342\0.342\0.343\0.342\0.343\0.346\0.346\0.344\0.345\0.342\0.343\0.344\0.343\0.345\0.343\0.346\0.348\0.347\0.349\0.349\0.35\0.35\0.35\0.353\0.352\0.353\0.353\0.356\0.357\0.357\0.358\0.361\0.362\0.362\0.362\0.362\0.364\0.364\0.364\0.365\0.365\0.367\0.365\0.367</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>152</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="KcXPoC" Title="Quasi-Newton method results">
   <Caption Id="vHdVOt">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.102
0.367
151
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="OzevZA" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="Dwnu02" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="dE3foW" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="WCKi2w">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.022277 and its percentage error 2.22769</Caption>
   <Data>0\14.6917\1.04471\1.22156
0\0.864217\0.0614536\0.0718567
0\86.4217\6.14536\7.18567</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="bhyrKJ" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="o3rE8N" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="ChtLcz" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="dpzyNY" Title="Quasi-Newton method errors history">
   <Caption Id="It0PmT">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 3.03899, and the final value after 177 epochs is 0.186034.
The initial value of the selection error is 2.72227, and the final value after 177 epochs is 0.719691.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177</X2Data>
   <Y1Data>3.04\1.27\0.912\0.8\0.775\0.755\0.686\0.62\0.6\0.595\0.587\0.574\0.549\0.529\0.521\0.508\0.492\0.485\0.48\0.474\0.467\0.459\0.45\0.443\0.436\0.429\0.424\0.416\0.41\0.407\0.4\0.395\0.392\0.387\0.382\0.376\0.372\0.368\0.365\0.361\0.354\0.351\0.348\0.343\0.34\0.338\0.332\0.329\0.325\0.323\0.319\0.317\0.312\0.31\0.308\0.305\0.304\0.302\0.301\0.299\0.298\0.295\0.294\0.293\0.292\0.29\0.289\0.288\0.287\0.285\0.284\0.283\0.282\0.281\0.28\0.279\0.278\0.277\0.276\0.275\0.274\0.273\0.272\0.271\0.27\0.269\0.267\0.267\0.266\0.265\0.264\0.263\0.262\0.26\0.259\0.258\0.257\0.256\0.255\0.254\0.253\0.252\0.251\0.249\0.249\0.248\0.247\0.246\0.245\0.244\0.244\0.243\0.242\0.241\0.24\0.239\0.238\0.237\0.236\0.235\0.234\0.233\0.232\0.231\0.23\0.23\0.229\0.228\0.227\0.226\0.225\0.224\0.224\0.223\0.222\0.221\0.221\0.22\0.219\0.218\0.217\0.216\0.215\0.214\0.213\0.212\0.211\0.21\0.209\0.207\0.206\0.205\0.204\0.203\0.202\0.201\0.2\0.2\0.199\0.198\0.197\0.196\0.195\0.194\0.194\0.193\0.192\0.191\0.191\0.19\0.189\0.189\0.189\0.188\0.187\0.187\0.187\0.186</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.72\1.17\0.987\1.02\0.942\0.976\0.762\0.867\0.76\0.739\0.782\0.721\0.66\0.634\0.63\0.592\0.545\0.555\0.555\0.527\0.529\0.561\0.531\0.506\0.504\0.501\0.485\0.479\0.482\0.478\0.468\0.464\0.463\0.461\0.473\0.49\0.48\0.456\0.463\0.468\0.464\0.465\0.467\0.476\0.48\0.48\0.479\0.491\0.492\0.489\0.481\0.501\0.504\0.501\0.498\0.502\0.505\0.502\0.498\0.492\0.502\0.497\0.499\0.502\0.502\0.5\0.486\0.489\0.503\0.503\0.493\0.497\0.498\0.502\0.507\0.498\0.508\0.504\0.508\0.501\0.503\0.512\0.514\0.517\0.514\0.515\0.513\0.528\0.524\0.533\0.534\0.532\0.54\0.535\0.541\0.545\0.54\0.539\0.546\0.541\0.551\0.549\0.547\0.548\0.558\0.56\0.563\0.565\0.555\0.557\0.555\0.554\0.557\0.561\0.568\0.569\0.568\0.569\0.572\0.57\0.58\0.581\0.579\0.577\0.587\0.583\0.588\0.59\0.592\0.592\0.595\0.592\0.586\0.594\0.6\0.601\0.599\0.61\0.608\0.6\0.606\0.612\0.615\0.614\0.616\0.625\0.63\0.633\0.632\0.64\0.648\0.648\0.649\0.648\0.655\0.648\0.653\0.657\0.669\0.676\0.672\0.667\0.673\0.676\0.68\0.688\0.691\0.689\0.689\0.693\0.696\0.69\0.697\0.709\0.715\0.718\0.715\0.72</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>178</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>4</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="MFDgKg" Title="Quasi-Newton method results">
   <Caption Id="LjTLVs">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.186
0.72
177
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="puWUoH" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="MCKfsz" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 694.</Text>
 </Task>
 <Task Id="lGqBuJ" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="PPIkyW" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="OIgGEU" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="D5pXNR" Title="Quasi-Newton method errors history">
   <Caption Id="NT2bmo">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.03293, and the final value after 150 epochs is 0.163167.
The initial value of the selection error is 1.36013, and the final value after 150 epochs is 0.813646.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150</X2Data>
   <Y1Data>1.03\0.761\0.668\0.589\0.565\0.512\0.477\0.456\0.439\0.426\0.413\0.402\0.395\0.389\0.38\0.374\0.367\0.361\0.356\0.351\0.346\0.342\0.337\0.331\0.327\0.322\0.318\0.314\0.31\0.306\0.302\0.298\0.294\0.291\0.286\0.283\0.279\0.276\0.272\0.268\0.264\0.261\0.258\0.255\0.252\0.249\0.247\0.245\0.243\0.24\0.238\0.237\0.234\0.231\0.228\0.227\0.225\0.224\0.222\0.221\0.22\0.218\0.216\0.215\0.213\0.212\0.211\0.209\0.208\0.207\0.205\0.204\0.203\0.202\0.201\0.2\0.198\0.197\0.196\0.195\0.194\0.193\0.193\0.192\0.191\0.191\0.189\0.189\0.188\0.187\0.187\0.186\0.186\0.185\0.184\0.184\0.183\0.182\0.182\0.181\0.181\0.181\0.18\0.18\0.179\0.179\0.178\0.177\0.177\0.177\0.176\0.176\0.175\0.175\0.174\0.174\0.173\0.173\0.173\0.172\0.172\0.171\0.171\0.171\0.17\0.17\0.169\0.169\0.169\0.168\0.168\0.168\0.167\0.167\0.167\0.166\0.166\0.166\0.166\0.165\0.165\0.165\0.165\0.165\0.164\0.164\0.164\0.164\0.164\0.163\0.163</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.36\1.1\0.839\0.788\0.696\0.573\0.466\0.481\0.45\0.4\0.403\0.441\0.427\0.403\0.409\0.396\0.405\0.413\0.39\0.408\0.409\0.415\0.414\0.418\0.427\0.428\0.438\0.446\0.448\0.468\0.495\0.496\0.504\0.483\0.494\0.498\0.527\0.525\0.52\0.549\0.541\0.557\0.55\0.554\0.563\0.56\0.573\0.575\0.561\0.562\0.576\0.582\0.597\0.59\0.602\0.607\0.615\0.608\0.608\0.606\0.603\0.606\0.614\0.62\0.616\0.629\0.631\0.641\0.641\0.633\0.641\0.655\0.658\0.664\0.668\0.673\0.68\0.682\0.69\0.696\0.695\0.699\0.695\0.702\0.71\0.709\0.72\0.712\0.715\0.72\0.725\0.732\0.739\0.734\0.735\0.74\0.742\0.747\0.745\0.749\0.746\0.744\0.749\0.753\0.755\0.759\0.754\0.76\0.759\0.76\0.764\0.762\0.763\0.764\0.766\0.767\0.777\0.776\0.778\0.779\0.784\0.785\0.789\0.79\0.789\0.791\0.796\0.799\0.796\0.797\0.797\0.799\0.801\0.802\0.803\0.807\0.806\0.807\0.805\0.808\0.812\0.812\0.811\0.811\0.813\0.813\0.813\0.811\0.807\0.807\0.814</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>151</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="3xm8Ca" Title="Quasi-Newton method results">
   <Caption Id="ohWcNg">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.163
0.814
150
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="sTbu7y" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="oVPwCw" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="2XW2IL" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Gu4PDj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0823567 and its percentage error 8.23568</Caption>
   <Data>0.00470829\50.3472\7.54079\5.97871
6.53929e-5\0.699266\0.104733\0.0830376
0.00653929\69.9266\10.4733\8.30376</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="mkOt16" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="qik86q" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 694.</Text>
 </Task>
 <Task Id="vbYWI7" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="njZlU9" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="NuZo0H" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="JIs53R" Title="Quasi-Newton method errors history">
   <Caption Id="uxO84z">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.38947, and the final value after 137 epochs is 0.25066.
The initial value of the selection error is 0.88769, and the final value after 137 epochs is 0.670033.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137</X2Data>
   <Y1Data>1.39\0.705\0.659\0.625\0.588\0.568\0.537\0.51\0.486\0.478\0.468\0.462\0.453\0.446\0.442\0.436\0.432\0.427\0.425\0.421\0.418\0.415\0.411\0.408\0.404\0.4\0.398\0.395\0.392\0.387\0.385\0.381\0.378\0.375\0.372\0.37\0.367\0.364\0.361\0.358\0.356\0.354\0.351\0.35\0.347\0.345\0.343\0.341\0.339\0.337\0.334\0.332\0.33\0.33\0.327\0.326\0.324\0.323\0.321\0.319\0.317\0.315\0.313\0.312\0.31\0.309\0.308\0.306\0.305\0.303\0.302\0.301\0.3\0.299\0.298\0.296\0.295\0.293\0.293\0.291\0.29\0.289\0.288\0.287\0.286\0.285\0.285\0.284\0.283\0.282\0.281\0.28\0.279\0.278\0.277\0.276\0.275\0.274\0.274\0.273\0.272\0.271\0.271\0.27\0.269\0.268\0.268\0.268\0.267\0.266\0.266\0.265\0.264\0.264\0.263\0.262\0.262\0.261\0.261\0.26\0.259\0.259\0.258\0.258\0.257\0.257\0.256\0.256\0.255\0.255\0.254\0.254\0.253\0.252\0.252\0.251\0.251\0.251</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.888\0.452\0.478\0.486\0.385\0.411\0.363\0.329\0.295\0.295\0.283\0.281\0.28\0.276\0.279\0.283\0.284\0.296\0.294\0.296\0.299\0.3\0.295\0.304\0.315\0.32\0.325\0.32\0.317\0.329\0.335\0.344\0.348\0.346\0.351\0.346\0.36\0.371\0.379\0.377\0.387\0.398\0.4\0.394\0.395\0.402\0.404\0.41\0.402\0.404\0.404\0.414\0.414\0.415\0.429\0.436\0.435\0.439\0.437\0.443\0.456\0.458\0.464\0.468\0.473\0.477\0.483\0.488\0.492\0.499\0.503\0.496\0.495\0.502\0.501\0.504\0.504\0.508\0.512\0.52\0.522\0.524\0.527\0.531\0.531\0.533\0.533\0.537\0.544\0.548\0.549\0.561\0.556\0.566\0.567\0.567\0.565\0.569\0.569\0.566\0.57\0.574\0.577\0.584\0.585\0.585\0.581\0.582\0.589\0.593\0.595\0.598\0.599\0.598\0.602\0.612\0.618\0.617\0.619\0.62\0.623\0.621\0.626\0.632\0.638\0.634\0.641\0.639\0.642\0.643\0.653\0.656\0.659\0.662\0.662\0.662\0.665\0.67</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>138</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="srtyK9" Title="Quasi-Newton method results">
   <Caption Id="Fgmf6s">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.251
0.67
137
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="2oIjVn" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="CpgRiZ" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="18Q448" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="MdbxtC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0796598 and its percentage error 7.96599</Caption>
   <Data>0.0126343\36.237\8.90435\6.87404
5.09447e-5\0.146117\0.0359046\0.0277179
0.00509447\14.6117\3.59046\2.77179</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="mkr34G" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="FrmH0i" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 694.</Text>
 </Task>
 <Task Id="5I7yRe" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="BlIYFG" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="NSmiRR" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="SgllUt" Title="Quasi-Newton method errors history">
   <Caption Id="jYd5D6">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.03637, and the final value after 192 epochs is 0.0447267.
The initial value of the selection error is 0.965782, and the final value after 192 epochs is 0.214176.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192</X2Data>
   <Y1Data>1.04\0.676\0.499\0.368\0.286\0.258\0.214\0.191\0.176\0.161\0.152\0.144\0.136\0.132\0.127\0.123\0.12\0.116\0.113\0.111\0.109\0.107\0.106\0.104\0.102\0.1\0.0989\0.0973\0.0957\0.0941\0.0931\0.0915\0.0901\0.0889\0.0876\0.0865\0.0854\0.0845\0.0835\0.0825\0.0815\0.0807\0.0799\0.0788\0.0782\0.0775\0.0769\0.076\0.0754\0.0749\0.0744\0.0739\0.0732\0.0725\0.0718\0.0711\0.0705\0.0699\0.0695\0.0686\0.0679\0.0673\0.067\0.0666\0.0661\0.0657\0.0651\0.0646\0.0645\0.064\0.0634\0.0632\0.0626\0.0623\0.0619\0.0617\0.0613\0.0611\0.0607\0.0606\0.0601\0.0598\0.0595\0.0594\0.059\0.0589\0.0586\0.0584\0.0582\0.058\0.0577\0.0575\0.0572\0.0572\0.0569\0.0565\0.0564\0.0559\0.0558\0.0555\0.0552\0.0549\0.0548\0.0543\0.0542\0.054\0.0539\0.0535\0.0532\0.0531\0.0527\0.0527\0.0523\0.0521\0.0519\0.0515\0.0513\0.0511\0.0508\0.0507\0.0503\0.0503\0.0501\0.0498\0.0495\0.0494\0.0492\0.0491\0.0489\0.0487\0.0485\0.0485\0.0483\0.0482\0.0479\0.048\0.0477\0.0477\0.0476\0.0475\0.0475\0.0474\0.0472\0.0473\0.0472\0.0471\0.0471\0.047\0.047\0.0469\0.0469\0.0468\0.0467\0.0466\0.0466\0.0465\0.0464\0.0464\0.0463\0.0462\0.0462\0.0462\0.0461\0.046\0.046\0.0459\0.0459\0.0458\0.0458\0.0457\0.0457\0.0456\0.0456\0.0455\0.0455\0.0454\0.0454\0.0453\0.0452\0.0452\0.0451\0.0451\0.045\0.0451\0.045\0.045\0.0449\0.0449\0.0449\0.0448\0.0449\0.0448\0.0447</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.966\0.754\0.529\0.515\0.454\0.376\0.328\0.297\0.264\0.238\0.226\0.213\0.209\0.206\0.195\0.185\0.177\0.174\0.167\0.164\0.164\0.161\0.168\0.172\0.175\0.172\0.173\0.177\0.178\0.175\0.174\0.178\0.179\0.178\0.175\0.177\0.182\0.183\0.182\0.179\0.179\0.179\0.177\0.179\0.179\0.179\0.178\0.182\0.18\0.178\0.174\0.172\0.176\0.175\0.174\0.172\0.174\0.173\0.175\0.176\0.173\0.174\0.176\0.177\0.177\0.178\0.183\0.183\0.18\0.179\0.181\0.18\0.182\0.183\0.185\0.185\0.186\0.186\0.186\0.187\0.188\0.189\0.189\0.188\0.188\0.187\0.186\0.186\0.186\0.186\0.186\0.185\0.186\0.184\0.184\0.185\0.184\0.186\0.185\0.185\0.187\0.186\0.186\0.188\0.188\0.187\0.187\0.186\0.187\0.187\0.189\0.188\0.19\0.19\0.19\0.193\0.193\0.194\0.194\0.195\0.196\0.196\0.196\0.198\0.198\0.198\0.199\0.198\0.199\0.2\0.201\0.2\0.201\0.204\0.205\0.205\0.206\0.205\0.205\0.206\0.206\0.206\0.206\0.206\0.206\0.207\0.207\0.209\0.208\0.207\0.208\0.209\0.209\0.21\0.209\0.21\0.21\0.211\0.21\0.211\0.212\0.211\0.212\0.212\0.212\0.212\0.212\0.212\0.212\0.213\0.212\0.213\0.212\0.213\0.212\0.212\0.212\0.211\0.212\0.212\0.212\0.213\0.213\0.214\0.214\0.214\0.213\0.213\0.213\0.214\0.214\0.214\0.214</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>193</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="06vHgk" Title="Quasi-Newton method results">
   <Caption Id="Sfx3qJ">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0447
0.214
192
00:00:02
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="FrzI18" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="meC45O" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="a3Zc3w" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="3wcKyn">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788808 and its percentage error 7.88807</Caption>
   <Data>0.00320435\111.235\7.17152\8.22096
2.05407e-5\0.713042\0.0459713\0.0526985
0.00205407\71.3042\4.59713\5.26985</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="nNygEX" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="nl2vf8" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 143.</Text>
 </Task>
 <Task Id="RQpO2T" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="PzJXe3" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="2YvkPI" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="jKkHu4" Title="Quasi-Newton method errors history">
   <Caption Id="BhPLva">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.0312, and the final value after 128 epochs is 0.367435.
The initial value of the selection error is 0.813978, and the final value after 128 epochs is 0.365668.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128</X2Data>
   <Y1Data>1.03\0.884\0.769\0.609\0.584\0.547\0.527\0.5\0.483\0.472\0.456\0.449\0.436\0.426\0.419\0.411\0.406\0.401\0.397\0.395\0.392\0.389\0.387\0.386\0.384\0.384\0.382\0.381\0.38\0.379\0.378\0.377\0.377\0.377\0.376\0.376\0.375\0.375\0.375\0.374\0.374\0.374\0.374\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.372\0.372\0.372\0.371\0.371\0.371\0.37\0.37\0.369\0.369\0.369\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.814\0.772\0.7\0.542\0.502\0.477\0.478\0.474\0.452\0.432\0.413\0.405\0.401\0.401\0.404\0.397\0.391\0.387\0.385\0.383\0.379\0.376\0.374\0.372\0.37\0.368\0.368\0.366\0.364\0.364\0.364\0.364\0.364\0.364\0.363\0.363\0.364\0.365\0.366\0.365\0.365\0.364\0.364\0.364\0.365\0.364\0.364\0.364\0.364\0.364\0.363\0.364\0.363\0.363\0.363\0.364\0.364\0.364\0.364\0.365\0.365\0.366\0.366\0.367\0.368\0.367\0.368\0.368\0.37\0.369\0.37\0.371\0.371\0.371\0.37\0.369\0.369\0.369\0.37\0.37\0.37\0.369\0.369\0.369\0.368\0.369\0.368\0.367\0.366\0.367\0.367\0.367\0.367\0.366\0.366\0.366\0.367\0.366\0.366\0.365\0.365\0.365\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.365\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>129</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="VMz5yC" Title="Quasi-Newton method results">
   <Caption Id="YUs8Wq">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.367
0.366
128
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="X9aaF8" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="oZQDsT" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="fnmXz9" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="nAT8kp">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00502014\102.461\8.20822\9.05759
3.21804e-5\0.656804\0.0526168\0.0580615
0.00321804\65.6804\5.26168\5.80615</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="FuGJle" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="JANhOT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00389862\308.329\6.70107\14.2088
1.14665e-5\0.90685\0.019709\0.0417907
0.00114665\90.685\1.9709\4.17907</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="rJaKmX" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ECg7HL">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00147438\33.3299\5.44257\4.73511
5.94508e-6\0.134395\0.0219458\0.0190932
0.000594508\13.4395\2.19459\1.90932</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="C5L9Oe" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="r8o6NF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0406513\39.2903\7.72004\5.10117
0.000564602\0.545699\0.107223\0.0708496
0.0564602\54.5699\10.7223\7.08495</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="OBDCbC" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9TTwYP">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00197387\14.459\0.776321\0.992806
0.00011611\0.850532\0.045666\0.0584004
0.011611\85.0532\4.5666\5.84004</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="qnFbdq" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="Vaw024" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="uRxsEP" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="ZR80Cw" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="zqcMj9" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="VwsSR0" Title="Quasi-Newton method errors history">
   <Caption Id="JaVxHe">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.12822, and the final value after 240 epochs is 0.274843.
The initial value of the selection error is 0.806868, and the final value after 240 epochs is 0.262268.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240</X2Data>
   <Y1Data>1.13\0.749\0.624\0.569\0.519\0.47\0.438\0.419\0.401\0.394\0.385\0.377\0.368\0.36\0.354\0.35\0.346\0.343\0.341\0.337\0.334\0.331\0.329\0.327\0.326\0.324\0.322\0.321\0.32\0.319\0.317\0.316\0.316\0.315\0.314\0.313\0.313\0.312\0.311\0.311\0.31\0.309\0.309\0.308\0.307\0.307\0.306\0.306\0.306\0.305\0.305\0.305\0.304\0.304\0.304\0.303\0.303\0.303\0.302\0.302\0.302\0.301\0.301\0.301\0.3\0.3\0.299\0.3\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.295\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.294\0.294\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.292\0.292\0.291\0.291\0.29\0.29\0.29\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.279\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.807\0.668\0.637\0.569\0.465\0.419\0.407\0.389\0.371\0.36\0.342\0.333\0.329\0.317\0.301\0.296\0.287\0.286\0.285\0.278\0.277\0.278\0.28\0.277\0.276\0.278\0.279\0.279\0.28\0.282\0.281\0.279\0.277\0.279\0.282\0.283\0.284\0.284\0.284\0.282\0.283\0.28\0.278\0.278\0.279\0.281\0.281\0.28\0.28\0.279\0.28\0.28\0.279\0.279\0.28\0.282\0.282\0.282\0.283\0.285\0.285\0.285\0.284\0.283\0.284\0.284\0.285\0.285\0.286\0.285\0.284\0.285\0.286\0.285\0.283\0.285\0.285\0.285\0.286\0.286\0.287\0.287\0.287\0.287\0.288\0.288\0.288\0.288\0.288\0.288\0.29\0.29\0.29\0.291\0.291\0.291\0.292\0.292\0.292\0.292\0.291\0.292\0.291\0.291\0.292\0.291\0.29\0.29\0.288\0.289\0.286\0.285\0.284\0.281\0.281\0.28\0.279\0.277\0.276\0.274\0.274\0.272\0.271\0.27\0.269\0.27\0.271\0.273\0.272\0.273\0.273\0.273\0.274\0.273\0.272\0.273\0.274\0.274\0.275\0.274\0.275\0.273\0.274\0.274\0.274\0.274\0.273\0.273\0.272\0.272\0.273\0.273\0.275\0.275\0.275\0.275\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.273\0.274\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.271\0.27\0.271\0.271\0.271\0.27\0.27\0.27\0.269\0.269\0.269\0.268\0.268\0.268\0.267\0.267\0.267\0.266\0.265\0.266\0.266\0.267\0.266\0.266\0.266\0.266\0.266\0.265\0.264\0.264\0.263\0.263\0.263\0.263\0.263\0.263\0.262\0.263\0.262\0.262\0.262\0.262\0.262\0.261\0.261\0.262\0.261\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>241</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="xZLert" Title="Quasi-Newton method results">
   <Caption Id="rQfGjP">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.275
0.262
240
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="1dhbR4" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="yXFCQM" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="d5y9Jz" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="YI7KHl">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0191422\109.26\6.47401\8.17822
0.000122706\0.700383\0.0415001\0.0524245
0.0122706\70.0383\4.15001\5.24245</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="roNu7i" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ltkGIA">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00803947\307.424\6.7632\14.0902
2.36455e-5\0.904188\0.0198918\0.0414419
0.00236455\90.4188\1.98918\4.14419</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="BEsSy7" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="vCBOit">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0247517\39.2035\5.07567\4.63172
9.98051e-5\0.158078\0.0204664\0.0186763
0.00998051\15.8078\2.04664\1.86763</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="G22Da5" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="oCVicK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0118294\39.7349\5.22745\4.21113
0.000164297\0.551874\0.0726035\0.0584879
0.0164297\55.1874\7.26035\5.84879</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4389Wf" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="UwtueU">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\14.3084\0.753212\1.01895
0\0.84167\0.0443066\0.0599383
0\84.167\4.43066\5.99383</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="XpszTi" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="OJlZML" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="vESRhq" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="RKAQkR" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="roxkFx" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="2pR0yC" Title="Quasi-Newton method errors history">
   <Caption Id="iDXVah">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.05143, and the final value after 222 epochs is 0.266079.
The initial value of the selection error is 0.754065, and the final value after 222 epochs is 0.279293.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222</X2Data>
   <Y1Data>1.05\0.675\0.609\0.552\0.505\0.489\0.455\0.421\0.399\0.389\0.379\0.373\0.365\0.358\0.352\0.348\0.343\0.34\0.336\0.333\0.331\0.329\0.327\0.325\0.323\0.322\0.32\0.318\0.317\0.315\0.314\0.313\0.312\0.311\0.31\0.31\0.309\0.308\0.306\0.305\0.305\0.304\0.303\0.302\0.302\0.301\0.301\0.3\0.3\0.299\0.298\0.298\0.298\0.297\0.297\0.296\0.296\0.295\0.295\0.295\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.289\0.289\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.281\0.281\0.281\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.266\0.266\0.266\0.266\0.266\0.266\0.266</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.754\0.629\0.571\0.473\0.45\0.448\0.433\0.385\0.367\0.364\0.354\0.348\0.33\0.314\0.309\0.31\0.302\0.298\0.295\0.293\0.287\0.285\0.286\0.286\0.287\0.286\0.289\0.291\0.29\0.289\0.29\0.29\0.29\0.293\0.295\0.296\0.297\0.299\0.292\0.296\0.294\0.295\0.296\0.295\0.298\0.299\0.3\0.301\0.3\0.301\0.301\0.3\0.3\0.3\0.299\0.299\0.301\0.303\0.3\0.3\0.299\0.302\0.304\0.304\0.305\0.306\0.307\0.308\0.31\0.309\0.311\0.313\0.312\0.311\0.312\0.311\0.312\0.312\0.311\0.308\0.309\0.307\0.31\0.308\0.308\0.308\0.307\0.307\0.307\0.309\0.308\0.309\0.309\0.309\0.311\0.312\0.312\0.312\0.314\0.315\0.315\0.316\0.316\0.314\0.313\0.313\0.312\0.309\0.308\0.307\0.305\0.305\0.303\0.302\0.3\0.301\0.299\0.298\0.298\0.298\0.297\0.297\0.296\0.295\0.295\0.294\0.293\0.295\0.294\0.294\0.293\0.293\0.291\0.29\0.289\0.288\0.284\0.282\0.281\0.282\0.28\0.281\0.281\0.282\0.281\0.279\0.28\0.278\0.278\0.277\0.278\0.278\0.278\0.278\0.279\0.278\0.277\0.278\0.278\0.277\0.277\0.278\0.278\0.279\0.28\0.278\0.279\0.279\0.28\0.279\0.279\0.279\0.279\0.278\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.28\0.281\0.28\0.28\0.28\0.28\0.279\0.279\0.28\0.28\0.281\0.281\0.281\0.28\0.28\0.28\0.28\0.28\0.28\0.279\0.28\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.277\0.277\0.277\0.277\0.278\0.277\0.277\0.277\0.277\0.278\0.278\0.279\0.279</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>223</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="neWP6K" Title="Quasi-Newton method results">
   <Caption Id="lvHZn5">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.266
0.279
222
00:00:02
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="1QDeuW" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="KuHcyr" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="z3KWJn" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="5egOoT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0212555\114.743\6.30687\8.47379
0.000136253\0.735531\0.0404286\0.0543192
0.0136253\73.5531\4.04286\5.43192</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="GwOSQl" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="bxOXWb">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00938988\299.612\6.91572\13.9886
2.76173e-5\0.881211\0.0203404\0.041143
0.00276173\88.1211\2.03404\4.1143</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4jTF2m" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="BSoaQ6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0194244\56.0534\5.46944\5.16218
7.83244e-5\0.226022\0.0220542\0.0208152
0.00783243\22.6022\2.20542\2.08152</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="mnGlV3" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Kb2PhJ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00256443\47.0363\5.66455\4.44728
3.56171e-5\0.653282\0.0786743\0.0617678
0.00356171\65.3282\7.86743\6.17677</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="grJCZS" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="HP5D67">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\15.5729\0.772709\1.11126
0\0.916055\0.0454535\0.0653685
0\91.6055\4.54535\6.53685</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="P3NgeE" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="SOzZDX" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="b7upBU" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="JW1KPA" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="1b8fqS" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="tidQKX" Title="Quasi-Newton method errors history">
   <Caption Id="0oeI4h">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.07171, and the final value after 241 epochs is 0.290525.
The initial value of the selection error is 0.736207, and the final value after 241 epochs is 0.25478.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241</X2Data>
   <Y1Data>1.07\0.767\0.607\0.566\0.497\0.458\0.424\0.404\0.391\0.38\0.373\0.365\0.359\0.354\0.347\0.343\0.34\0.336\0.334\0.332\0.329\0.328\0.327\0.325\0.323\0.323\0.322\0.321\0.32\0.32\0.319\0.318\0.317\0.317\0.316\0.315\0.315\0.315\0.314\0.314\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.311\0.31\0.31\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.301\0.301\0.3\0.299\0.298\0.297\0.297\0.296\0.296\0.295\0.295\0.295\0.294\0.294\0.294\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.736\0.692\0.58\0.518\0.433\0.386\0.371\0.362\0.356\0.329\0.317\0.31\0.305\0.301\0.281\0.279\0.274\0.272\0.271\0.267\0.27\0.273\0.275\0.272\0.273\0.278\0.276\0.272\0.27\0.268\0.267\0.269\0.268\0.27\0.27\0.268\0.271\0.27\0.269\0.269\0.27\0.269\0.268\0.269\0.268\0.268\0.268\0.266\0.265\0.264\0.264\0.265\0.265\0.265\0.265\0.264\0.265\0.265\0.266\0.267\0.267\0.268\0.268\0.267\0.268\0.268\0.267\0.269\0.269\0.268\0.268\0.269\0.268\0.267\0.267\0.268\0.268\0.268\0.267\0.266\0.266\0.267\0.267\0.268\0.268\0.267\0.267\0.267\0.267\0.269\0.268\0.266\0.267\0.267\0.267\0.268\0.269\0.269\0.268\0.268\0.267\0.268\0.267\0.267\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.269\0.27\0.269\0.269\0.268\0.267\0.267\0.267\0.267\0.266\0.265\0.263\0.262\0.262\0.261\0.26\0.26\0.259\0.26\0.26\0.26\0.258\0.258\0.258\0.257\0.257\0.258\0.257\0.257\0.256\0.258\0.258\0.258\0.258\0.257\0.257\0.256\0.256\0.256\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.256\0.256\0.256\0.256\0.256\0.256\0.255\0.255\0.255\0.255\0.255\0.255\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.254\0.255\0.255\0.255\0.255\0.255\0.255\0.255</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>242</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="dc1soG" Title="Quasi-Newton method results">
   <Caption Id="lU42GH">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.291
0.255
241
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ip8YO9" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="Cb8Wda" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="fqzRQI" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="wXsIAI">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00839233\117.097\5.82133\8.01011
5.3797e-5\0.750619\0.0373162\0.0513468
0.0053797\75.0619\3.73162\5.13468</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Y1VGul" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="vvt0FY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0173855\305.152\6.72583\14.0124
5.11338e-5\0.897507\0.0197819\0.041213
0.00511338\89.7507\1.97819\4.1213</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4ImiWy" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="XTe0tX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0117493\39.7296\4.81845\4.42926
4.73761e-5\0.1602\0.0194292\0.0178599
0.00473761\16.02\1.94292\1.78599</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="aU6SaX" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="EcI4AG">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000116348\41.0403\5.38085\4.38338
1.61595e-6\0.570005\0.0747341\0.0608802
0.000161595\57.0005\7.47341\6.08802</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="oqR0qA" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="PgMx6Z">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000636578\14.5904\0.68788\0.966627
3.74457e-5\0.858258\0.0404635\0.0568604
0.00374457\85.8258\4.04635\5.68604</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Uco4Lp" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="Tyt1mu" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 329.</Text>
 </Task>
 <Task Id="8qUYUy" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="fzbTnj" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="4iZHNL" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="lQEQm4" Title="Quasi-Newton method errors history">
   <Caption Id="QCo3xy">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.09978, and the final value after 234 epochs is 0.330601.
The initial value of the selection error is 0.817943, and the final value after 234 epochs is 0.262439.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234</X2Data>
   <Y1Data>1.1\0.74\0.618\0.56\0.507\0.458\0.429\0.414\0.404\0.393\0.386\0.376\0.371\0.366\0.361\0.358\0.355\0.353\0.351\0.349\0.348\0.346\0.345\0.344\0.343\0.342\0.341\0.341\0.34\0.34\0.34\0.339\0.339\0.339\0.338\0.338\0.338\0.338\0.337\0.337\0.337\0.337\0.337\0.337\0.336\0.336\0.336\0.336\0.336\0.335\0.335\0.335\0.335\0.335\0.335\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.818\0.72\0.589\0.477\0.441\0.416\0.382\0.36\0.356\0.349\0.338\0.322\0.312\0.301\0.292\0.286\0.28\0.275\0.274\0.272\0.276\0.276\0.273\0.274\0.277\0.277\0.278\0.277\0.275\0.274\0.272\0.274\0.275\0.275\0.275\0.274\0.273\0.272\0.272\0.272\0.271\0.272\0.271\0.272\0.272\0.271\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.274\0.274\0.274\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.273\0.273\0.272\0.271\0.27\0.269\0.269\0.268\0.268\0.267\0.267\0.267\0.267\0.267\0.268\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.268\0.267\0.267\0.267\0.267\0.267\0.266\0.265\0.265\0.266\0.266\0.266\0.267\0.266\0.266\0.266\0.266\0.267\0.267\0.266\0.266\0.266\0.265\0.265\0.265\0.265\0.266\0.266\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.263\0.263\0.263\0.263\0.263\0.263\0.264\0.264\0.264\0.263\0.263\0.263\0.264\0.263\0.263\0.263\0.263\0.263\0.263\0.262\0.262\0.262\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.263\0.263\0.263\0.263\0.263\0.263\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>235</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="I4yhs6" Title="Quasi-Newton method results">
   <Caption Id="03d6aB">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.331
0.262
234
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="F3FHhd" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="YYi82A" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="ixv3RB" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="olxHcp">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0255013\119.082\6.31765\8.23653
0.00016347\0.763346\0.0404978\0.0527983
0.016347\76.3346\4.04978\5.27983</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="yPf1nW" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="6OqpJN">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0320969\306.08\6.78875\14.1928
9.44025e-5\0.900235\0.0199669\0.0417435
0.00944025\90.0235\1.99669\4.17435</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="osLmqt" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="8UYojp">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0235348\22.3194\4.69311\3.97312
9.48983e-5\0.0899978\0.0189238\0.0160206
0.00948983\8.99978\1.89238\1.60206</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="7sfwBN" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="EoZ9jC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0268288\44.3434\5.62184\4.50767
0.000372622\0.61588\0.0780811\0.0626065
0.0372622\61.588\7.80811\6.26065</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="LPMfze" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="W1xq2R">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00247264\15.1966\0.682297\1.01859
0.000145449\0.893916\0.0401351\0.0599171
0.0145449\89.3916\4.01351\5.99171</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="l6hZze" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="w397mc" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 329.</Text>
 </Task>
 <Task Id="95OKd1" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="kpBW6i" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="B7QkX7" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="PgKd7g" Title="Quasi-Newton method errors history">
   <Caption Id="RFE5iU">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 5.35447, and the final value after 208 epochs is 1.49419.
The initial value of the selection error is 3.81486, and the final value after 208 epochs is 1.41556.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208</X2Data>
   <Y1Data>5.35\3.77\3.02\2.76\2.33\2.22\2.13\2.04\1.99\1.95\1.89\1.85\1.81\1.79\1.77\1.76\1.74\1.73\1.72\1.71\1.7\1.7\1.69\1.68\1.68\1.67\1.67\1.66\1.66\1.66\1.65\1.65\1.65\1.64\1.64\1.64\1.63\1.63\1.63\1.63\1.63\1.62\1.62\1.62\1.62\1.62\1.62\1.62\1.61\1.61\1.61\1.61\1.61\1.61\1.61\1.61\1.61\1.61\1.61\1.6\1.6\1.6\1.6\1.6\1.6\1.6\1.59\1.59\1.59\1.59\1.59\1.59\1.59\1.59\1.59\1.58\1.58\1.58\1.58\1.58\1.58\1.58\1.58\1.58\1.58\1.58\1.57\1.57\1.57\1.57\1.57\1.57\1.57\1.57\1.57\1.57\1.57\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.55\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.54\1.53\1.53\1.53\1.53\1.53\1.53\1.53\1.53\1.53\1.52\1.52\1.52\1.52\1.52\1.52\1.52\1.52\1.52\1.52\1.52\1.52\1.52\1.52\1.51\1.51\1.51\1.51\1.51\1.51\1.51\1.51\1.51\1.51\1.51\1.51\1.51\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.5\1.49\1.49</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>3.81\3.55\3.03\2.59\2.03\1.91\1.86\1.78\1.79\1.81\1.67\1.59\1.51\1.48\1.46\1.43\1.43\1.4\1.4\1.39\1.39\1.39\1.41\1.42\1.42\1.41\1.4\1.39\1.39\1.39\1.39\1.4\1.4\1.39\1.38\1.37\1.37\1.37\1.36\1.37\1.37\1.37\1.38\1.38\1.38\1.38\1.38\1.38\1.37\1.38\1.38\1.38\1.38\1.39\1.4\1.39\1.39\1.39\1.4\1.4\1.4\1.4\1.4\1.41\1.42\1.42\1.43\1.42\1.43\1.43\1.43\1.44\1.44\1.44\1.45\1.45\1.45\1.46\1.46\1.46\1.46\1.46\1.45\1.45\1.45\1.46\1.46\1.47\1.47\1.47\1.47\1.47\1.47\1.48\1.48\1.49\1.49\1.5\1.5\1.51\1.51\1.51\1.52\1.52\1.52\1.52\1.53\1.54\1.53\1.54\1.54\1.55\1.55\1.54\1.54\1.54\1.55\1.56\1.56\1.55\1.56\1.56\1.56\1.56\1.57\1.56\1.56\1.57\1.57\1.57\1.57\1.57\1.57\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.56\1.55\1.56\1.56\1.56\1.57\1.57\1.56\1.56\1.56\1.56\1.56\1.56\1.55\1.54\1.54\1.53\1.52\1.5\1.49\1.48\1.48\1.48\1.48\1.48\1.47\1.47\1.47\1.47\1.48\1.48\1.46\1.46\1.45\1.45\1.45\1.45\1.46\1.45\1.45\1.45\1.46\1.45\1.45\1.45\1.44\1.44\1.43\1.43\1.42\1.43\1.42\1.42\1.42\1.42\1.42\1.41\1.41\1.41\1.4\1.41\1.41\1.41\1.41\1.41\1.41\1.41\1.42</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Mean squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>209</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>6</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="99sQpv" Title="Quasi-Newton method results">
   <Caption Id="0ZuKKy">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>1.49
1.42
208
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="FI9WMh" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="9vICK8" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="IMnXbx" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="TV3VxR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00579453\120.759\6.42862\8.65846
3.71444e-5\0.774093\0.0412091\0.055503
0.00371444\77.4093\4.12091\5.5503</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="zT53XY" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="xwk2FD">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00283718\306.101\6.92877\14.1315
8.34465e-6\0.900296\0.0203787\0.0415633
0.000834465\90.0296\2.03787\4.15633</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="nN60af" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="FVOfz9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0175095\25.2137\4.89906\4.22183
7.06027e-5\0.101668\0.0197543\0.0170235
0.00706027\10.1668\1.97543\1.70235</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="CWira9" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="XBuJug">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00935364\41.9549\5.6983\4.33248
0.000129912\0.582708\0.079143\0.0601733
0.0129912\58.2708\7.9143\6.01733</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="xXKAQx" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="KIjtNo">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00197136\14.5549\0.742145\1.03512
0.000115963\0.856171\0.0436556\0.0608896
0.0115963\85.6171\4.36556\6.08896</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="6QEEFm" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="ZXoNBE" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 329.</Text>
 </Task>
 <Task Id="JQ6e1q" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="89S1cL" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="AiKmJu" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="Vtrhn9" Title="Quasi-Newton method errors history">
   <Caption Id="yC6czB">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.231316, and the final value after 333 epochs is 0.118394.
The initial value of the selection error is 0.277956, and the final value after 333 epochs is 0.155939.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333</X2Data>
   <Y1Data>0.231\0.181\0.166\0.159\0.156\0.15\0.147\0.139\0.137\0.133\0.131\0.13\0.129\0.128\0.127\0.126\0.126\0.126\0.125\0.124\0.124\0.123\0.123\0.123\0.122\0.122\0.122\0.122\0.122\0.122\0.121\0.121\0.121\0.121\0.121\0.121\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.12\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.119\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118\0.118</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.278\0.253\0.241\0.215\0.213\0.208\0.212\0.205\0.202\0.192\0.19\0.185\0.182\0.182\0.181\0.178\0.179\0.18\0.177\0.177\0.176\0.175\0.174\0.173\0.173\0.172\0.172\0.171\0.17\0.17\0.169\0.168\0.167\0.167\0.166\0.165\0.164\0.163\0.163\0.163\0.163\0.162\0.162\0.162\0.162\0.162\0.162\0.161\0.161\0.161\0.161\0.16\0.16\0.16\0.16\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.157\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156\0.156</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Minkowski error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>334</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="y3VHwy" Title="Quasi-Newton method results">
   <Caption Id="QXANZf">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.118
0.156
333
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="T0Tybq" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="E1sWST" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="MJ6DPP" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="59ikI3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0325737\119.089\6.36794\8.38027
0.000208806\0.763388\0.0408201\0.0537197
0.0208806\76.3388\4.08201\5.37197</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="3n8J0W" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Bl4ipP">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>4.95911e-5\305.436\6.77475\14.1703
1.45856e-7\0.898341\0.0199257\0.0416774
1.45856e-5\89.8341\1.99257\4.16774</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="t1DeRx" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9FK0jG">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00314522\21.978\4.5573\3.79484
1.26823e-5\0.0886208\0.0183762\0.0153018
0.00126823\8.86208\1.83762\1.53018</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="VXbVOd" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="0Xz4EY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00434875\47.3317\5.39636\4.3402
6.03994e-5\0.657385\0.0749494\0.0602805
0.00603994\65.7385\7.49494\6.02805</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="USDapT" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="PZnxCp">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00112534\15.725\0.563107\0.983701
6.61962e-5\0.925002\0.033124\0.0578648
0.00661962\92.5002\3.3124\5.78648</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Gc3bUQ" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="chGbh5" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 329.</Text>
 </Task>
 <Task Id="GCABk2" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="i3wwuc" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="OIK86M" Title="Optimization algorithm">Gradient descent is used here for training.
This method updates the neural parameters in the direction of the negative gradient of the loss function. </Text>
  <DoubleLineChart Id="Y6m72Z" Title="Gradient descent errors history">
   <Caption Id="hM9fwA">The following plot shows the training and selection errors at each epoch.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.09093, and the final value after 394 epochs is 0.34321.
The initial value of the selection error is 0.771867, and the final value after 394 epochs is 0.280619.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394</X2Data>
   <Y1Data>1.09\0.741\0.647\0.594\0.566\0.543\0.524\0.507\0.492\0.479\0.468\0.459\0.451\0.444\0.438\0.433\0.429\0.425\0.422\0.419\0.417\0.414\0.412\0.41\0.408\0.407\0.405\0.404\0.402\0.401\0.4\0.399\0.398\0.396\0.395\0.394\0.393\0.392\0.392\0.391\0.39\0.389\0.388\0.387\0.387\0.386\0.385\0.384\0.384\0.383\0.382\0.382\0.381\0.38\0.38\0.379\0.379\0.378\0.377\0.377\0.376\0.376\0.375\0.375\0.374\0.374\0.373\0.373\0.372\0.372\0.371\0.371\0.37\0.37\0.369\0.369\0.369\0.368\0.368\0.367\0.367\0.367\0.366\0.366\0.366\0.365\0.365\0.365\0.364\0.364\0.364\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.361\0.361\0.361\0.361\0.36\0.36\0.36\0.36\0.36\0.359\0.359\0.359\0.359\0.359\0.358\0.358\0.358\0.358\0.358\0.358\0.357\0.357\0.357\0.357\0.357\0.357\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.772\0.669\0.568\0.518\0.491\0.47\0.455\0.442\0.432\0.422\0.416\0.41\0.406\0.401\0.399\0.396\0.394\0.391\0.39\0.388\0.387\0.386\0.384\0.383\0.382\0.381\0.38\0.379\0.378\0.377\0.376\0.375\0.374\0.373\0.372\0.371\0.37\0.369\0.368\0.367\0.367\0.365\0.365\0.363\0.363\0.361\0.361\0.359\0.359\0.357\0.357\0.355\0.355\0.353\0.353\0.351\0.351\0.35\0.349\0.348\0.347\0.346\0.345\0.344\0.343\0.342\0.342\0.34\0.34\0.338\0.338\0.337\0.336\0.335\0.334\0.333\0.333\0.331\0.331\0.33\0.329\0.328\0.328\0.327\0.326\0.325\0.325\0.324\0.323\0.322\0.322\0.321\0.321\0.32\0.319\0.318\0.318\0.317\0.317\0.316\0.316\0.315\0.315\0.314\0.314\0.313\0.313\0.312\0.312\0.311\0.311\0.31\0.31\0.309\0.309\0.308\0.308\0.308\0.307\0.307\0.307\0.306\0.306\0.305\0.305\0.304\0.304\0.304\0.304\0.303\0.303\0.302\0.302\0.302\0.302\0.301\0.301\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.294\0.294\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.288\0.289\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.286\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.285\0.286\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.282\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.281\0.282\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>395</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="1fHpw0" Title="Gradient descent results">
   <Caption Id="WORtIh">The following table shows the training results by the gradient descent method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.343
0.281
394
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="MvuNxh" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="7c3882" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="WWhugD" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="bv1VOK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00233459\112.554\6.76873\8.19732
1.49654e-5\0.721501\0.0433893\0.0525469
0.00149654\72.1501\4.33893\5.25469</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Fa0cIE" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Jz9RNH">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00905991\307.858\6.78288\14.2459
2.66468e-5\0.905466\0.0199497\0.0418996
0.00266468\90.5466\1.99497\4.18996</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="qCd5YP" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="TGdicU">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0595226\22.1541\4.75095\3.96582
0.000240011\0.089331\0.0191571\0.0159912
0.0240011\8.9331\1.91571\1.59912</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="9otjpW" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ESh8WN">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00686264\43.7669\5.78173\4.44215
9.53144e-5\0.607874\0.0803018\0.0616966
0.00953144\60.7874\8.03018\6.16966</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="dsXCvK" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="F4bGGr">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0050149\13.9483\0.793942\0.968739
0.000294994\0.820488\0.0467025\0.0569847
0.0294994\82.0488\4.67025\5.69847</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="FtaDk1" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="OgS72y" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="Q6jaQb" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="licq7E" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="2sCBXi" Title="Optimization algorithm">Gradient descent is used here for training.
This method updates the neural parameters in the direction of the negative gradient of the loss function. </Text>
  <DoubleLineChart Id="gb6FRK" Title="Gradient descent errors history">
   <Caption Id="7LNamE">The following plot shows the training and selection errors at each epoch.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.27682, and the final value after 340 epochs is 0.326618.
The initial value of the selection error is 0.925636, and the final value after 340 epochs is 0.274139.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340</X2Data>
   <Y1Data>1.28\0.813\0.614\0.568\0.529\0.503\0.487\0.477\0.468\0.459\0.452\0.445\0.44\0.434\0.429\0.425\0.421\0.418\0.415\0.412\0.409\0.407\0.405\0.403\0.401\0.399\0.398\0.396\0.395\0.393\0.392\0.391\0.389\0.388\0.387\0.386\0.385\0.384\0.383\0.382\0.381\0.38\0.379\0.378\0.377\0.377\0.376\0.375\0.374\0.373\0.373\0.372\0.371\0.371\0.37\0.369\0.368\0.368\0.367\0.366\0.366\0.365\0.365\0.364\0.364\0.363\0.362\0.362\0.361\0.361\0.36\0.36\0.359\0.359\0.358\0.358\0.358\0.357\0.357\0.356\0.356\0.355\0.355\0.355\0.354\0.354\0.353\0.353\0.353\0.352\0.352\0.352\0.351\0.351\0.351\0.35\0.35\0.35\0.349\0.349\0.349\0.349\0.348\0.348\0.348\0.348\0.347\0.347\0.347\0.347\0.346\0.346\0.346\0.346\0.345\0.345\0.345\0.345\0.345\0.344\0.344\0.344\0.344\0.344\0.343\0.343\0.343\0.343\0.343\0.342\0.342\0.342\0.342\0.342\0.342\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.926\0.626\0.568\0.476\0.476\0.441\0.436\0.425\0.421\0.412\0.409\0.402\0.399\0.393\0.392\0.387\0.385\0.382\0.381\0.377\0.377\0.374\0.373\0.371\0.37\0.368\0.367\0.366\0.365\0.363\0.363\0.361\0.36\0.359\0.358\0.356\0.356\0.354\0.353\0.352\0.351\0.35\0.349\0.347\0.347\0.345\0.344\0.343\0.342\0.341\0.34\0.339\0.338\0.337\0.336\0.335\0.334\0.332\0.332\0.331\0.33\0.329\0.328\0.327\0.326\0.325\0.325\0.323\0.323\0.322\0.321\0.32\0.319\0.318\0.318\0.317\0.316\0.315\0.314\0.313\0.313\0.312\0.311\0.31\0.31\0.309\0.309\0.308\0.307\0.306\0.306\0.305\0.305\0.304\0.304\0.303\0.302\0.302\0.301\0.3\0.3\0.299\0.299\0.298\0.298\0.297\0.297\0.297\0.296\0.296\0.295\0.295\0.295\0.294\0.294\0.293\0.293\0.292\0.292\0.291\0.291\0.291\0.291\0.29\0.29\0.289\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.281\0.281\0.281\0.281\0.281\0.281\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.279\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.276\0.277\0.276\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.275\0.276\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.275\0.274\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>341</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="nqcrYe" Title="Gradient descent results">
   <Caption Id="qM1Uyx">The following table shows the training results by the gradient descent method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.327
0.274
340
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="TSLBnw" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="xUPgjw" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="bEcJZN" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="DWDw6w">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00377274\102.572\6.50811\7.83089
2.41842e-5\0.657513\0.0417186\0.050198
0.00241842\65.7513\4.17186\5.0198</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ei3oxV" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="gFJAWI">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000595093\311.617\6.59417\14.3466
1.75027e-6\0.916521\0.0193946\0.0421959
0.000175027\91.6521\1.93946\4.21959</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="T1s5b4" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="11YBHM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0089035\22.5276\4.99669\4.26563
3.59012e-5\0.0908372\0.020148\0.0172001
0.00359012\9.08372\2.0148\1.72001</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4QwOsS" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="lVnXKZ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00237274\37.9621\5.6534\4.37294
3.29547e-5\0.527251\0.0785194\0.0607353
0.00329547\52.7251\7.85194\6.07353</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="1m0ViV" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="RttEbZ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00146341\12.1862\0.759511\0.915627
8.60831e-5\0.716838\0.0446771\0.0538604
0.00860831\71.6838\4.46771\5.38604</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="birVEq" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="GYE3Ti" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="tLV3nk" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="8i4K0c" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="DfmfhI" Title="Optimization algorithm">The conjugate gradient is used here for training.
In this algorithm search is performed along conjugate directions, which produces generally faster convergence than gradient descent directions. </Text>
  <DoubleLineChart Id="wHXRIg" Title="Conjugate gradient errors history">
   <Caption Id="GM9psS">The following plot shows the training and selection errors in each iteration.
The initial value of the training error is 1.15411, and the final value after 248 epochs is 0.308377.
The initial value of the selection error is 0.769376, and the final value after 248 epochs is 0.273294.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248</X2Data>
   <Y1Data>1.15\0.72\0.613\0.573\0.526\0.495\0.471\0.453\0.426\0.407\0.394\0.388\0.381\0.372\0.367\0.362\0.357\0.351\0.349\0.346\0.343\0.34\0.339\0.336\0.335\0.333\0.332\0.331\0.33\0.329\0.328\0.327\0.326\0.326\0.325\0.325\0.324\0.324\0.323\0.323\0.323\0.322\0.322\0.322\0.321\0.321\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.318\0.318\0.318\0.317\0.317\0.317\0.317\0.316\0.316\0.316\0.316\0.316\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.308</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.769\0.593\0.603\0.529\0.455\0.431\0.412\0.397\0.377\0.362\0.36\0.364\0.363\0.344\0.335\0.331\0.33\0.314\0.308\0.307\0.306\0.3\0.297\0.288\0.29\0.293\0.287\0.284\0.284\0.284\0.284\0.286\0.285\0.282\0.282\0.284\0.285\0.285\0.284\0.283\0.282\0.282\0.282\0.283\0.282\0.281\0.281\0.281\0.281\0.28\0.28\0.28\0.281\0.281\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.278\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.278\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.275\0.275\0.276\0.276\0.275\0.275\0.276\0.276\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.276\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.275\0.275\0.275\0.274\0.274\0.274\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.273\0.273\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>249</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="8NvVCB" Title="Conjugate gradient results">
   <Caption Id="gHbqne">The following table shows the training results by the conjugate gradient method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.308
0.273
248
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="3uNNNE" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="D4VMql" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="QmYanJ" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="WXN0Ja">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00913239\113.781\6.06756\7.91796
5.85409e-5\0.729364\0.0388946\0.0507561
0.00585409\72.9364\3.88946\5.07561</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="LwAWto" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="K6Ux4b">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0108566\307.827\6.80707\14.1259
3.19313e-5\0.905374\0.0200208\0.0415468
0.00319313\90.5374\2.00208\4.15469</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="dkA7fw" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="oHE1Uq">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0010643\32.1509\4.70233\4.26579
4.29153e-6\0.129641\0.018961\0.0172008
0.000429153\12.9641\1.8961\1.72008</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="V0CWFP" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="hmXBBK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00303459\38.3781\5.99689\4.54099
4.21471e-5\0.533029\0.0832902\0.0630694
0.00421471\53.3029\8.32902\6.30693</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="wS1co6" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="vHk595">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00532508\14.1699\0.71834\0.963945
0.00031324\0.833524\0.0422553\0.0567027
0.031324\83.3524\4.22553\5.67027</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Yd5PCM" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="ABluq2" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="Kg3e0d" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="WOG93G" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="40R8Tg" Title="Optimization algorithm">The Levenberg-Marquardt algorithm is used here for training, which was designed to approach second-order training speed without having to compute the Hessian matrix.
The Levenberg-Marquardt algorithm can only be applied when the loss index has the form of a sum of squares (as the mean squared error or the normalized squared error). </Text>
  <DoubleLineChart Id="JvM8Jn" Title="Levenberg-Marquardt algorithm errors history">
   <Caption Id="Uudrkt">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.11855, and the final value after 315 epochs is 0.318405.
The initial value of the selection error is 0.794301, and the final value after 315 epochs is 0.268039.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315</X2Data>
   <Y1Data>1.12\0.854\0.647\0.626\0.546\0.435\0.421\0.414\0.411\0.408\0.406\0.403\0.402\0.401\0.4\0.399\0.398\0.398\0.396\0.395\0.392\0.392\0.389\0.389\0.386\0.371\0.366\0.364\0.363\0.361\0.36\0.359\0.359\0.357\0.356\0.355\0.355\0.354\0.353\0.352\0.352\0.351\0.35\0.349\0.349\0.348\0.348\0.347\0.346\0.346\0.345\0.345\0.344\0.344\0.343\0.343\0.342\0.342\0.342\0.341\0.341\0.34\0.34\0.34\0.339\0.339\0.339\0.338\0.338\0.338\0.337\0.337\0.337\0.337\0.336\0.336\0.336\0.335\0.335\0.335\0.335\0.334\0.334\0.334\0.334\0.334\0.334\0.333\0.333\0.333\0.333\0.332\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.794\0.707\0.62\0.547\0.497\0.401\0.389\0.389\0.385\0.387\0.381\0.385\0.378\0.383\0.375\0.381\0.372\0.378\0.369\0.374\0.366\0.369\0.362\0.365\0.359\0.344\0.339\0.337\0.339\0.337\0.333\0.332\0.333\0.332\0.328\0.326\0.327\0.326\0.322\0.321\0.323\0.321\0.32\0.317\0.316\0.316\0.315\0.312\0.312\0.312\0.311\0.308\0.307\0.307\0.307\0.306\0.303\0.303\0.303\0.302\0.3\0.299\0.299\0.299\0.298\0.296\0.296\0.296\0.295\0.293\0.293\0.293\0.293\0.292\0.29\0.29\0.29\0.289\0.288\0.288\0.288\0.287\0.285\0.285\0.286\0.285\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.281\0.281\0.28\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.269\0.27\0.27\0.269\0.269\0.27\0.27\0.269\0.269\0.269\0.27\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.268\0.268\0.268\0.269\0.269\0.269\0.268\0.268\0.268\0.269\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>316</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="4zHJ3S" Title="Levenberg-Marquardt algorithm results">
   <Caption Id="w3yYAK">In the next  the training results by the Levenberg-Marquardt algorithm are listed.
They include final states from the neural network, the loss index and the optimization algorithm.
</Caption>
   <Data>0.318
0.268
315
00:00:08
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="oNfcj8" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="oz3iaf" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="gpHNnj" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Xhtmo0">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0169754\108.973\6.14376\7.94875
0.000108817\0.698546\0.0393831\0.0509535
0.0108817\69.8546\3.93831\5.09535</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="DfiumQ" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="h8T7Cj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00208664\304.228\6.61492\13.9736
6.13717e-6\0.894788\0.0194557\0.0410989
0.000613717\89.4788\1.94557\4.10989</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="My9OIj" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="xXuFVi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00344086\23.3198\4.7645\4.09219
1.38744e-5\0.0940313\0.0192117\0.0165007
0.00138744\9.40313\1.92117\1.65007</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="rYmNNC" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="NZ6vrc">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>9.91821e-5\38.8674\5.7583\4.44166
1.37753e-6\0.539826\0.0799764\0.0616897
0.000137753\53.9826\7.99764\6.16897</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="HpZS5n" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="npRQzo">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.003124\14.4035\0.711859\0.988181
0.000183765\0.847267\0.041874\0.0581283
0.0183765\84.7267\4.1874\5.81283</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="EGQjW2" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="l1xXnC" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="1LKZJb" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="B3T1OU" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="0HEAkx" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="glEYpB" Title="Quasi-Newton method errors history">
   <Caption Id="Fsb0VF">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.04113, and the final value after 189 epochs is 0.304679.
The initial value of the selection error is 0.813652, and the final value after 189 epochs is 0.275212.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189</X2Data>
   <Y1Data>1.04\0.712\0.617\0.576\0.516\0.472\0.437\0.417\0.401\0.388\0.379\0.371\0.364\0.359\0.352\0.349\0.345\0.341\0.337\0.334\0.331\0.33\0.328\0.326\0.325\0.324\0.322\0.321\0.32\0.319\0.318\0.318\0.317\0.316\0.316\0.315\0.315\0.314\0.313\0.313\0.313\0.312\0.312\0.311\0.311\0.311\0.31\0.31\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.814\0.633\0.595\0.555\0.45\0.41\0.399\0.393\0.38\0.365\0.351\0.343\0.329\0.318\0.31\0.311\0.303\0.291\0.277\0.274\0.267\0.274\0.268\0.265\0.265\0.266\0.267\0.269\0.268\0.266\0.265\0.267\0.264\0.265\0.266\0.267\0.266\0.265\0.265\0.267\0.267\0.267\0.266\0.267\0.267\0.267\0.267\0.266\0.267\0.267\0.268\0.267\0.268\0.267\0.268\0.269\0.269\0.27\0.271\0.271\0.27\0.272\0.271\0.271\0.273\0.272\0.273\0.273\0.273\0.273\0.274\0.273\0.272\0.272\0.272\0.273\0.273\0.272\0.273\0.273\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.274\0.274\0.274\0.274\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.275\0.275\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>190</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="XdKhX3" Title="Quasi-Newton method results">
   <Caption Id="P7yH0u">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.305
0.275
189
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="7X4mWk" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="adbhcv" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="eWaxKn" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="j7xvew">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\110.683\6.16535\7.90404
0\0.709508\0.0395215\0.0506669
0\70.9508\3.95215\5.06669</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="n6Syrn" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="KRCJSL">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0190601\305.657\6.65825\14.0113
5.60592e-5\0.898991\0.0195831\0.0412098
0.00560592\89.8991\1.95831\4.12098</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="jvgTFb" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="kYuvSE">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00550461\40.5712\4.79741\4.29278
2.2196e-5\0.163594\0.0193444\0.0173096
0.0022196\16.3594\1.93444\1.73096</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="40Zw1A" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9m3AgB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0105515\37.9291\5.74639\4.37755
0.000146548\0.526794\0.079811\0.0607993
0.0146548\52.6794\7.9811\6.07993</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uFhu2H" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="syjZ4C">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00147772\14.301\0.765201\0.9942
8.69246e-5\0.841237\0.0450118\0.0584823
0.00869246\84.1237\4.50118\5.84823</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="AfhQPe" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="07ABkK" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="baMosK" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="J068Wn" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="hoVoyA" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="W9y9Y0" Title="Quasi-Newton method errors history">
   <Caption Id="rFjlj4">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.0786, and the final value after 212 epochs is 0.384725.
The initial value of the selection error is 0.83792, and the final value after 212 epochs is 0.306242.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212</X2Data>
   <Y1Data>1.08\0.739\0.634\0.598\0.557\0.532\0.511\0.496\0.488\0.48\0.474\0.47\0.463\0.458\0.452\0.449\0.446\0.443\0.44\0.437\0.435\0.433\0.432\0.43\0.428\0.427\0.426\0.425\0.424\0.423\0.422\0.421\0.42\0.42\0.419\0.418\0.417\0.417\0.416\0.415\0.414\0.414\0.413\0.413\0.412\0.411\0.411\0.41\0.409\0.409\0.408\0.407\0.407\0.406\0.406\0.405\0.405\0.404\0.404\0.404\0.403\0.403\0.402\0.402\0.401\0.401\0.4\0.4\0.399\0.399\0.399\0.399\0.398\0.398\0.398\0.398\0.397\0.397\0.397\0.397\0.397\0.396\0.396\0.396\0.396\0.396\0.396\0.395\0.395\0.395\0.395\0.395\0.395\0.394\0.394\0.394\0.394\0.394\0.394\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.838\0.59\0.541\0.505\0.433\0.397\0.384\0.397\0.395\0.382\0.363\0.352\0.343\0.348\0.343\0.336\0.334\0.329\0.319\0.319\0.32\0.319\0.313\0.316\0.318\0.319\0.315\0.313\0.314\0.313\0.312\0.31\0.312\0.308\0.308\0.306\0.306\0.307\0.308\0.305\0.306\0.306\0.305\0.305\0.307\0.308\0.309\0.308\0.308\0.308\0.307\0.306\0.304\0.303\0.3\0.299\0.299\0.297\0.297\0.297\0.298\0.298\0.297\0.297\0.296\0.297\0.298\0.298\0.299\0.298\0.297\0.298\0.298\0.298\0.297\0.296\0.296\0.297\0.297\0.296\0.297\0.296\0.298\0.298\0.297\0.297\0.299\0.297\0.297\0.297\0.296\0.295\0.294\0.295\0.293\0.293\0.293\0.293\0.294\0.293\0.293\0.294\0.294\0.294\0.295\0.294\0.294\0.294\0.295\0.295\0.294\0.294\0.295\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.294\0.293\0.293\0.293\0.293\0.293\0.292\0.293\0.293\0.293\0.294\0.294\0.294\0.294\0.294\0.295\0.295\0.295\0.295\0.295\0.295\0.296\0.296\0.296\0.297\0.296\0.297\0.296\0.297\0.297\0.298\0.298\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.3\0.3\0.299\0.299\0.299\0.299\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.302\0.302\0.301\0.302\0.303\0.303\0.304\0.303\0.305\0.304\0.306\0.307\0.306\0.307\0.307\0.306\0.306\0.306\0.306\0.307\0.307\0.306\0.306\0.307\0.306\0.305\0.305\0.305\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.306\0.306</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>213</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="12gEKK" Title="Quasi-Newton method results">
   <Caption Id="SLsc9x">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.385
0.306
212
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="hI615z" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="iZDokE" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="p2raY1" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ZUh1OI">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0526695\108.005\7.75\10.1036
0.000337625\0.69234\0.0496795\0.0647669
0.0337625\69.234\4.96795\6.47669</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="vzaiZc" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="pMYqkb">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0156612\310.467\6.90641\14.4155
4.60625e-5\0.913139\0.020313\0.0423984
0.00460625\91.3139\2.0313\4.23984</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="mfaoOe" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="HYCTsw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00883484\28.3097\5.19247\4.34808
3.56244e-5\0.114152\0.0209374\0.0175326
0.00356244\11.4152\2.09374\1.75326</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="0p4bpZ" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="bTJVdR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0115738\47.1125\5.86515\4.49267
0.000160747\0.65434\0.0814605\0.0623982
0.0160747\65.434\8.14605\6.23982</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uFtbBf" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="doli4q">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00289321\12.3278\0.76493\0.879817
0.000170189\0.725165\0.0449959\0.0517539
0.0170189\72.5165\4.49959\5.17539</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="YluF97" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="gdVkl5" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="iQJPlY" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="0dk0F1" Title="Quasi-Newton method errors history">
   <Caption Id="HejDaT">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.384725, and the final value after 224 epochs is 0.404535.
The initial value of the selection error is 0.306242, and the final value after 224 epochs is 0.283116.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224</X2Data>
   <Y1Data>0.385\0.385\0.386\0.386\0.388\0.388\0.389\0.39\0.39\0.391\0.392\0.393\0.394\0.395\0.396\0.395\0.395\0.396\0.397\0.397\0.398\0.398\0.399\0.399\0.4\0.399\0.4\0.399\0.4\0.4\0.401\0.4\0.4\0.4\0.4\0.4\0.401\0.401\0.401\0.401\0.401\0.402\0.401\0.401\0.402\0.402\0.402\0.402\0.402\0.403\0.403\0.403\0.402\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.404\0.404\0.404\0.404\0.404\0.404\0.404\0.404\0.404\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.306\0.303\0.304\0.301\0.301\0.3\0.302\0.297\0.3\0.299\0.3\0.3\0.301\0.306\0.309\0.306\0.305\0.3\0.298\0.298\0.297\0.301\0.298\0.297\0.296\0.293\0.295\0.292\0.296\0.295\0.294\0.293\0.292\0.292\0.293\0.291\0.292\0.292\0.291\0.293\0.293\0.293\0.292\0.292\0.292\0.291\0.29\0.289\0.289\0.291\0.292\0.292\0.291\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.289\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>225</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="MW6lrT" Title="Quasi-Newton method results">
   <Caption Id="FBvZk9">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.405
0.283
224
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="rt9NJ7" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="srqHSL" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="78skTZ" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="B9RbkK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.013855\109.398\7.64951\10.1022
8.8814e-5\0.701268\0.0490353\0.0647575
0.0088814\70.1268\4.90353\6.47575</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="vpn5cA" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="FSi77I">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0457516\307.936\6.66772\14.3417
0.000134563\0.905693\0.0196109\0.0421814
0.0134563\90.5693\1.96109\4.21814</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="CdX70F" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="5ovSbh">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00799561\34.8489\4.99987\4.21119
3.22403e-5\0.14052\0.0201608\0.0169806
0.00322403\14.052\2.01608\1.69806</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="MjsKg2" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="RcD5kT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00497055\42.2372\5.27811\4.16886
6.90354e-5\0.586628\0.0733071\0.0579008
0.00690354\58.6628\7.33071\5.79008</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="gJlGL5" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="qdIekj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00650644\15.0089\0.749446\0.924925
0.000382732\0.882874\0.0440851\0.0544073
0.0382732\88.2874\4.40851\5.44073</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="URh8Lw" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="p7gH7e" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="YRxR0P" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="ceBuN4" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="9qiOmF" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="c4Z9sL" Title="Quasi-Newton method errors history">
   <Caption Id="tgoNmc">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.01862, and the final value after 237 epochs is 0.293277.
The initial value of the selection error is 0.737634, and the final value after 237 epochs is 0.249643.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237</X2Data>
   <Y1Data>1.02\0.647\0.579\0.521\0.467\0.439\0.419\0.406\0.394\0.38\0.369\0.363\0.355\0.35\0.345\0.341\0.338\0.335\0.333\0.331\0.33\0.329\0.327\0.326\0.325\0.325\0.323\0.322\0.321\0.32\0.32\0.319\0.319\0.318\0.317\0.317\0.316\0.316\0.316\0.315\0.315\0.314\0.313\0.313\0.312\0.312\0.311\0.311\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.306\0.306\0.305\0.305\0.304\0.304\0.303\0.303\0.302\0.302\0.301\0.301\0.3\0.3\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.738\0.573\0.52\0.493\0.436\0.413\0.387\0.366\0.357\0.353\0.341\0.32\0.306\0.307\0.297\0.293\0.284\0.278\0.274\0.27\0.268\0.269\0.274\0.273\0.272\0.273\0.274\0.273\0.273\0.272\0.27\0.267\0.269\0.271\0.27\0.272\0.27\0.268\0.269\0.27\0.271\0.271\0.27\0.269\0.27\0.272\0.272\0.27\0.271\0.269\0.27\0.27\0.271\0.272\0.272\0.272\0.273\0.273\0.273\0.272\0.272\0.271\0.271\0.271\0.27\0.27\0.268\0.269\0.268\0.269\0.267\0.267\0.266\0.265\0.264\0.264\0.263\0.263\0.262\0.262\0.263\0.265\0.263\0.261\0.26\0.259\0.26\0.26\0.26\0.26\0.259\0.258\0.258\0.258\0.258\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.256\0.255\0.255\0.255\0.254\0.252\0.252\0.251\0.252\0.252\0.251\0.251\0.251\0.251\0.251\0.251\0.252\0.251\0.251\0.25\0.249\0.249\0.249\0.25\0.25\0.25\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.248\0.248\0.248\0.248\0.248\0.248\0.249\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.25\0.25\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.25\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.25\0.25\0.25\0.249\0.249\0.25\0.25</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>238</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="FJHIJh" Title="Quasi-Newton method results">
   <Caption Id="shNE3q">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.293
0.25
237
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="boaYNk" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="pRzkzm" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="ii82oQ" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9zAq2T">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>6.10352e-5\115.34\6.03445\8.12623
3.91251e-7\0.739357\0.0386823\0.0520912
3.91251e-5\73.9357\3.86823\5.20912</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Q3zbxt" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="eEyblK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0165176\303.316\6.62111\13.9005
4.85813e-5\0.892106\0.0194739\0.0408837
0.00485813\89.2106\1.94739\4.08838</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="1qEpfx" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="aZGaA6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0274391\35.0986\4.8245\4.30973
0.000110642\0.141526\0.0194536\0.0173779
0.0110642\14.1526\1.94536\1.73779</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="g7oSbs" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QhbQ7G">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00508118\42.7296\5.15187\4.20066
7.05719e-5\0.593466\0.0715538\0.0583426
0.00705719\59.3466\7.15538\5.83426</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="EmZMs4" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="2EQ6V7">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000364065\14.2768\0.627225\0.958567
2.14156e-5\0.839812\0.0368956\0.0563863
0.00214156\83.9812\3.68956\5.63863</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="aRXZlP" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="TFjP0f" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="y6xqL1" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="4phYJj" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="YlzsXQ" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="NmZAFe" Title="Quasi-Newton method errors history">
   <Caption Id="UDEFwt">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.31569, and the final value after 225 epochs is 0.328161.
The initial value of the selection error is 0.889098, and the final value after 225 epochs is 0.272827.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225</X2Data>
   <Y1Data>1.32\0.779\0.647\0.577\0.529\0.49\0.459\0.425\0.398\0.384\0.376\0.367\0.362\0.356\0.352\0.35\0.348\0.345\0.343\0.342\0.341\0.339\0.338\0.338\0.337\0.337\0.337\0.336\0.336\0.336\0.335\0.335\0.335\0.334\0.334\0.334\0.334\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.889\0.663\0.582\0.499\0.499\0.478\0.455\0.421\0.368\0.347\0.336\0.313\0.318\0.305\0.297\0.286\0.284\0.29\0.292\0.285\0.278\0.279\0.28\0.28\0.274\0.272\0.27\0.272\0.273\0.271\0.274\0.273\0.274\0.273\0.272\0.272\0.274\0.274\0.274\0.276\0.274\0.275\0.274\0.273\0.274\0.276\0.276\0.274\0.275\0.274\0.274\0.275\0.275\0.275\0.273\0.274\0.275\0.275\0.275\0.275\0.273\0.273\0.274\0.273\0.273\0.273\0.273\0.272\0.273\0.272\0.272\0.272\0.271\0.271\0.271\0.272\0.271\0.272\0.272\0.272\0.273\0.272\0.273\0.272\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.274\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.271\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.273\0.273</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>226</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="I8xYEk" Title="Quasi-Newton method results">
   <Caption Id="1xJlcp">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.328
0.273
225
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="5Jh3Zg" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="TPoEX9" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="C0pe7H" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="R7IY5P">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\115.685\6.06932\8.07183
0\0.74157\0.0389059\0.0517425
0\74.1571\3.89059\5.17425</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="YQ5CIy" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="6RLAe1">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00337982\306.754\6.81308\14.5574
9.94065e-6\0.902219\0.0200385\0.042816
0.000994065\90.2219\2.00385\4.2816</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Lm1xsX" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="IaIrc9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0323639\40.3086\5.12675\4.66931
0.0001305\0.162535\0.0206724\0.0188279
0.01305\16.2535\2.06724\1.88279</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="kYVXnq" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="rkI2ML">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00550079\40.1669\5.5693\4.3197
7.63999e-5\0.557873\0.0773514\0.0599958
0.00763999\55.7873\7.73514\5.99958</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="QezmK3" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="siqacC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00209308\14.8212\0.694678\1.02581
0.000123122\0.871834\0.0408634\0.0603417
0.0123122\87.1834\4.08634\6.03417</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="W4uxk6" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="rxmjBe" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="Cpxokn" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="jLyKn4" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="CjXoBv" Title="Optimization algorithm">Stochastic gradient descent is used here for training.
This method updates the neural parameters using an approximation of the gradient of the loss function. </Text>
  <DoubleLineChart Id="Vp6ySO" Title="Stochastic gradient descent errors history">
   <Caption Id="0mxCRY">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.02605, and the final value after 10000 epochs is 0.262694.
The initial value of the selection error is 0.805294, and the final value after 10000 epochs is 0.276881.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661\662\663\664\665\666\667\668\669\670\671\672\673\674\675\676\677\678\679\680\681\682\683\684\685\686\687\688\689\690\691\692\693\694\695\696\697\698\699\700\701\702\703\704\705\706\707\708\709\710\711\712\713\714\715\716\717\718\719\720\721\722\723\724\725\726\727\728\729\730\731\732\733\734\735\736\737\738\739\740\741\742\743\744\745\746\747\748\749\750\751\752\753\754\755\756\757\758\759\760\761\762\763\764\765\766\767\768\769\770\771\772\773\774\775\776\777\778\779\780\781\782\783\784\785\786\787\788\789\790\791\792\793\794\795\796\797\798\799\800\801\802\803\804\805\806\807\808\809\810\811\812\813\814\815\816\817\818\819\820\821\822\823\824\825\826\827\828\829\830\831\832\833\834\835\836\837\838\839\840\841\842\843\844\845\846\847\848\849\850\851\852\853\854\855\856\857\858\859\860\861\862\863\864\865\866\867\868\869\870\871\872\873\874\875\876\877\878\879\880\881\882\883\884\885\886\887\888\889\890\891\892\893\894\895\896\897\898\899\900\901\902\903\904\905\906\907\908\909\910\911\912\913\914\915\916\917\918\919\920\921\922\923\924\925\926\927\928\929\930\931\932\933\934\935\936\937\938\939\940\941\942\943\944\945\946\947\948\949\950\951\952\953\954\955\956\957\958\959\960\961\962\963\964\965\966\967\968\969\970\971\972\973\974\975\976\977\978\979\980\981\982\983\984\985\986\987\988\989\990\991\992\993\994\995\996\997\998\999\1000\1001\1002\1003\1004\1005\1006\1007\1008\1009\1010\1011\1012\1013\1014\1015\1016\1017\1018\1019\1020\1021\1022\1023\1024\1025\1026\1027\1028\1029\1030\1031\1032\1033\1034\1035\1036\1037\1038\1039\1040\1041\1042\1043\1044\1045\1046\1047\1048\1049\1050\1051\1052\1053\1054\1055\1056\1057\1058\1059\1060\1061\1062\1063\1064\1065\1066\1067\1068\1069\1070\1071\1072\1073\1074\1075\1076\1077\1078\1079\1080\1081\1082\1083\1084\1085\1086\1087\1088\1089\1090\1091\1092\1093\1094\1095\1096\1097\1098\1099\1100\1101\1102\1103\1104\1105\1106\1107\1108\1109\1110\1111\1112\1113\1114\1115\1116\1117\1118\1119\1120\1121\1122\1123\1124\1125\1126\1127\1128\1129\1130\1131\1132\1133\1134\1135\1136\1137\1138\1139\1140\1141\1142\1143\1144\1145\1146\1147\1148\1149\1150\1151\1152\1153\1154\1155\1156\1157\1158\1159\1160\1161\1162\1163\1164\1165\1166\1167\1168\1169\1170\1171\1172\1173\1174\1175\1176\1177\1178\1179\1180\1181\1182\1183\1184\1185\1186\1187\1188\1189\1190\1191\1192\1193\1194\1195\1196\1197\1198\1199\1200\1201\1202\1203\1204\1205\1206\1207\1208\1209\1210\1211\1212\1213\1214\1215\1216\1217\1218\1219\1220\1221\1222\1223\1224\1225\1226\1227\1228\1229\1230\1231\1232\1233\1234\1235\1236\1237\1238\1239\1240\1241\1242\1243\1244\1245\1246\1247\1248\1249\1250\1251\1252\1253\1254\1255\1256\1257\1258\1259\1260\1261\1262\1263\1264\1265\1266\1267\1268\1269\1270\1271\1272\1273\1274\1275\1276\1277\1278\1279\1280\1281\1282\1283\1284\1285\1286\1287\1288\1289\1290\1291\1292\1293\1294\1295\1296\1297\1298\1299\1300\1301\1302\1303\1304\1305\1306\1307\1308\1309\1310\1311\1312\1313\1314\1315\1316\1317\1318\1319\1320\1321\1322\1323\1324\1325\1326\1327\1328\1329\1330\1331\1332\1333\1334\1335\1336\1337\1338\1339\1340\1341\1342\1343\1344\1345\1346\1347\1348\1349\1350\1351\1352\1353\1354\1355\1356\1357\1358\1359\1360\1361\1362\1363\1364\1365\1366\1367\1368\1369\1370\1371\1372\1373\1374\1375\1376\1377\1378\1379\1380\1381\1382\1383\1384\1385\1386\1387\1388\1389\1390\1391\1392\1393\1394\1395\1396\1397\1398\1399\1400\1401\1402\1403\1404\1405\1406\1407\1408\1409\1410\1411\1412\1413\1414\1415\1416\1417\1418\1419\1420\1421\1422\1423\1424\1425\1426\1427\1428\1429\1430\1431\1432\1433\1434\1435\1436\1437\1438\1439\1440\1441\1442\1443\1444\1445\1446\1447\1448\1449\1450\1451\1452\1453\1454\1455\1456\1457\1458\1459\1460\1461\1462\1463\1464\1465\1466\1467\1468\1469\1470\1471\1472\1473\1474\1475\1476\1477\1478\1479\1480\1481\1482\1483\1484\1485\1486\1487\1488\1489\1490\1491\1492\1493\1494\1495\1496\1497\1498\1499\1500\1501\1502\1503\1504\1505\1506\1507\1508\1509\1510\1511\1512\1513\1514\1515\1516\1517\1518\1519\1520\1521\1522\1523\1524\1525\1526\1527\1528\1529\1530\1531\1532\1533\1534\1535\1536\1537\1538\1539\1540\1541\1542\1543\1544\1545\1546\1547\1548\1549\1550\1551\1552\1553\1554\1555\1556\1557\1558\1559\1560\1561\1562\1563\1564\1565\1566\1567\1568\1569\1570\1571\1572\1573\1574\1575\1576\1577\1578\1579\1580\1581\1582\1583\1584\1585\1586\1587\1588\1589\1590\1591\1592\1593\1594\1595\1596\1597\1598\1599\1600\1601\1602\1603\1604\1605\1606\1607\1608\1609\1610\1611\1612\1613\1614\1615\1616\1617\1618\1619\1620\1621\1622\1623\1624\1625\1626\1627\1628\1629\1630\1631\1632\1633\1634\1635\1636\1637\1638\1639\1640\1641\1642\1643\1644\1645\1646\1647\1648\1649\1650\1651\1652\1653\1654\1655\1656\1657\1658\1659\1660\1661\1662\1663\1664\1665\1666\1667\1668\1669\1670\1671\1672\1673\1674\1675\1676\1677\1678\1679\1680\1681\1682\1683\1684\1685\1686\1687\1688\1689\1690\1691\1692\1693\1694\1695\1696\1697\1698\1699\1700\1701\1702\1703\1704\1705\1706\1707\1708\1709\1710\1711\1712\1713\1714\1715\1716\1717\1718\1719\1720\1721\1722\1723\1724\1725\1726\1727\1728\1729\1730\1731\1732\1733\1734\1735\1736\1737\1738\1739\1740\1741\1742\1743\1744\1745\1746\1747\1748\1749\1750\1751\1752\1753\1754\1755\1756\1757\1758\1759\1760\1761\1762\1763\1764\1765\1766\1767\1768\1769\1770\1771\1772\1773\1774\1775\1776\1777\1778\1779\1780\1781\1782\1783\1784\1785\1786\1787\1788\1789\1790\1791\1792\1793\1794\1795\1796\1797\1798\1799\1800\1801\1802\1803\1804\1805\1806\1807\1808\1809\1810\1811\1812\1813\1814\1815\1816\1817\1818\1819\1820\1821\1822\1823\1824\1825\1826\1827\1828\1829\1830\1831\1832\1833\1834\1835\1836\1837\1838\1839\1840\1841\1842\1843\1844\1845\1846\1847\1848\1849\1850\1851\1852\1853\1854\1855\1856\1857\1858\1859\1860\1861\1862\1863\1864\1865\1866\1867\1868\1869\1870\1871\1872\1873\1874\1875\1876\1877\1878\1879\1880\1881\1882\1883\1884\1885\1886\1887\1888\1889\1890\1891\1892\1893\1894\1895\1896\1897\1898\1899\1900\1901\1902\1903\1904\1905\1906\1907\1908\1909\1910\1911\1912\1913\1914\1915\1916\1917\1918\1919\1920\1921\1922\1923\1924\1925\1926\1927\1928\1929\1930\1931\1932\1933\1934\1935\1936\1937\1938\1939\1940\1941\1942\1943\1944\1945\1946\1947\1948\1949\1950\1951\1952\1953\1954\1955\1956\1957\1958\1959\1960\1961\1962\1963\1964\1965\1966\1967\1968\1969\1970\1971\1972\1973\1974\1975\1976\1977\1978\1979\1980\1981\1982\1983\1984\1985\1986\1987\1988\1989\1990\1991\1992\1993\1994\1995\1996\1997\1998\1999\2000\2001\2002\2003\2004\2005\2006\2007\2008\2009\2010\2011\2012\2013\2014\2015\2016\2017\2018\2019\2020\2021\2022\2023\2024\2025\2026\2027\2028\2029\2030\2031\2032\2033\2034\2035\2036\2037\2038\2039\2040\2041\2042\2043\2044\2045\2046\2047\2048\2049\2050\2051\2052\2053\2054\2055\2056\2057\2058\2059\2060\2061\2062\2063\2064\2065\2066\2067\2068\2069\2070\2071\2072\2073\2074\2075\2076\2077\2078\2079\2080\2081\2082\2083\2084\2085\2086\2087\2088\2089\2090\2091\2092\2093\2094\2095\2096\2097\2098\2099\2100\2101\2102\2103\2104\2105\2106\2107\2108\2109\2110\2111\2112\2113\2114\2115\2116\2117\2118\2119\2120\2121\2122\2123\2124\2125\2126\2127\2128\2129\2130\2131\2132\2133\2134\2135\2136\2137\2138\2139\2140\2141\2142\2143\2144\2145\2146\2147\2148\2149\2150\2151\2152\2153\2154\2155\2156\2157\2158\2159\2160\2161\2162\2163\2164\2165\2166\2167\2168\2169\2170\2171\2172\2173\2174\2175\2176\2177\2178\2179\2180\2181\2182\2183\2184\2185\2186\2187\2188\2189\2190\2191\2192\2193\2194\2195\2196\2197\2198\2199\2200\2201\2202\2203\2204\2205\2206\2207\2208\2209\2210\2211\2212\2213\2214\2215\2216\2217\2218\2219\2220\2221\2222\2223\2224\2225\2226\2227\2228\2229\2230\2231\2232\2233\2234\2235\2236\2237\2238\2239\2240\2241\2242\2243\2244\2245\2246\2247\2248\2249\2250\2251\2252\2253\2254\2255\2256\2257\2258\2259\2260\2261\2262\2263\2264\2265\2266\2267\2268\2269\2270\2271\2272\2273\2274\2275\2276\2277\2278\2279\2280\2281\2282\2283\2284\2285\2286\2287\2288\2289\2290\2291\2292\2293\2294\2295\2296\2297\2298\2299\2300\2301\2302\2303\2304\2305\2306\2307\2308\2309\2310\2311\2312\2313\2314\2315\2316\2317\2318\2319\2320\2321\2322\2323\2324\2325\2326\2327\2328\2329\2330\2331\2332\2333\2334\2335\2336\2337\2338\2339\2340\2341\2342\2343\2344\2345\2346\2347\2348\2349\2350\2351\2352\2353\2354\2355\2356\2357\2358\2359\2360\2361\2362\2363\2364\2365\2366\2367\2368\2369\2370\2371\2372\2373\2374\2375\2376\2377\2378\2379\2380\2381\2382\2383\2384\2385\2386\2387\2388\2389\2390\2391\2392\2393\2394\2395\2396\2397\2398\2399\2400\2401\2402\2403\2404\2405\2406\2407\2408\2409\2410\2411\2412\2413\2414\2415\2416\2417\2418\2419\2420\2421\2422\2423\2424\2425\2426\2427\2428\2429\2430\2431\2432\2433\2434\2435\2436\2437\2438\2439\2440\2441\2442\2443\2444\2445\2446\2447\2448\2449\2450\2451\2452\2453\2454\2455\2456\2457\2458\2459\2460\2461\2462\2463\2464\2465\2466\2467\2468\2469\2470\2471\2472\2473\2474\2475\2476\2477\2478\2479\2480\2481\2482\2483\2484\2485\2486\2487\2488\2489\2490\2491\2492\2493\2494\2495\2496\2497\2498\2499\2500\2501\2502\2503\2504\2505\2506\2507\2508\2509\2510\2511\2512\2513\2514\2515\2516\2517\2518\2519\2520\2521\2522\2523\2524\2525\2526\2527\2528\2529\2530\2531\2532\2533\2534\2535\2536\2537\2538\2539\2540\2541\2542\2543\2544\2545\2546\2547\2548\2549\2550\2551\2552\2553\2554\2555\2556\2557\2558\2559\2560\2561\2562\2563\2564\2565\2566\2567\2568\2569\2570\2571\2572\2573\2574\2575\2576\2577\2578\2579\2580\2581\2582\2583\2584\2585\2586\2587\2588\2589\2590\2591\2592\2593\2594\2595\2596\2597\2598\2599\2600\2601\2602\2603\2604\2605\2606\2607\2608\2609\2610\2611\2612\2613\2614\2615\2616\2617\2618\2619\2620\2621\2622\2623\2624\2625\2626\2627\2628\2629\2630\2631\2632\2633\2634\2635\2636\2637\2638\2639\2640\2641\2642\2643\2644\2645\2646\2647\2648\2649\2650\2651\2652\2653\2654\2655\2656\2657\2658\2659\2660\2661\2662\2663\2664\2665\2666\2667\2668\2669\2670\2671\2672\2673\2674\2675\2676\2677\2678\2679\2680\2681\2682\2683\2684\2685\2686\2687\2688\2689\2690\2691\2692\2693\2694\2695\2696\2697\2698\2699\2700\2701\2702\2703\2704\2705\2706\2707\2708\2709\2710\2711\2712\2713\2714\2715\2716\2717\2718\2719\2720\2721\2722\2723\2724\2725\2726\2727\2728\2729\2730\2731\2732\2733\2734\2735\2736\2737\2738\2739\2740\2741\2742\2743\2744\2745\2746\2747\2748\2749\2750\2751\2752\2753\2754\2755\2756\2757\2758\2759\2760\2761\2762\2763\2764\2765\2766\2767\2768\2769\2770\2771\2772\2773\2774\2775\2776\2777\2778\2779\2780\2781\2782\2783\2784\2785\2786\2787\2788\2789\2790\2791\2792\2793\2794\2795\2796\2797\2798\2799\2800\2801\2802\2803\2804\2805\2806\2807\2808\2809\2810\2811\2812\2813\2814\2815\2816\2817\2818\2819\2820\2821\2822\2823\2824\2825\2826\2827\2828\2829\2830\2831\2832\2833\2834\2835\2836\2837\2838\2839\2840\2841\2842\2843\2844\2845\2846\2847\2848\2849\2850\2851\2852\2853\2854\2855\2856\2857\2858\2859\2860\2861\2862\2863\2864\2865\2866\2867\2868\2869\2870\2871\2872\2873\2874\2875\2876\2877\2878\2879\2880\2881\2882\2883\2884\2885\2886\2887\2888\2889\2890\2891\2892\2893\2894\2895\2896\2897\2898\2899\2900\2901\2902\2903\2904\2905\2906\2907\2908\2909\2910\2911\2912\2913\2914\2915\2916\2917\2918\2919\2920\2921\2922\2923\2924\2925\2926\2927\2928\2929\2930\2931\2932\2933\2934\2935\2936\2937\2938\2939\2940\2941\2942\2943\2944\2945\2946\2947\2948\2949\2950\2951\2952\2953\2954\2955\2956\2957\2958\2959\2960\2961\2962\2963\2964\2965\2966\2967\2968\2969\2970\2971\2972\2973\2974\2975\2976\2977\2978\2979\2980\2981\2982\2983\2984\2985\2986\2987\2988\2989\2990\2991\2992\2993\2994\2995\2996\2997\2998\2999\3000\3001\3002\3003\3004\3005\3006\3007\3008\3009\3010\3011\3012\3013\3014\3015\3016\3017\3018\3019\3020\3021\3022\3023\3024\3025\3026\3027\3028\3029\3030\3031\3032\3033\3034\3035\3036\3037\3038\3039\3040\3041\3042\3043\3044\3045\3046\3047\3048\3049\3050\3051\3052\3053\3054\3055\3056\3057\3058\3059\3060\3061\3062\3063\3064\3065\3066\3067\3068\3069\3070\3071\3072\3073\3074\3075\3076\3077\3078\3079\3080\3081\3082\3083\3084\3085\3086\3087\3088\3089\3090\3091\3092\3093\3094\3095\3096\3097\3098\3099\3100\3101\3102\3103\3104\3105\3106\3107\3108\3109\3110\3111\3112\3113\3114\3115\3116\3117\3118\3119\3120\3121\3122\3123\3124\3125\3126\3127\3128\3129\3130\3131\3132\3133\3134\3135\3136\3137\3138\3139\3140\3141\3142\3143\3144\3145\3146\3147\3148\3149\3150\3151\3152\3153\3154\3155\3156\3157\3158\3159\3160\3161\3162\3163\3164\3165\3166\3167\3168\3169\3170\3171\3172\3173\3174\3175\3176\3177\3178\3179\3180\3181\3182\3183\3184\3185\3186\3187\3188\3189\3190\3191\3192\3193\3194\3195\3196\3197\3198\3199\3200\3201\3202\3203\3204\3205\3206\3207\3208\3209\3210\3211\3212\3213\3214\3215\3216\3217\3218\3219\3220\3221\3222\3223\3224\3225\3226\3227\3228\3229\3230\3231\3232\3233\3234\3235\3236\3237\3238\3239\3240\3241\3242\3243\3244\3245\3246\3247\3248\3249\3250\3251\3252\3253\3254\3255\3256\3257\3258\3259\3260\3261\3262\3263\3264\3265\3266\3267\3268\3269\3270\3271\3272\3273\3274\3275\3276\3277\3278\3279\3280\3281\3282\3283\3284\3285\3286\3287\3288\3289\3290\3291\3292\3293\3294\3295\3296\3297\3298\3299\3300\3301\3302\3303\3304\3305\3306\3307\3308\3309\3310\3311\3312\3313\3314\3315\3316\3317\3318\3319\3320\3321\3322\3323\3324\3325\3326\3327\3328\3329\3330\3331\3332\3333\3334\3335\3336\3337\3338\3339\3340\3341\3342\3343\3344\3345\3346\3347\3348\3349\3350\3351\3352\3353\3354\3355\3356\3357\3358\3359\3360\3361\3362\3363\3364\3365\3366\3367\3368\3369\3370\3371\3372\3373\3374\3375\3376\3377\3378\3379\3380\3381\3382\3383\3384\3385\3386\3387\3388\3389\3390\3391\3392\3393\3394\3395\3396\3397\3398\3399\3400\3401\3402\3403\3404\3405\3406\3407\3408\3409\3410\3411\3412\3413\3414\3415\3416\3417\3418\3419\3420\3421\3422\3423\3424\3425\3426\3427\3428\3429\3430\3431\3432\3433\3434\3435\3436\3437\3438\3439\3440\3441\3442\3443\3444\3445\3446\3447\3448\3449\3450\3451\3452\3453\3454\3455\3456\3457\3458\3459\3460\3461\3462\3463\3464\3465\3466\3467\3468\3469\3470\3471\3472\3473\3474\3475\3476\3477\3478\3479\3480\3481\3482\3483\3484\3485\3486\3487\3488\3489\3490\3491\3492\3493\3494\3495\3496\3497\3498\3499\3500\3501\3502\3503\3504\3505\3506\3507\3508\3509\3510\3511\3512\3513\3514\3515\3516\3517\3518\3519\3520\3521\3522\3523\3524\3525\3526\3527\3528\3529\3530\3531\3532\3533\3534\3535\3536\3537\3538\3539\3540\3541\3542\3543\3544\3545\3546\3547\3548\3549\3550\3551\3552\3553\3554\3555\3556\3557\3558\3559\3560\3561\3562\3563\3564\3565\3566\3567\3568\3569\3570\3571\3572\3573\3574\3575\3576\3577\3578\3579\3580\3581\3582\3583\3584\3585\3586\3587\3588\3589\3590\3591\3592\3593\3594\3595\3596\3597\3598\3599\3600\3601\3602\3603\3604\3605\3606\3607\3608\3609\3610\3611\3612\3613\3614\3615\3616\3617\3618\3619\3620\3621\3622\3623\3624\3625\3626\3627\3628\3629\3630\3631\3632\3633\3634\3635\3636\3637\3638\3639\3640\3641\3642\3643\3644\3645\3646\3647\3648\3649\3650\3651\3652\3653\3654\3655\3656\3657\3658\3659\3660\3661\3662\3663\3664\3665\3666\3667\3668\3669\3670\3671\3672\3673\3674\3675\3676\3677\3678\3679\3680\3681\3682\3683\3684\3685\3686\3687\3688\3689\3690\3691\3692\3693\3694\3695\3696\3697\3698\3699\3700\3701\3702\3703\3704\3705\3706\3707\3708\3709\3710\3711\3712\3713\3714\3715\3716\3717\3718\3719\3720\3721\3722\3723\3724\3725\3726\3727\3728\3729\3730\3731\3732\3733\3734\3735\3736\3737\3738\3739\3740\3741\3742\3743\3744\3745\3746\3747\3748\3749\3750\3751\3752\3753\3754\3755\3756\3757\3758\3759\3760\3761\3762\3763\3764\3765\3766\3767\3768\3769\3770\3771\3772\3773\3774\3775\3776\3777\3778\3779\3780\3781\3782\3783\3784\3785\3786\3787\3788\3789\3790\3791\3792\3793\3794\3795\3796\3797\3798\3799\3800\3801\3802\3803\3804\3805\3806\3807\3808\3809\3810\3811\3812\3813\3814\3815\3816\3817\3818\3819\3820\3821\3822\3823\3824\3825\3826\3827\3828\3829\3830\3831\3832\3833\3834\3835\3836\3837\3838\3839\3840\3841\3842\3843\3844\3845\3846\3847\3848\3849\3850\3851\3852\3853\3854\3855\3856\3857\3858\3859\3860\3861\3862\3863\3864\3865\3866\3867\3868\3869\3870\3871\3872\3873\3874\3875\3876\3877\3878\3879\3880\3881\3882\3883\3884\3885\3886\3887\3888\3889\3890\3891\3892\3893\3894\3895\3896\3897\3898\3899\3900\3901\3902\3903\3904\3905\3906\3907\3908\3909\3910\3911\3912\3913\3914\3915\3916\3917\3918\3919\3920\3921\3922\3923\3924\3925\3926\3927\3928\3929\3930\3931\3932\3933\3934\3935\3936\3937\3938\3939\3940\3941\3942\3943\3944\3945\3946\3947\3948\3949\3950\3951\3952\3953\3954\3955\3956\3957\3958\3959\3960\3961\3962\3963\3964\3965\3966\3967\3968\3969\3970\3971\3972\3973\3974\3975\3976\3977\3978\3979\3980\3981\3982\3983\3984\3985\3986\3987\3988\3989\3990\3991\3992\3993\3994\3995\3996\3997\3998\3999\4000\4001\4002\4003\4004\4005\4006\4007\4008\4009\4010\4011\4012\4013\4014\4015\4016\4017\4018\4019\4020\4021\4022\4023\4024\4025\4026\4027\4028\4029\4030\4031\4032\4033\4034\4035\4036\4037\4038\4039\4040\4041\4042\4043\4044\4045\4046\4047\4048\4049\4050\4051\4052\4053\4054\4055\4056\4057\4058\4059\4060\4061\4062\4063\4064\4065\4066\4067\4068\4069\4070\4071\4072\4073\4074\4075\4076\4077\4078\4079\4080\4081\4082\4083\4084\4085\4086\4087\4088\4089\4090\4091\4092\4093\4094\4095\4096\4097\4098\4099\4100\4101\4102\4103\4104\4105\4106\4107\4108\4109\4110\4111\4112\4113\4114\4115\4116\4117\4118\4119\4120\4121\4122\4123\4124\4125\4126\4127\4128\4129\4130\4131\4132\4133\4134\4135\4136\4137\4138\4139\4140\4141\4142\4143\4144\4145\4146\4147\4148\4149\4150\4151\4152\4153\4154\4155\4156\4157\4158\4159\4160\4161\4162\4163\4164\4165\4166\4167\4168\4169\4170\4171\4172\4173\4174\4175\4176\4177\4178\4179\4180\4181\4182\4183\4184\4185\4186\4187\4188\4189\4190\4191\4192\4193\4194\4195\4196\4197\4198\4199\4200\4201\4202\4203\4204\4205\4206\4207\4208\4209\4210\4211\4212\4213\4214\4215\4216\4217\4218\4219\4220\4221\4222\4223\4224\4225\4226\4227\4228\4229\4230\4231\4232\4233\4234\4235\4236\4237\4238\4239\4240\4241\4242\4243\4244\4245\4246\4247\4248\4249\4250\4251\4252\4253\4254\4255\4256\4257\4258\4259\4260\4261\4262\4263\4264\4265\4266\4267\4268\4269\4270\4271\4272\4273\4274\4275\4276\4277\4278\4279\4280\4281\4282\4283\4284\4285\4286\4287\4288\4289\4290\4291\4292\4293\4294\4295\4296\4297\4298\4299\4300\4301\4302\4303\4304\4305\4306\4307\4308\4309\4310\4311\4312\4313\4314\4315\4316\4317\4318\4319\4320\4321\4322\4323\4324\4325\4326\4327\4328\4329\4330\4331\4332\4333\4334\4335\4336\4337\4338\4339\4340\4341\4342\4343\4344\4345\4346\4347\4348\4349\4350\4351\4352\4353\4354\4355\4356\4357\4358\4359\4360\4361\4362\4363\4364\4365\4366\4367\4368\4369\4370\4371\4372\4373\4374\4375\4376\4377\4378\4379\4380\4381\4382\4383\4384\4385\4386\4387\4388\4389\4390\4391\4392\4393\4394\4395\4396\4397\4398\4399\4400\4401\4402\4403\4404\4405\4406\4407\4408\4409\4410\4411\4412\4413\4414\4415\4416\4417\4418\4419\4420\4421\4422\4423\4424\4425\4426\4427\4428\4429\4430\4431\4432\4433\4434\4435\4436\4437\4438\4439\4440\4441\4442\4443\4444\4445\4446\4447\4448\4449\4450\4451\4452\4453\4454\4455\4456\4457\4458\4459\4460\4461\4462\4463\4464\4465\4466\4467\4468\4469\4470\4471\4472\4473\4474\4475\4476\4477\4478\4479\4480\4481\4482\4483\4484\4485\4486\4487\4488\4489\4490\4491\4492\4493\4494\4495\4496\4497\4498\4499\4500\4501\4502\4503\4504\4505\4506\4507\4508\4509\4510\4511\4512\4513\4514\4515\4516\4517\4518\4519\4520\4521\4522\4523\4524\4525\4526\4527\4528\4529\4530\4531\4532\4533\4534\4535\4536\4537\4538\4539\4540\4541\4542\4543\4544\4545\4546\4547\4548\4549\4550\4551\4552\4553\4554\4555\4556\4557\4558\4559\4560\4561\4562\4563\4564\4565\4566\4567\4568\4569\4570\4571\4572\4573\4574\4575\4576\4577\4578\4579\4580\4581\4582\4583\4584\4585\4586\4587\4588\4589\4590\4591\4592\4593\4594\4595\4596\4597\4598\4599\4600\4601\4602\4603\4604\4605\4606\4607\4608\4609\4610\4611\4612\4613\4614\4615\4616\4617\4618\4619\4620\4621\4622\4623\4624\4625\4626\4627\4628\4629\4630\4631\4632\4633\4634\4635\4636\4637\4638\4639\4640\4641\4642\4643\4644\4645\4646\4647\4648\4649\4650\4651\4652\4653\4654\4655\4656\4657\4658\4659\4660\4661\4662\4663\4664\4665\4666\4667\4668\4669\4670\4671\4672\4673\4674\4675\4676\4677\4678\4679\4680\4681\4682\4683\4684\4685\4686\4687\4688\4689\4690\4691\4692\4693\4694\4695\4696\4697\4698\4699\4700\4701\4702\4703\4704\4705\4706\4707\4708\4709\4710\4711\4712\4713\4714\4715\4716\4717\4718\4719\4720\4721\4722\4723\4724\4725\4726\4727\4728\4729\4730\4731\4732\4733\4734\4735\4736\4737\4738\4739\4740\4741\4742\4743\4744\4745\4746\4747\4748\4749\4750\4751\4752\4753\4754\4755\4756\4757\4758\4759\4760\4761\4762\4763\4764\4765\4766\4767\4768\4769\4770\4771\4772\4773\4774\4775\4776\4777\4778\4779\4780\4781\4782\4783\4784\4785\4786\4787\4788\4789\4790\4791\4792\4793\4794\4795\4796\4797\4798\4799\4800\4801\4802\4803\4804\4805\4806\4807\4808\4809\4810\4811\4812\4813\4814\4815\4816\4817\4818\4819\4820\4821\4822\4823\4824\4825\4826\4827\4828\4829\4830\4831\4832\4833\4834\4835\4836\4837\4838\4839\4840\4841\4842\4843\4844\4845\4846\4847\4848\4849\4850\4851\4852\4853\4854\4855\4856\4857\4858\4859\4860\4861\4862\4863\4864\4865\4866\4867\4868\4869\4870\4871\4872\4873\4874\4875\4876\4877\4878\4879\4880\4881\4882\4883\4884\4885\4886\4887\4888\4889\4890\4891\4892\4893\4894\4895\4896\4897\4898\4899\4900\4901\4902\4903\4904\4905\4906\4907\4908\4909\4910\4911\4912\4913\4914\4915\4916\4917\4918\4919\4920\4921\4922\4923\4924\4925\4926\4927\4928\4929\4930\4931\4932\4933\4934\4935\4936\4937\4938\4939\4940\4941\4942\4943\4944\4945\4946\4947\4948\4949\4950\4951\4952\4953\4954\4955\4956\4957\4958\4959\4960\4961\4962\4963\4964\4965\4966\4967\4968\4969\4970\4971\4972\4973\4974\4975\4976\4977\4978\4979\4980\4981\4982\4983\4984\4985\4986\4987\4988\4989\4990\4991\4992\4993\4994\4995\4996\4997\4998\4999\5000\5001\5002\5003\5004\5005\5006\5007\5008\5009\5010\5011\5012\5013\5014\5015\5016\5017\5018\5019\5020\5021\5022\5023\5024\5025\5026\5027\5028\5029\5030\5031\5032\5033\5034\5035\5036\5037\5038\5039\5040\5041\5042\5043\5044\5045\5046\5047\5048\5049\5050\5051\5052\5053\5054\5055\5056\5057\5058\5059\5060\5061\5062\5063\5064\5065\5066\5067\5068\5069\5070\5071\5072\5073\5074\5075\5076\5077\5078\5079\5080\5081\5082\5083\5084\5085\5086\5087\5088\5089\5090\5091\5092\5093\5094\5095\5096\5097\5098\5099\5100\5101\5102\5103\5104\5105\5106\5107\5108\5109\5110\5111\5112\5113\5114\5115\5116\5117\5118\5119\5120\5121\5122\5123\5124\5125\5126\5127\5128\5129\5130\5131\5132\5133\5134\5135\5136\5137\5138\5139\5140\5141\5142\5143\5144\5145\5146\5147\5148\5149\5150\5151\5152\5153\5154\5155\5156\5157\5158\5159\5160\5161\5162\5163\5164\5165\5166\5167\5168\5169\5170\5171\5172\5173\5174\5175\5176\5177\5178\5179\5180\5181\5182\5183\5184\5185\5186\5187\5188\5189\5190\5191\5192\5193\5194\5195\5196\5197\5198\5199\5200\5201\5202\5203\5204\5205\5206\5207\5208\5209\5210\5211\5212\5213\5214\5215\5216\5217\5218\5219\5220\5221\5222\5223\5224\5225\5226\5227\5228\5229\5230\5231\5232\5233\5234\5235\5236\5237\5238\5239\5240\5241\5242\5243\5244\5245\5246\5247\5248\5249\5250\5251\5252\5253\5254\5255\5256\5257\5258\5259\5260\5261\5262\5263\5264\5265\5266\5267\5268\5269\5270\5271\5272\5273\5274\5275\5276\5277\5278\5279\5280\5281\5282\5283\5284\5285\5286\5287\5288\5289\5290\5291\5292\5293\5294\5295\5296\5297\5298\5299\5300\5301\5302\5303\5304\5305\5306\5307\5308\5309\5310\5311\5312\5313\5314\5315\5316\5317\5318\5319\5320\5321\5322\5323\5324\5325\5326\5327\5328\5329\5330\5331\5332\5333\5334\5335\5336\5337\5338\5339\5340\5341\5342\5343\5344\5345\5346\5347\5348\5349\5350\5351\5352\5353\5354\5355\5356\5357\5358\5359\5360\5361\5362\5363\5364\5365\5366\5367\5368\5369\5370\5371\5372\5373\5374\5375\5376\5377\5378\5379\5380\5381\5382\5383\5384\5385\5386\5387\5388\5389\5390\5391\5392\5393\5394\5395\5396\5397\5398\5399\5400\5401\5402\5403\5404\5405\5406\5407\5408\5409\5410\5411\5412\5413\5414\5415\5416\5417\5418\5419\5420\5421\5422\5423\5424\5425\5426\5427\5428\5429\5430\5431\5432\5433\5434\5435\5436\5437\5438\5439\5440\5441\5442\5443\5444\5445\5446\5447\5448\5449\5450\5451\5452\5453\5454\5455\5456\5457\5458\5459\5460\5461\5462\5463\5464\5465\5466\5467\5468\5469\5470\5471\5472\5473\5474\5475\5476\5477\5478\5479\5480\5481\5482\5483\5484\5485\5486\5487\5488\5489\5490\5491\5492\5493\5494\5495\5496\5497\5498\5499\5500\5501\5502\5503\5504\5505\5506\5507\5508\5509\5510\5511\5512\5513\5514\5515\5516\5517\5518\5519\5520\5521\5522\5523\5524\5525\5526\5527\5528\5529\5530\5531\5532\5533\5534\5535\5536\5537\5538\5539\5540\5541\5542\5543\5544\5545\5546\5547\5548\5549\5550\5551\5552\5553\5554\5555\5556\5557\5558\5559\5560\5561\5562\5563\5564\5565\5566\5567\5568\5569\5570\5571\5572\5573\5574\5575\5576\5577\5578\5579\5580\5581\5582\5583\5584\5585\5586\5587\5588\5589\5590\5591\5592\5593\5594\5595\5596\5597\5598\5599\5600\5601\5602\5603\5604\5605\5606\5607\5608\5609\5610\5611\5612\5613\5614\5615\5616\5617\5618\5619\5620\5621\5622\5623\5624\5625\5626\5627\5628\5629\5630\5631\5632\5633\5634\5635\5636\5637\5638\5639\5640\5641\5642\5643\5644\5645\5646\5647\5648\5649\5650\5651\5652\5653\5654\5655\5656\5657\5658\5659\5660\5661\5662\5663\5664\5665\5666\5667\5668\5669\5670\5671\5672\5673\5674\5675\5676\5677\5678\5679\5680\5681\5682\5683\5684\5685\5686\5687\5688\5689\5690\5691\5692\5693\5694\5695\5696\5697\5698\5699\5700\5701\5702\5703\5704\5705\5706\5707\5708\5709\5710\5711\5712\5713\5714\5715\5716\5717\5718\5719\5720\5721\5722\5723\5724\5725\5726\5727\5728\5729\5730\5731\5732\5733\5734\5735\5736\5737\5738\5739\5740\5741\5742\5743\5744\5745\5746\5747\5748\5749\5750\5751\5752\5753\5754\5755\5756\5757\5758\5759\5760\5761\5762\5763\5764\5765\5766\5767\5768\5769\5770\5771\5772\5773\5774\5775\5776\5777\5778\5779\5780\5781\5782\5783\5784\5785\5786\5787\5788\5789\5790\5791\5792\5793\5794\5795\5796\5797\5798\5799\5800\5801\5802\5803\5804\5805\5806\5807\5808\5809\5810\5811\5812\5813\5814\5815\5816\5817\5818\5819\5820\5821\5822\5823\5824\5825\5826\5827\5828\5829\5830\5831\5832\5833\5834\5835\5836\5837\5838\5839\5840\5841\5842\5843\5844\5845\5846\5847\5848\5849\5850\5851\5852\5853\5854\5855\5856\5857\5858\5859\5860\5861\5862\5863\5864\5865\5866\5867\5868\5869\5870\5871\5872\5873\5874\5875\5876\5877\5878\5879\5880\5881\5882\5883\5884\5885\5886\5887\5888\5889\5890\5891\5892\5893\5894\5895\5896\5897\5898\5899\5900\5901\5902\5903\5904\5905\5906\5907\5908\5909\5910\5911\5912\5913\5914\5915\5916\5917\5918\5919\5920\5921\5922\5923\5924\5925\5926\5927\5928\5929\5930\5931\5932\5933\5934\5935\5936\5937\5938\5939\5940\5941\5942\5943\5944\5945\5946\5947\5948\5949\5950\5951\5952\5953\5954\5955\5956\5957\5958\5959\5960\5961\5962\5963\5964\5965\5966\5967\5968\5969\5970\5971\5972\5973\5974\5975\5976\5977\5978\5979\5980\5981\5982\5983\5984\5985\5986\5987\5988\5989\5990\5991\5992\5993\5994\5995\5996\5997\5998\5999\6000\6001\6002\6003\6004\6005\6006\6007\6008\6009\6010\6011\6012\6013\6014\6015\6016\6017\6018\6019\6020\6021\6022\6023\6024\6025\6026\6027\6028\6029\6030\6031\6032\6033\6034\6035\6036\6037\6038\6039\6040\6041\6042\6043\6044\6045\6046\6047\6048\6049\6050\6051\6052\6053\6054\6055\6056\6057\6058\6059\6060\6061\6062\6063\6064\6065\6066\6067\6068\6069\6070\6071\6072\6073\6074\6075\6076\6077\6078\6079\6080\6081\6082\6083\6084\6085\6086\6087\6088\6089\6090\6091\6092\6093\6094\6095\6096\6097\6098\6099\6100\6101\6102\6103\6104\6105\6106\6107\6108\6109\6110\6111\6112\6113\6114\6115\6116\6117\6118\6119\6120\6121\6122\6123\6124\6125\6126\6127\6128\6129\6130\6131\6132\6133\6134\6135\6136\6137\6138\6139\6140\6141\6142\6143\6144\6145\6146\6147\6148\6149\6150\6151\6152\6153\6154\6155\6156\6157\6158\6159\6160\6161\6162\6163\6164\6165\6166\6167\6168\6169\6170\6171\6172\6173\6174\6175\6176\6177\6178\6179\6180\6181\6182\6183\6184\6185\6186\6187\6188\6189\6190\6191\6192\6193\6194\6195\6196\6197\6198\6199\6200\6201\6202\6203\6204\6205\6206\6207\6208\6209\6210\6211\6212\6213\6214\6215\6216\6217\6218\6219\6220\6221\6222\6223\6224\6225\6226\6227\6228\6229\6230\6231\6232\6233\6234\6235\6236\6237\6238\6239\6240\6241\6242\6243\6244\6245\6246\6247\6248\6249\6250\6251\6252\6253\6254\6255\6256\6257\6258\6259\6260\6261\6262\6263\6264\6265\6266\6267\6268\6269\6270\6271\6272\6273\6274\6275\6276\6277\6278\6279\6280\6281\6282\6283\6284\6285\6286\6287\6288\6289\6290\6291\6292\6293\6294\6295\6296\6297\6298\6299\6300\6301\6302\6303\6304\6305\6306\6307\6308\6309\6310\6311\6312\6313\6314\6315\6316\6317\6318\6319\6320\6321\6322\6323\6324\6325\6326\6327\6328\6329\6330\6331\6332\6333\6334\6335\6336\6337\6338\6339\6340\6341\6342\6343\6344\6345\6346\6347\6348\6349\6350\6351\6352\6353\6354\6355\6356\6357\6358\6359\6360\6361\6362\6363\6364\6365\6366\6367\6368\6369\6370\6371\6372\6373\6374\6375\6376\6377\6378\6379\6380\6381\6382\6383\6384\6385\6386\6387\6388\6389\6390\6391\6392\6393\6394\6395\6396\6397\6398\6399\6400\6401\6402\6403\6404\6405\6406\6407\6408\6409\6410\6411\6412\6413\6414\6415\6416\6417\6418\6419\6420\6421\6422\6423\6424\6425\6426\6427\6428\6429\6430\6431\6432\6433\6434\6435\6436\6437\6438\6439\6440\6441\6442\6443\6444\6445\6446\6447\6448\6449\6450\6451\6452\6453\6454\6455\6456\6457\6458\6459\6460\6461\6462\6463\6464\6465\6466\6467\6468\6469\6470\6471\6472\6473\6474\6475\6476\6477\6478\6479\6480\6481\6482\6483\6484\6485\6486\6487\6488\6489\6490\6491\6492\6493\6494\6495\6496\6497\6498\6499\6500\6501\6502\6503\6504\6505\6506\6507\6508\6509\6510\6511\6512\6513\6514\6515\6516\6517\6518\6519\6520\6521\6522\6523\6524\6525\6526\6527\6528\6529\6530\6531\6532\6533\6534\6535\6536\6537\6538\6539\6540\6541\6542\6543\6544\6545\6546\6547\6548\6549\6550\6551\6552\6553\6554\6555\6556\6557\6558\6559\6560\6561\6562\6563\6564\6565\6566\6567\6568\6569\6570\6571\6572\6573\6574\6575\6576\6577\6578\6579\6580\6581\6582\6583\6584\6585\6586\6587\6588\6589\6590\6591\6592\6593\6594\6595\6596\6597\6598\6599\6600\6601\6602\6603\6604\6605\6606\6607\6608\6609\6610\6611\6612\6613\6614\6615\6616\6617\6618\6619\6620\6621\6622\6623\6624\6625\6626\6627\6628\6629\6630\6631\6632\6633\6634\6635\6636\6637\6638\6639\6640\6641\6642\6643\6644\6645\6646\6647\6648\6649\6650\6651\6652\6653\6654\6655\6656\6657\6658\6659\6660\6661\6662\6663\6664\6665\6666\6667\6668\6669\6670\6671\6672\6673\6674\6675\6676\6677\6678\6679\6680\6681\6682\6683\6684\6685\6686\6687\6688\6689\6690\6691\6692\6693\6694\6695\6696\6697\6698\6699\6700\6701\6702\6703\6704\6705\6706\6707\6708\6709\6710\6711\6712\6713\6714\6715\6716\6717\6718\6719\6720\6721\6722\6723\6724\6725\6726\6727\6728\6729\6730\6731\6732\6733\6734\6735\6736\6737\6738\6739\6740\6741\6742\6743\6744\6745\6746\6747\6748\6749\6750\6751\6752\6753\6754\6755\6756\6757\6758\6759\6760\6761\6762\6763\6764\6765\6766\6767\6768\6769\6770\6771\6772\6773\6774\6775\6776\6777\6778\6779\6780\6781\6782\6783\6784\6785\6786\6787\6788\6789\6790\6791\6792\6793\6794\6795\6796\6797\6798\6799\6800\6801\6802\6803\6804\6805\6806\6807\6808\6809\6810\6811\6812\6813\6814\6815\6816\6817\6818\6819\6820\6821\6822\6823\6824\6825\6826\6827\6828\6829\6830\6831\6832\6833\6834\6835\6836\6837\6838\6839\6840\6841\6842\6843\6844\6845\6846\6847\6848\6849\6850\6851\6852\6853\6854\6855\6856\6857\6858\6859\6860\6861\6862\6863\6864\6865\6866\6867\6868\6869\6870\6871\6872\6873\6874\6875\6876\6877\6878\6879\6880\6881\6882\6883\6884\6885\6886\6887\6888\6889\6890\6891\6892\6893\6894\6895\6896\6897\6898\6899\6900\6901\6902\6903\6904\6905\6906\6907\6908\6909\6910\6911\6912\6913\6914\6915\6916\6917\6918\6919\6920\6921\6922\6923\6924\6925\6926\6927\6928\6929\6930\6931\6932\6933\6934\6935\6936\6937\6938\6939\6940\6941\6942\6943\6944\6945\6946\6947\6948\6949\6950\6951\6952\6953\6954\6955\6956\6957\6958\6959\6960\6961\6962\6963\6964\6965\6966\6967\6968\6969\6970\6971\6972\6973\6974\6975\6976\6977\6978\6979\6980\6981\6982\6983\6984\6985\6986\6987\6988\6989\6990\6991\6992\6993\6994\6995\6996\6997\6998\6999\7000\7001\7002\7003\7004\7005\7006\7007\7008\7009\7010\7011\7012\7013\7014\7015\7016\7017\7018\7019\7020\7021\7022\7023\7024\7025\7026\7027\7028\7029\7030\7031\7032\7033\7034\7035\7036\7037\7038\7039\7040\7041\7042\7043\7044\7045\7046\7047\7048\7049\7050\7051\7052\7053\7054\7055\7056\7057\7058\7059\7060\7061\7062\7063\7064\7065\7066\7067\7068\7069\7070\7071\7072\7073\7074\7075\7076\7077\7078\7079\7080\7081\7082\7083\7084\7085\7086\7087\7088\7089\7090\7091\7092\7093\7094\7095\7096\7097\7098\7099\7100\7101\7102\7103\7104\7105\7106\7107\7108\7109\7110\7111\7112\7113\7114\7115\7116\7117\7118\7119\7120\7121\7122\7123\7124\7125\7126\7127\7128\7129\7130\7131\7132\7133\7134\7135\7136\7137\7138\7139\7140\7141\7142\7143\7144\7145\7146\7147\7148\7149\7150\7151\7152\7153\7154\7155\7156\7157\7158\7159\7160\7161\7162\7163\7164\7165\7166\7167\7168\7169\7170\7171\7172\7173\7174\7175\7176\7177\7178\7179\7180\7181\7182\7183\7184\7185\7186\7187\7188\7189\7190\7191\7192\7193\7194\7195\7196\7197\7198\7199\7200\7201\7202\7203\7204\7205\7206\7207\7208\7209\7210\7211\7212\7213\7214\7215\7216\7217\7218\7219\7220\7221\7222\7223\7224\7225\7226\7227\7228\7229\7230\7231\7232\7233\7234\7235\7236\7237\7238\7239\7240\7241\7242\7243\7244\7245\7246\7247\7248\7249\7250\7251\7252\7253\7254\7255\7256\7257\7258\7259\7260\7261\7262\7263\7264\7265\7266\7267\7268\7269\7270\7271\7272\7273\7274\7275\7276\7277\7278\7279\7280\7281\7282\7283\7284\7285\7286\7287\7288\7289\7290\7291\7292\7293\7294\7295\7296\7297\7298\7299\7300\7301\7302\7303\7304\7305\7306\7307\7308\7309\7310\7311\7312\7313\7314\7315\7316\7317\7318\7319\7320\7321\7322\7323\7324\7325\7326\7327\7328\7329\7330\7331\7332\7333\7334\7335\7336\7337\7338\7339\7340\7341\7342\7343\7344\7345\7346\7347\7348\7349\7350\7351\7352\7353\7354\7355\7356\7357\7358\7359\7360\7361\7362\7363\7364\7365\7366\7367\7368\7369\7370\7371\7372\7373\7374\7375\7376\7377\7378\7379\7380\7381\7382\7383\7384\7385\7386\7387\7388\7389\7390\7391\7392\7393\7394\7395\7396\7397\7398\7399\7400\7401\7402\7403\7404\7405\7406\7407\7408\7409\7410\7411\7412\7413\7414\7415\7416\7417\7418\7419\7420\7421\7422\7423\7424\7425\7426\7427\7428\7429\7430\7431\7432\7433\7434\7435\7436\7437\7438\7439\7440\7441\7442\7443\7444\7445\7446\7447\7448\7449\7450\7451\7452\7453\7454\7455\7456\7457\7458\7459\7460\7461\7462\7463\7464\7465\7466\7467\7468\7469\7470\7471\7472\7473\7474\7475\7476\7477\7478\7479\7480\7481\7482\7483\7484\7485\7486\7487\7488\7489\7490\7491\7492\7493\7494\7495\7496\7497\7498\7499\7500\7501\7502\7503\7504\7505\7506\7507\7508\7509\7510\7511\7512\7513\7514\7515\7516\7517\7518\7519\7520\7521\7522\7523\7524\7525\7526\7527\7528\7529\7530\7531\7532\7533\7534\7535\7536\7537\7538\7539\7540\7541\7542\7543\7544\7545\7546\7547\7548\7549\7550\7551\7552\7553\7554\7555\7556\7557\7558\7559\7560\7561\7562\7563\7564\7565\7566\7567\7568\7569\7570\7571\7572\7573\7574\7575\7576\7577\7578\7579\7580\7581\7582\7583\7584\7585\7586\7587\7588\7589\7590\7591\7592\7593\7594\7595\7596\7597\7598\7599\7600\7601\7602\7603\7604\7605\7606\7607\7608\7609\7610\7611\7612\7613\7614\7615\7616\7617\7618\7619\7620\7621\7622\7623\7624\7625\7626\7627\7628\7629\7630\7631\7632\7633\7634\7635\7636\7637\7638\7639\7640\7641\7642\7643\7644\7645\7646\7647\7648\7649\7650\7651\7652\7653\7654\7655\7656\7657\7658\7659\7660\7661\7662\7663\7664\7665\7666\7667\7668\7669\7670\7671\7672\7673\7674\7675\7676\7677\7678\7679\7680\7681\7682\7683\7684\7685\7686\7687\7688\7689\7690\7691\7692\7693\7694\7695\7696\7697\7698\7699\7700\7701\7702\7703\7704\7705\7706\7707\7708\7709\7710\7711\7712\7713\7714\7715\7716\7717\7718\7719\7720\7721\7722\7723\7724\7725\7726\7727\7728\7729\7730\7731\7732\7733\7734\7735\7736\7737\7738\7739\7740\7741\7742\7743\7744\7745\7746\7747\7748\7749\7750\7751\7752\7753\7754\7755\7756\7757\7758\7759\7760\7761\7762\7763\7764\7765\7766\7767\7768\7769\7770\7771\7772\7773\7774\7775\7776\7777\7778\7779\7780\7781\7782\7783\7784\7785\7786\7787\7788\7789\7790\7791\7792\7793\7794\7795\7796\7797\7798\7799\7800\7801\7802\7803\7804\7805\7806\7807\7808\7809\7810\7811\7812\7813\7814\7815\7816\7817\7818\7819\7820\7821\7822\7823\7824\7825\7826\7827\7828\7829\7830\7831\7832\7833\7834\7835\7836\7837\7838\7839\7840\7841\7842\7843\7844\7845\7846\7847\7848\7849\7850\7851\7852\7853\7854\7855\7856\7857\7858\7859\7860\7861\7862\7863\7864\7865\7866\7867\7868\7869\7870\7871\7872\7873\7874\7875\7876\7877\7878\7879\7880\7881\7882\7883\7884\7885\7886\7887\7888\7889\7890\7891\7892\7893\7894\7895\7896\7897\7898\7899\7900\7901\7902\7903\7904\7905\7906\7907\7908\7909\7910\7911\7912\7913\7914\7915\7916\7917\7918\7919\7920\7921\7922\7923\7924\7925\7926\7927\7928\7929\7930\7931\7932\7933\7934\7935\7936\7937\7938\7939\7940\7941\7942\7943\7944\7945\7946\7947\7948\7949\7950\7951\7952\7953\7954\7955\7956\7957\7958\7959\7960\7961\7962\7963\7964\7965\7966\7967\7968\7969\7970\7971\7972\7973\7974\7975\7976\7977\7978\7979\7980\7981\7982\7983\7984\7985\7986\7987\7988\7989\7990\7991\7992\7993\7994\7995\7996\7997\7998\7999\8000\8001\8002\8003\8004\8005\8006\8007\8008\8009\8010\8011\8012\8013\8014\8015\8016\8017\8018\8019\8020\8021\8022\8023\8024\8025\8026\8027\8028\8029\8030\8031\8032\8033\8034\8035\8036\8037\8038\8039\8040\8041\8042\8043\8044\8045\8046\8047\8048\8049\8050\8051\8052\8053\8054\8055\8056\8057\8058\8059\8060\8061\8062\8063\8064\8065\8066\8067\8068\8069\8070\8071\8072\8073\8074\8075\8076\8077\8078\8079\8080\8081\8082\8083\8084\8085\8086\8087\8088\8089\8090\8091\8092\8093\8094\8095\8096\8097\8098\8099\8100\8101\8102\8103\8104\8105\8106\8107\8108\8109\8110\8111\8112\8113\8114\8115\8116\8117\8118\8119\8120\8121\8122\8123\8124\8125\8126\8127\8128\8129\8130\8131\8132\8133\8134\8135\8136\8137\8138\8139\8140\8141\8142\8143\8144\8145\8146\8147\8148\8149\8150\8151\8152\8153\8154\8155\8156\8157\8158\8159\8160\8161\8162\8163\8164\8165\8166\8167\8168\8169\8170\8171\8172\8173\8174\8175\8176\8177\8178\8179\8180\8181\8182\8183\8184\8185\8186\8187\8188\8189\8190\8191\8192\8193\8194\8195\8196\8197\8198\8199\8200\8201\8202\8203\8204\8205\8206\8207\8208\8209\8210\8211\8212\8213\8214\8215\8216\8217\8218\8219\8220\8221\8222\8223\8224\8225\8226\8227\8228\8229\8230\8231\8232\8233\8234\8235\8236\8237\8238\8239\8240\8241\8242\8243\8244\8245\8246\8247\8248\8249\8250\8251\8252\8253\8254\8255\8256\8257\8258\8259\8260\8261\8262\8263\8264\8265\8266\8267\8268\8269\8270\8271\8272\8273\8274\8275\8276\8277\8278\8279\8280\8281\8282\8283\8284\8285\8286\8287\8288\8289\8290\8291\8292\8293\8294\8295\8296\8297\8298\8299\8300\8301\8302\8303\8304\8305\8306\8307\8308\8309\8310\8311\8312\8313\8314\8315\8316\8317\8318\8319\8320\8321\8322\8323\8324\8325\8326\8327\8328\8329\8330\8331\8332\8333\8334\8335\8336\8337\8338\8339\8340\8341\8342\8343\8344\8345\8346\8347\8348\8349\8350\8351\8352\8353\8354\8355\8356\8357\8358\8359\8360\8361\8362\8363\8364\8365\8366\8367\8368\8369\8370\8371\8372\8373\8374\8375\8376\8377\8378\8379\8380\8381\8382\8383\8384\8385\8386\8387\8388\8389\8390\8391\8392\8393\8394\8395\8396\8397\8398\8399\8400\8401\8402\8403\8404\8405\8406\8407\8408\8409\8410\8411\8412\8413\8414\8415\8416\8417\8418\8419\8420\8421\8422\8423\8424\8425\8426\8427\8428\8429\8430\8431\8432\8433\8434\8435\8436\8437\8438\8439\8440\8441\8442\8443\8444\8445\8446\8447\8448\8449\8450\8451\8452\8453\8454\8455\8456\8457\8458\8459\8460\8461\8462\8463\8464\8465\8466\8467\8468\8469\8470\8471\8472\8473\8474\8475\8476\8477\8478\8479\8480\8481\8482\8483\8484\8485\8486\8487\8488\8489\8490\8491\8492\8493\8494\8495\8496\8497\8498\8499\8500\8501\8502\8503\8504\8505\8506\8507\8508\8509\8510\8511\8512\8513\8514\8515\8516\8517\8518\8519\8520\8521\8522\8523\8524\8525\8526\8527\8528\8529\8530\8531\8532\8533\8534\8535\8536\8537\8538\8539\8540\8541\8542\8543\8544\8545\8546\8547\8548\8549\8550\8551\8552\8553\8554\8555\8556\8557\8558\8559\8560\8561\8562\8563\8564\8565\8566\8567\8568\8569\8570\8571\8572\8573\8574\8575\8576\8577\8578\8579\8580\8581\8582\8583\8584\8585\8586\8587\8588\8589\8590\8591\8592\8593\8594\8595\8596\8597\8598\8599\8600\8601\8602\8603\8604\8605\8606\8607\8608\8609\8610\8611\8612\8613\8614\8615\8616\8617\8618\8619\8620\8621\8622\8623\8624\8625\8626\8627\8628\8629\8630\8631\8632\8633\8634\8635\8636\8637\8638\8639\8640\8641\8642\8643\8644\8645\8646\8647\8648\8649\8650\8651\8652\8653\8654\8655\8656\8657\8658\8659\8660\8661\8662\8663\8664\8665\8666\8667\8668\8669\8670\8671\8672\8673\8674\8675\8676\8677\8678\8679\8680\8681\8682\8683\8684\8685\8686\8687\8688\8689\8690\8691\8692\8693\8694\8695\8696\8697\8698\8699\8700\8701\8702\8703\8704\8705\8706\8707\8708\8709\8710\8711\8712\8713\8714\8715\8716\8717\8718\8719\8720\8721\8722\8723\8724\8725\8726\8727\8728\8729\8730\8731\8732\8733\8734\8735\8736\8737\8738\8739\8740\8741\8742\8743\8744\8745\8746\8747\8748\8749\8750\8751\8752\8753\8754\8755\8756\8757\8758\8759\8760\8761\8762\8763\8764\8765\8766\8767\8768\8769\8770\8771\8772\8773\8774\8775\8776\8777\8778\8779\8780\8781\8782\8783\8784\8785\8786\8787\8788\8789\8790\8791\8792\8793\8794\8795\8796\8797\8798\8799\8800\8801\8802\8803\8804\8805\8806\8807\8808\8809\8810\8811\8812\8813\8814\8815\8816\8817\8818\8819\8820\8821\8822\8823\8824\8825\8826\8827\8828\8829\8830\8831\8832\8833\8834\8835\8836\8837\8838\8839\8840\8841\8842\8843\8844\8845\8846\8847\8848\8849\8850\8851\8852\8853\8854\8855\8856\8857\8858\8859\8860\8861\8862\8863\8864\8865\8866\8867\8868\8869\8870\8871\8872\8873\8874\8875\8876\8877\8878\8879\8880\8881\8882\8883\8884\8885\8886\8887\8888\8889\8890\8891\8892\8893\8894\8895\8896\8897\8898\8899\8900\8901\8902\8903\8904\8905\8906\8907\8908\8909\8910\8911\8912\8913\8914\8915\8916\8917\8918\8919\8920\8921\8922\8923\8924\8925\8926\8927\8928\8929\8930\8931\8932\8933\8934\8935\8936\8937\8938\8939\8940\8941\8942\8943\8944\8945\8946\8947\8948\8949\8950\8951\8952\8953\8954\8955\8956\8957\8958\8959\8960\8961\8962\8963\8964\8965\8966\8967\8968\8969\8970\8971\8972\8973\8974\8975\8976\8977\8978\8979\8980\8981\8982\8983\8984\8985\8986\8987\8988\8989\8990\8991\8992\8993\8994\8995\8996\8997\8998\8999\9000\9001\9002\9003\9004\9005\9006\9007\9008\9009\9010\9011\9012\9013\9014\9015\9016\9017\9018\9019\9020\9021\9022\9023\9024\9025\9026\9027\9028\9029\9030\9031\9032\9033\9034\9035\9036\9037\9038\9039\9040\9041\9042\9043\9044\9045\9046\9047\9048\9049\9050\9051\9052\9053\9054\9055\9056\9057\9058\9059\9060\9061\9062\9063\9064\9065\9066\9067\9068\9069\9070\9071\9072\9073\9074\9075\9076\9077\9078\9079\9080\9081\9082\9083\9084\9085\9086\9087\9088\9089\9090\9091\9092\9093\9094\9095\9096\9097\9098\9099\9100\9101\9102\9103\9104\9105\9106\9107\9108\9109\9110\9111\9112\9113\9114\9115\9116\9117\9118\9119\9120\9121\9122\9123\9124\9125\9126\9127\9128\9129\9130\9131\9132\9133\9134\9135\9136\9137\9138\9139\9140\9141\9142\9143\9144\9145\9146\9147\9148\9149\9150\9151\9152\9153\9154\9155\9156\9157\9158\9159\9160\9161\9162\9163\9164\9165\9166\9167\9168\9169\9170\9171\9172\9173\9174\9175\9176\9177\9178\9179\9180\9181\9182\9183\9184\9185\9186\9187\9188\9189\9190\9191\9192\9193\9194\9195\9196\9197\9198\9199\9200\9201\9202\9203\9204\9205\9206\9207\9208\9209\9210\9211\9212\9213\9214\9215\9216\9217\9218\9219\9220\9221\9222\9223\9224\9225\9226\9227\9228\9229\9230\9231\9232\9233\9234\9235\9236\9237\9238\9239\9240\9241\9242\9243\9244\9245\9246\9247\9248\9249\9250\9251\9252\9253\9254\9255\9256\9257\9258\9259\9260\9261\9262\9263\9264\9265\9266\9267\9268\9269\9270\9271\9272\9273\9274\9275\9276\9277\9278\9279\9280\9281\9282\9283\9284\9285\9286\9287\9288\9289\9290\9291\9292\9293\9294\9295\9296\9297\9298\9299\9300\9301\9302\9303\9304\9305\9306\9307\9308\9309\9310\9311\9312\9313\9314\9315\9316\9317\9318\9319\9320\9321\9322\9323\9324\9325\9326\9327\9328\9329\9330\9331\9332\9333\9334\9335\9336\9337\9338\9339\9340\9341\9342\9343\9344\9345\9346\9347\9348\9349\9350\9351\9352\9353\9354\9355\9356\9357\9358\9359\9360\9361\9362\9363\9364\9365\9366\9367\9368\9369\9370\9371\9372\9373\9374\9375\9376\9377\9378\9379\9380\9381\9382\9383\9384\9385\9386\9387\9388\9389\9390\9391\9392\9393\9394\9395\9396\9397\9398\9399\9400\9401\9402\9403\9404\9405\9406\9407\9408\9409\9410\9411\9412\9413\9414\9415\9416\9417\9418\9419\9420\9421\9422\9423\9424\9425\9426\9427\9428\9429\9430\9431\9432\9433\9434\9435\9436\9437\9438\9439\9440\9441\9442\9443\9444\9445\9446\9447\9448\9449\9450\9451\9452\9453\9454\9455\9456\9457\9458\9459\9460\9461\9462\9463\9464\9465\9466\9467\9468\9469\9470\9471\9472\9473\9474\9475\9476\9477\9478\9479\9480\9481\9482\9483\9484\9485\9486\9487\9488\9489\9490\9491\9492\9493\9494\9495\9496\9497\9498\9499\9500\9501\9502\9503\9504\9505\9506\9507\9508\9509\9510\9511\9512\9513\9514\9515\9516\9517\9518\9519\9520\9521\9522\9523\9524\9525\9526\9527\9528\9529\9530\9531\9532\9533\9534\9535\9536\9537\9538\9539\9540\9541\9542\9543\9544\9545\9546\9547\9548\9549\9550\9551\9552\9553\9554\9555\9556\9557\9558\9559\9560\9561\9562\9563\9564\9565\9566\9567\9568\9569\9570\9571\9572\9573\9574\9575\9576\9577\9578\9579\9580\9581\9582\9583\9584\9585\9586\9587\9588\9589\9590\9591\9592\9593\9594\9595\9596\9597\9598\9599\9600\9601\9602\9603\9604\9605\9606\9607\9608\9609\9610\9611\9612\9613\9614\9615\9616\9617\9618\9619\9620\9621\9622\9623\9624\9625\9626\9627\9628\9629\9630\9631\9632\9633\9634\9635\9636\9637\9638\9639\9640\9641\9642\9643\9644\9645\9646\9647\9648\9649\9650\9651\9652\9653\9654\9655\9656\9657\9658\9659\9660\9661\9662\9663\9664\9665\9666\9667\9668\9669\9670\9671\9672\9673\9674\9675\9676\9677\9678\9679\9680\9681\9682\9683\9684\9685\9686\9687\9688\9689\9690\9691\9692\9693\9694\9695\9696\9697\9698\9699\9700\9701\9702\9703\9704\9705\9706\9707\9708\9709\9710\9711\9712\9713\9714\9715\9716\9717\9718\9719\9720\9721\9722\9723\9724\9725\9726\9727\9728\9729\9730\9731\9732\9733\9734\9735\9736\9737\9738\9739\9740\9741\9742\9743\9744\9745\9746\9747\9748\9749\9750\9751\9752\9753\9754\9755\9756\9757\9758\9759\9760\9761\9762\9763\9764\9765\9766\9767\9768\9769\9770\9771\9772\9773\9774\9775\9776\9777\9778\9779\9780\9781\9782\9783\9784\9785\9786\9787\9788\9789\9790\9791\9792\9793\9794\9795\9796\9797\9798\9799\9800\9801\9802\9803\9804\9805\9806\9807\9808\9809\9810\9811\9812\9813\9814\9815\9816\9817\9818\9819\9820\9821\9822\9823\9824\9825\9826\9827\9828\9829\9830\9831\9832\9833\9834\9835\9836\9837\9838\9839\9840\9841\9842\9843\9844\9845\9846\9847\9848\9849\9850\9851\9852\9853\9854\9855\9856\9857\9858\9859\9860\9861\9862\9863\9864\9865\9866\9867\9868\9869\9870\9871\9872\9873\9874\9875\9876\9877\9878\9879\9880\9881\9882\9883\9884\9885\9886\9887\9888\9889\9890\9891\9892\9893\9894\9895\9896\9897\9898\9899\9900\9901\9902\9903\9904\9905\9906\9907\9908\9909\9910\9911\9912\9913\9914\9915\9916\9917\9918\9919\9920\9921\9922\9923\9924\9925\9926\9927\9928\9929\9930\9931\9932\9933\9934\9935\9936\9937\9938\9939\9940\9941\9942\9943\9944\9945\9946\9947\9948\9949\9950\9951\9952\9953\9954\9955\9956\9957\9958\9959\9960\9961\9962\9963\9964\9965\9966\9967\9968\9969\9970\9971\9972\9973\9974\9975\9976\9977\9978\9979\9980\9981\9982\9983\9984\9985\9986\9987\9988\9989\9990\9991\9992\9993\9994\9995\9996\9997\9998\9999\10000</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661\662\663\664\665\666\667\668\669\670\671\672\673\674\675\676\677\678\679\680\681\682\683\684\685\686\687\688\689\690\691\692\693\694\695\696\697\698\699\700\701\702\703\704\705\706\707\708\709\710\711\712\713\714\715\716\717\718\719\720\721\722\723\724\725\726\727\728\729\730\731\732\733\734\735\736\737\738\739\740\741\742\743\744\745\746\747\748\749\750\751\752\753\754\755\756\757\758\759\760\761\762\763\764\765\766\767\768\769\770\771\772\773\774\775\776\777\778\779\780\781\782\783\784\785\786\787\788\789\790\791\792\793\794\795\796\797\798\799\800\801\802\803\804\805\806\807\808\809\810\811\812\813\814\815\816\817\818\819\820\821\822\823\824\825\826\827\828\829\830\831\832\833\834\835\836\837\838\839\840\841\842\843\844\845\846\847\848\849\850\851\852\853\854\855\856\857\858\859\860\861\862\863\864\865\866\867\868\869\870\871\872\873\874\875\876\877\878\879\880\881\882\883\884\885\886\887\888\889\890\891\892\893\894\895\896\897\898\899\900\901\902\903\904\905\906\907\908\909\910\911\912\913\914\915\916\917\918\919\920\921\922\923\924\925\926\927\928\929\930\931\932\933\934\935\936\937\938\939\940\941\942\943\944\945\946\947\948\949\950\951\952\953\954\955\956\957\958\959\960\961\962\963\964\965\966\967\968\969\970\971\972\973\974\975\976\977\978\979\980\981\982\983\984\985\986\987\988\989\990\991\992\993\994\995\996\997\998\999\1000\1001\1002\1003\1004\1005\1006\1007\1008\1009\1010\1011\1012\1013\1014\1015\1016\1017\1018\1019\1020\1021\1022\1023\1024\1025\1026\1027\1028\1029\1030\1031\1032\1033\1034\1035\1036\1037\1038\1039\1040\1041\1042\1043\1044\1045\1046\1047\1048\1049\1050\1051\1052\1053\1054\1055\1056\1057\1058\1059\1060\1061\1062\1063\1064\1065\1066\1067\1068\1069\1070\1071\1072\1073\1074\1075\1076\1077\1078\1079\1080\1081\1082\1083\1084\1085\1086\1087\1088\1089\1090\1091\1092\1093\1094\1095\1096\1097\1098\1099\1100\1101\1102\1103\1104\1105\1106\1107\1108\1109\1110\1111\1112\1113\1114\1115\1116\1117\1118\1119\1120\1121\1122\1123\1124\1125\1126\1127\1128\1129\1130\1131\1132\1133\1134\1135\1136\1137\1138\1139\1140\1141\1142\1143\1144\1145\1146\1147\1148\1149\1150\1151\1152\1153\1154\1155\1156\1157\1158\1159\1160\1161\1162\1163\1164\1165\1166\1167\1168\1169\1170\1171\1172\1173\1174\1175\1176\1177\1178\1179\1180\1181\1182\1183\1184\1185\1186\1187\1188\1189\1190\1191\1192\1193\1194\1195\1196\1197\1198\1199\1200\1201\1202\1203\1204\1205\1206\1207\1208\1209\1210\1211\1212\1213\1214\1215\1216\1217\1218\1219\1220\1221\1222\1223\1224\1225\1226\1227\1228\1229\1230\1231\1232\1233\1234\1235\1236\1237\1238\1239\1240\1241\1242\1243\1244\1245\1246\1247\1248\1249\1250\1251\1252\1253\1254\1255\1256\1257\1258\1259\1260\1261\1262\1263\1264\1265\1266\1267\1268\1269\1270\1271\1272\1273\1274\1275\1276\1277\1278\1279\1280\1281\1282\1283\1284\1285\1286\1287\1288\1289\1290\1291\1292\1293\1294\1295\1296\1297\1298\1299\1300\1301\1302\1303\1304\1305\1306\1307\1308\1309\1310\1311\1312\1313\1314\1315\1316\1317\1318\1319\1320\1321\1322\1323\1324\1325\1326\1327\1328\1329\1330\1331\1332\1333\1334\1335\1336\1337\1338\1339\1340\1341\1342\1343\1344\1345\1346\1347\1348\1349\1350\1351\1352\1353\1354\1355\1356\1357\1358\1359\1360\1361\1362\1363\1364\1365\1366\1367\1368\1369\1370\1371\1372\1373\1374\1375\1376\1377\1378\1379\1380\1381\1382\1383\1384\1385\1386\1387\1388\1389\1390\1391\1392\1393\1394\1395\1396\1397\1398\1399\1400\1401\1402\1403\1404\1405\1406\1407\1408\1409\1410\1411\1412\1413\1414\1415\1416\1417\1418\1419\1420\1421\1422\1423\1424\1425\1426\1427\1428\1429\1430\1431\1432\1433\1434\1435\1436\1437\1438\1439\1440\1441\1442\1443\1444\1445\1446\1447\1448\1449\1450\1451\1452\1453\1454\1455\1456\1457\1458\1459\1460\1461\1462\1463\1464\1465\1466\1467\1468\1469\1470\1471\1472\1473\1474\1475\1476\1477\1478\1479\1480\1481\1482\1483\1484\1485\1486\1487\1488\1489\1490\1491\1492\1493\1494\1495\1496\1497\1498\1499\1500\1501\1502\1503\1504\1505\1506\1507\1508\1509\1510\1511\1512\1513\1514\1515\1516\1517\1518\1519\1520\1521\1522\1523\1524\1525\1526\1527\1528\1529\1530\1531\1532\1533\1534\1535\1536\1537\1538\1539\1540\1541\1542\1543\1544\1545\1546\1547\1548\1549\1550\1551\1552\1553\1554\1555\1556\1557\1558\1559\1560\1561\1562\1563\1564\1565\1566\1567\1568\1569\1570\1571\1572\1573\1574\1575\1576\1577\1578\1579\1580\1581\1582\1583\1584\1585\1586\1587\1588\1589\1590\1591\1592\1593\1594\1595\1596\1597\1598\1599\1600\1601\1602\1603\1604\1605\1606\1607\1608\1609\1610\1611\1612\1613\1614\1615\1616\1617\1618\1619\1620\1621\1622\1623\1624\1625\1626\1627\1628\1629\1630\1631\1632\1633\1634\1635\1636\1637\1638\1639\1640\1641\1642\1643\1644\1645\1646\1647\1648\1649\1650\1651\1652\1653\1654\1655\1656\1657\1658\1659\1660\1661\1662\1663\1664\1665\1666\1667\1668\1669\1670\1671\1672\1673\1674\1675\1676\1677\1678\1679\1680\1681\1682\1683\1684\1685\1686\1687\1688\1689\1690\1691\1692\1693\1694\1695\1696\1697\1698\1699\1700\1701\1702\1703\1704\1705\1706\1707\1708\1709\1710\1711\1712\1713\1714\1715\1716\1717\1718\1719\1720\1721\1722\1723\1724\1725\1726\1727\1728\1729\1730\1731\1732\1733\1734\1735\1736\1737\1738\1739\1740\1741\1742\1743\1744\1745\1746\1747\1748\1749\1750\1751\1752\1753\1754\1755\1756\1757\1758\1759\1760\1761\1762\1763\1764\1765\1766\1767\1768\1769\1770\1771\1772\1773\1774\1775\1776\1777\1778\1779\1780\1781\1782\1783\1784\1785\1786\1787\1788\1789\1790\1791\1792\1793\1794\1795\1796\1797\1798\1799\1800\1801\1802\1803\1804\1805\1806\1807\1808\1809\1810\1811\1812\1813\1814\1815\1816\1817\1818\1819\1820\1821\1822\1823\1824\1825\1826\1827\1828\1829\1830\1831\1832\1833\1834\1835\1836\1837\1838\1839\1840\1841\1842\1843\1844\1845\1846\1847\1848\1849\1850\1851\1852\1853\1854\1855\1856\1857\1858\1859\1860\1861\1862\1863\1864\1865\1866\1867\1868\1869\1870\1871\1872\1873\1874\1875\1876\1877\1878\1879\1880\1881\1882\1883\1884\1885\1886\1887\1888\1889\1890\1891\1892\1893\1894\1895\1896\1897\1898\1899\1900\1901\1902\1903\1904\1905\1906\1907\1908\1909\1910\1911\1912\1913\1914\1915\1916\1917\1918\1919\1920\1921\1922\1923\1924\1925\1926\1927\1928\1929\1930\1931\1932\1933\1934\1935\1936\1937\1938\1939\1940\1941\1942\1943\1944\1945\1946\1947\1948\1949\1950\1951\1952\1953\1954\1955\1956\1957\1958\1959\1960\1961\1962\1963\1964\1965\1966\1967\1968\1969\1970\1971\1972\1973\1974\1975\1976\1977\1978\1979\1980\1981\1982\1983\1984\1985\1986\1987\1988\1989\1990\1991\1992\1993\1994\1995\1996\1997\1998\1999\2000\2001\2002\2003\2004\2005\2006\2007\2008\2009\2010\2011\2012\2013\2014\2015\2016\2017\2018\2019\2020\2021\2022\2023\2024\2025\2026\2027\2028\2029\2030\2031\2032\2033\2034\2035\2036\2037\2038\2039\2040\2041\2042\2043\2044\2045\2046\2047\2048\2049\2050\2051\2052\2053\2054\2055\2056\2057\2058\2059\2060\2061\2062\2063\2064\2065\2066\2067\2068\2069\2070\2071\2072\2073\2074\2075\2076\2077\2078\2079\2080\2081\2082\2083\2084\2085\2086\2087\2088\2089\2090\2091\2092\2093\2094\2095\2096\2097\2098\2099\2100\2101\2102\2103\2104\2105\2106\2107\2108\2109\2110\2111\2112\2113\2114\2115\2116\2117\2118\2119\2120\2121\2122\2123\2124\2125\2126\2127\2128\2129\2130\2131\2132\2133\2134\2135\2136\2137\2138\2139\2140\2141\2142\2143\2144\2145\2146\2147\2148\2149\2150\2151\2152\2153\2154\2155\2156\2157\2158\2159\2160\2161\2162\2163\2164\2165\2166\2167\2168\2169\2170\2171\2172\2173\2174\2175\2176\2177\2178\2179\2180\2181\2182\2183\2184\2185\2186\2187\2188\2189\2190\2191\2192\2193\2194\2195\2196\2197\2198\2199\2200\2201\2202\2203\2204\2205\2206\2207\2208\2209\2210\2211\2212\2213\2214\2215\2216\2217\2218\2219\2220\2221\2222\2223\2224\2225\2226\2227\2228\2229\2230\2231\2232\2233\2234\2235\2236\2237\2238\2239\2240\2241\2242\2243\2244\2245\2246\2247\2248\2249\2250\2251\2252\2253\2254\2255\2256\2257\2258\2259\2260\2261\2262\2263\2264\2265\2266\2267\2268\2269\2270\2271\2272\2273\2274\2275\2276\2277\2278\2279\2280\2281\2282\2283\2284\2285\2286\2287\2288\2289\2290\2291\2292\2293\2294\2295\2296\2297\2298\2299\2300\2301\2302\2303\2304\2305\2306\2307\2308\2309\2310\2311\2312\2313\2314\2315\2316\2317\2318\2319\2320\2321\2322\2323\2324\2325\2326\2327\2328\2329\2330\2331\2332\2333\2334\2335\2336\2337\2338\2339\2340\2341\2342\2343\2344\2345\2346\2347\2348\2349\2350\2351\2352\2353\2354\2355\2356\2357\2358\2359\2360\2361\2362\2363\2364\2365\2366\2367\2368\2369\2370\2371\2372\2373\2374\2375\2376\2377\2378\2379\2380\2381\2382\2383\2384\2385\2386\2387\2388\2389\2390\2391\2392\2393\2394\2395\2396\2397\2398\2399\2400\2401\2402\2403\2404\2405\2406\2407\2408\2409\2410\2411\2412\2413\2414\2415\2416\2417\2418\2419\2420\2421\2422\2423\2424\2425\2426\2427\2428\2429\2430\2431\2432\2433\2434\2435\2436\2437\2438\2439\2440\2441\2442\2443\2444\2445\2446\2447\2448\2449\2450\2451\2452\2453\2454\2455\2456\2457\2458\2459\2460\2461\2462\2463\2464\2465\2466\2467\2468\2469\2470\2471\2472\2473\2474\2475\2476\2477\2478\2479\2480\2481\2482\2483\2484\2485\2486\2487\2488\2489\2490\2491\2492\2493\2494\2495\2496\2497\2498\2499\2500\2501\2502\2503\2504\2505\2506\2507\2508\2509\2510\2511\2512\2513\2514\2515\2516\2517\2518\2519\2520\2521\2522\2523\2524\2525\2526\2527\2528\2529\2530\2531\2532\2533\2534\2535\2536\2537\2538\2539\2540\2541\2542\2543\2544\2545\2546\2547\2548\2549\2550\2551\2552\2553\2554\2555\2556\2557\2558\2559\2560\2561\2562\2563\2564\2565\2566\2567\2568\2569\2570\2571\2572\2573\2574\2575\2576\2577\2578\2579\2580\2581\2582\2583\2584\2585\2586\2587\2588\2589\2590\2591\2592\2593\2594\2595\2596\2597\2598\2599\2600\2601\2602\2603\2604\2605\2606\2607\2608\2609\2610\2611\2612\2613\2614\2615\2616\2617\2618\2619\2620\2621\2622\2623\2624\2625\2626\2627\2628\2629\2630\2631\2632\2633\2634\2635\2636\2637\2638\2639\2640\2641\2642\2643\2644\2645\2646\2647\2648\2649\2650\2651\2652\2653\2654\2655\2656\2657\2658\2659\2660\2661\2662\2663\2664\2665\2666\2667\2668\2669\2670\2671\2672\2673\2674\2675\2676\2677\2678\2679\2680\2681\2682\2683\2684\2685\2686\2687\2688\2689\2690\2691\2692\2693\2694\2695\2696\2697\2698\2699\2700\2701\2702\2703\2704\2705\2706\2707\2708\2709\2710\2711\2712\2713\2714\2715\2716\2717\2718\2719\2720\2721\2722\2723\2724\2725\2726\2727\2728\2729\2730\2731\2732\2733\2734\2735\2736\2737\2738\2739\2740\2741\2742\2743\2744\2745\2746\2747\2748\2749\2750\2751\2752\2753\2754\2755\2756\2757\2758\2759\2760\2761\2762\2763\2764\2765\2766\2767\2768\2769\2770\2771\2772\2773\2774\2775\2776\2777\2778\2779\2780\2781\2782\2783\2784\2785\2786\2787\2788\2789\2790\2791\2792\2793\2794\2795\2796\2797\2798\2799\2800\2801\2802\2803\2804\2805\2806\2807\2808\2809\2810\2811\2812\2813\2814\2815\2816\2817\2818\2819\2820\2821\2822\2823\2824\2825\2826\2827\2828\2829\2830\2831\2832\2833\2834\2835\2836\2837\2838\2839\2840\2841\2842\2843\2844\2845\2846\2847\2848\2849\2850\2851\2852\2853\2854\2855\2856\2857\2858\2859\2860\2861\2862\2863\2864\2865\2866\2867\2868\2869\2870\2871\2872\2873\2874\2875\2876\2877\2878\2879\2880\2881\2882\2883\2884\2885\2886\2887\2888\2889\2890\2891\2892\2893\2894\2895\2896\2897\2898\2899\2900\2901\2902\2903\2904\2905\2906\2907\2908\2909\2910\2911\2912\2913\2914\2915\2916\2917\2918\2919\2920\2921\2922\2923\2924\2925\2926\2927\2928\2929\2930\2931\2932\2933\2934\2935\2936\2937\2938\2939\2940\2941\2942\2943\2944\2945\2946\2947\2948\2949\2950\2951\2952\2953\2954\2955\2956\2957\2958\2959\2960\2961\2962\2963\2964\2965\2966\2967\2968\2969\2970\2971\2972\2973\2974\2975\2976\2977\2978\2979\2980\2981\2982\2983\2984\2985\2986\2987\2988\2989\2990\2991\2992\2993\2994\2995\2996\2997\2998\2999\3000\3001\3002\3003\3004\3005\3006\3007\3008\3009\3010\3011\3012\3013\3014\3015\3016\3017\3018\3019\3020\3021\3022\3023\3024\3025\3026\3027\3028\3029\3030\3031\3032\3033\3034\3035\3036\3037\3038\3039\3040\3041\3042\3043\3044\3045\3046\3047\3048\3049\3050\3051\3052\3053\3054\3055\3056\3057\3058\3059\3060\3061\3062\3063\3064\3065\3066\3067\3068\3069\3070\3071\3072\3073\3074\3075\3076\3077\3078\3079\3080\3081\3082\3083\3084\3085\3086\3087\3088\3089\3090\3091\3092\3093\3094\3095\3096\3097\3098\3099\3100\3101\3102\3103\3104\3105\3106\3107\3108\3109\3110\3111\3112\3113\3114\3115\3116\3117\3118\3119\3120\3121\3122\3123\3124\3125\3126\3127\3128\3129\3130\3131\3132\3133\3134\3135\3136\3137\3138\3139\3140\3141\3142\3143\3144\3145\3146\3147\3148\3149\3150\3151\3152\3153\3154\3155\3156\3157\3158\3159\3160\3161\3162\3163\3164\3165\3166\3167\3168\3169\3170\3171\3172\3173\3174\3175\3176\3177\3178\3179\3180\3181\3182\3183\3184\3185\3186\3187\3188\3189\3190\3191\3192\3193\3194\3195\3196\3197\3198\3199\3200\3201\3202\3203\3204\3205\3206\3207\3208\3209\3210\3211\3212\3213\3214\3215\3216\3217\3218\3219\3220\3221\3222\3223\3224\3225\3226\3227\3228\3229\3230\3231\3232\3233\3234\3235\3236\3237\3238\3239\3240\3241\3242\3243\3244\3245\3246\3247\3248\3249\3250\3251\3252\3253\3254\3255\3256\3257\3258\3259\3260\3261\3262\3263\3264\3265\3266\3267\3268\3269\3270\3271\3272\3273\3274\3275\3276\3277\3278\3279\3280\3281\3282\3283\3284\3285\3286\3287\3288\3289\3290\3291\3292\3293\3294\3295\3296\3297\3298\3299\3300\3301\3302\3303\3304\3305\3306\3307\3308\3309\3310\3311\3312\3313\3314\3315\3316\3317\3318\3319\3320\3321\3322\3323\3324\3325\3326\3327\3328\3329\3330\3331\3332\3333\3334\3335\3336\3337\3338\3339\3340\3341\3342\3343\3344\3345\3346\3347\3348\3349\3350\3351\3352\3353\3354\3355\3356\3357\3358\3359\3360\3361\3362\3363\3364\3365\3366\3367\3368\3369\3370\3371\3372\3373\3374\3375\3376\3377\3378\3379\3380\3381\3382\3383\3384\3385\3386\3387\3388\3389\3390\3391\3392\3393\3394\3395\3396\3397\3398\3399\3400\3401\3402\3403\3404\3405\3406\3407\3408\3409\3410\3411\3412\3413\3414\3415\3416\3417\3418\3419\3420\3421\3422\3423\3424\3425\3426\3427\3428\3429\3430\3431\3432\3433\3434\3435\3436\3437\3438\3439\3440\3441\3442\3443\3444\3445\3446\3447\3448\3449\3450\3451\3452\3453\3454\3455\3456\3457\3458\3459\3460\3461\3462\3463\3464\3465\3466\3467\3468\3469\3470\3471\3472\3473\3474\3475\3476\3477\3478\3479\3480\3481\3482\3483\3484\3485\3486\3487\3488\3489\3490\3491\3492\3493\3494\3495\3496\3497\3498\3499\3500\3501\3502\3503\3504\3505\3506\3507\3508\3509\3510\3511\3512\3513\3514\3515\3516\3517\3518\3519\3520\3521\3522\3523\3524\3525\3526\3527\3528\3529\3530\3531\3532\3533\3534\3535\3536\3537\3538\3539\3540\3541\3542\3543\3544\3545\3546\3547\3548\3549\3550\3551\3552\3553\3554\3555\3556\3557\3558\3559\3560\3561\3562\3563\3564\3565\3566\3567\3568\3569\3570\3571\3572\3573\3574\3575\3576\3577\3578\3579\3580\3581\3582\3583\3584\3585\3586\3587\3588\3589\3590\3591\3592\3593\3594\3595\3596\3597\3598\3599\3600\3601\3602\3603\3604\3605\3606\3607\3608\3609\3610\3611\3612\3613\3614\3615\3616\3617\3618\3619\3620\3621\3622\3623\3624\3625\3626\3627\3628\3629\3630\3631\3632\3633\3634\3635\3636\3637\3638\3639\3640\3641\3642\3643\3644\3645\3646\3647\3648\3649\3650\3651\3652\3653\3654\3655\3656\3657\3658\3659\3660\3661\3662\3663\3664\3665\3666\3667\3668\3669\3670\3671\3672\3673\3674\3675\3676\3677\3678\3679\3680\3681\3682\3683\3684\3685\3686\3687\3688\3689\3690\3691\3692\3693\3694\3695\3696\3697\3698\3699\3700\3701\3702\3703\3704\3705\3706\3707\3708\3709\3710\3711\3712\3713\3714\3715\3716\3717\3718\3719\3720\3721\3722\3723\3724\3725\3726\3727\3728\3729\3730\3731\3732\3733\3734\3735\3736\3737\3738\3739\3740\3741\3742\3743\3744\3745\3746\3747\3748\3749\3750\3751\3752\3753\3754\3755\3756\3757\3758\3759\3760\3761\3762\3763\3764\3765\3766\3767\3768\3769\3770\3771\3772\3773\3774\3775\3776\3777\3778\3779\3780\3781\3782\3783\3784\3785\3786\3787\3788\3789\3790\3791\3792\3793\3794\3795\3796\3797\3798\3799\3800\3801\3802\3803\3804\3805\3806\3807\3808\3809\3810\3811\3812\3813\3814\3815\3816\3817\3818\3819\3820\3821\3822\3823\3824\3825\3826\3827\3828\3829\3830\3831\3832\3833\3834\3835\3836\3837\3838\3839\3840\3841\3842\3843\3844\3845\3846\3847\3848\3849\3850\3851\3852\3853\3854\3855\3856\3857\3858\3859\3860\3861\3862\3863\3864\3865\3866\3867\3868\3869\3870\3871\3872\3873\3874\3875\3876\3877\3878\3879\3880\3881\3882\3883\3884\3885\3886\3887\3888\3889\3890\3891\3892\3893\3894\3895\3896\3897\3898\3899\3900\3901\3902\3903\3904\3905\3906\3907\3908\3909\3910\3911\3912\3913\3914\3915\3916\3917\3918\3919\3920\3921\3922\3923\3924\3925\3926\3927\3928\3929\3930\3931\3932\3933\3934\3935\3936\3937\3938\3939\3940\3941\3942\3943\3944\3945\3946\3947\3948\3949\3950\3951\3952\3953\3954\3955\3956\3957\3958\3959\3960\3961\3962\3963\3964\3965\3966\3967\3968\3969\3970\3971\3972\3973\3974\3975\3976\3977\3978\3979\3980\3981\3982\3983\3984\3985\3986\3987\3988\3989\3990\3991\3992\3993\3994\3995\3996\3997\3998\3999\4000\4001\4002\4003\4004\4005\4006\4007\4008\4009\4010\4011\4012\4013\4014\4015\4016\4017\4018\4019\4020\4021\4022\4023\4024\4025\4026\4027\4028\4029\4030\4031\4032\4033\4034\4035\4036\4037\4038\4039\4040\4041\4042\4043\4044\4045\4046\4047\4048\4049\4050\4051\4052\4053\4054\4055\4056\4057\4058\4059\4060\4061\4062\4063\4064\4065\4066\4067\4068\4069\4070\4071\4072\4073\4074\4075\4076\4077\4078\4079\4080\4081\4082\4083\4084\4085\4086\4087\4088\4089\4090\4091\4092\4093\4094\4095\4096\4097\4098\4099\4100\4101\4102\4103\4104\4105\4106\4107\4108\4109\4110\4111\4112\4113\4114\4115\4116\4117\4118\4119\4120\4121\4122\4123\4124\4125\4126\4127\4128\4129\4130\4131\4132\4133\4134\4135\4136\4137\4138\4139\4140\4141\4142\4143\4144\4145\4146\4147\4148\4149\4150\4151\4152\4153\4154\4155\4156\4157\4158\4159\4160\4161\4162\4163\4164\4165\4166\4167\4168\4169\4170\4171\4172\4173\4174\4175\4176\4177\4178\4179\4180\4181\4182\4183\4184\4185\4186\4187\4188\4189\4190\4191\4192\4193\4194\4195\4196\4197\4198\4199\4200\4201\4202\4203\4204\4205\4206\4207\4208\4209\4210\4211\4212\4213\4214\4215\4216\4217\4218\4219\4220\4221\4222\4223\4224\4225\4226\4227\4228\4229\4230\4231\4232\4233\4234\4235\4236\4237\4238\4239\4240\4241\4242\4243\4244\4245\4246\4247\4248\4249\4250\4251\4252\4253\4254\4255\4256\4257\4258\4259\4260\4261\4262\4263\4264\4265\4266\4267\4268\4269\4270\4271\4272\4273\4274\4275\4276\4277\4278\4279\4280\4281\4282\4283\4284\4285\4286\4287\4288\4289\4290\4291\4292\4293\4294\4295\4296\4297\4298\4299\4300\4301\4302\4303\4304\4305\4306\4307\4308\4309\4310\4311\4312\4313\4314\4315\4316\4317\4318\4319\4320\4321\4322\4323\4324\4325\4326\4327\4328\4329\4330\4331\4332\4333\4334\4335\4336\4337\4338\4339\4340\4341\4342\4343\4344\4345\4346\4347\4348\4349\4350\4351\4352\4353\4354\4355\4356\4357\4358\4359\4360\4361\4362\4363\4364\4365\4366\4367\4368\4369\4370\4371\4372\4373\4374\4375\4376\4377\4378\4379\4380\4381\4382\4383\4384\4385\4386\4387\4388\4389\4390\4391\4392\4393\4394\4395\4396\4397\4398\4399\4400\4401\4402\4403\4404\4405\4406\4407\4408\4409\4410\4411\4412\4413\4414\4415\4416\4417\4418\4419\4420\4421\4422\4423\4424\4425\4426\4427\4428\4429\4430\4431\4432\4433\4434\4435\4436\4437\4438\4439\4440\4441\4442\4443\4444\4445\4446\4447\4448\4449\4450\4451\4452\4453\4454\4455\4456\4457\4458\4459\4460\4461\4462\4463\4464\4465\4466\4467\4468\4469\4470\4471\4472\4473\4474\4475\4476\4477\4478\4479\4480\4481\4482\4483\4484\4485\4486\4487\4488\4489\4490\4491\4492\4493\4494\4495\4496\4497\4498\4499\4500\4501\4502\4503\4504\4505\4506\4507\4508\4509\4510\4511\4512\4513\4514\4515\4516\4517\4518\4519\4520\4521\4522\4523\4524\4525\4526\4527\4528\4529\4530\4531\4532\4533\4534\4535\4536\4537\4538\4539\4540\4541\4542\4543\4544\4545\4546\4547\4548\4549\4550\4551\4552\4553\4554\4555\4556\4557\4558\4559\4560\4561\4562\4563\4564\4565\4566\4567\4568\4569\4570\4571\4572\4573\4574\4575\4576\4577\4578\4579\4580\4581\4582\4583\4584\4585\4586\4587\4588\4589\4590\4591\4592\4593\4594\4595\4596\4597\4598\4599\4600\4601\4602\4603\4604\4605\4606\4607\4608\4609\4610\4611\4612\4613\4614\4615\4616\4617\4618\4619\4620\4621\4622\4623\4624\4625\4626\4627\4628\4629\4630\4631\4632\4633\4634\4635\4636\4637\4638\4639\4640\4641\4642\4643\4644\4645\4646\4647\4648\4649\4650\4651\4652\4653\4654\4655\4656\4657\4658\4659\4660\4661\4662\4663\4664\4665\4666\4667\4668\4669\4670\4671\4672\4673\4674\4675\4676\4677\4678\4679\4680\4681\4682\4683\4684\4685\4686\4687\4688\4689\4690\4691\4692\4693\4694\4695\4696\4697\4698\4699\4700\4701\4702\4703\4704\4705\4706\4707\4708\4709\4710\4711\4712\4713\4714\4715\4716\4717\4718\4719\4720\4721\4722\4723\4724\4725\4726\4727\4728\4729\4730\4731\4732\4733\4734\4735\4736\4737\4738\4739\4740\4741\4742\4743\4744\4745\4746\4747\4748\4749\4750\4751\4752\4753\4754\4755\4756\4757\4758\4759\4760\4761\4762\4763\4764\4765\4766\4767\4768\4769\4770\4771\4772\4773\4774\4775\4776\4777\4778\4779\4780\4781\4782\4783\4784\4785\4786\4787\4788\4789\4790\4791\4792\4793\4794\4795\4796\4797\4798\4799\4800\4801\4802\4803\4804\4805\4806\4807\4808\4809\4810\4811\4812\4813\4814\4815\4816\4817\4818\4819\4820\4821\4822\4823\4824\4825\4826\4827\4828\4829\4830\4831\4832\4833\4834\4835\4836\4837\4838\4839\4840\4841\4842\4843\4844\4845\4846\4847\4848\4849\4850\4851\4852\4853\4854\4855\4856\4857\4858\4859\4860\4861\4862\4863\4864\4865\4866\4867\4868\4869\4870\4871\4872\4873\4874\4875\4876\4877\4878\4879\4880\4881\4882\4883\4884\4885\4886\4887\4888\4889\4890\4891\4892\4893\4894\4895\4896\4897\4898\4899\4900\4901\4902\4903\4904\4905\4906\4907\4908\4909\4910\4911\4912\4913\4914\4915\4916\4917\4918\4919\4920\4921\4922\4923\4924\4925\4926\4927\4928\4929\4930\4931\4932\4933\4934\4935\4936\4937\4938\4939\4940\4941\4942\4943\4944\4945\4946\4947\4948\4949\4950\4951\4952\4953\4954\4955\4956\4957\4958\4959\4960\4961\4962\4963\4964\4965\4966\4967\4968\4969\4970\4971\4972\4973\4974\4975\4976\4977\4978\4979\4980\4981\4982\4983\4984\4985\4986\4987\4988\4989\4990\4991\4992\4993\4994\4995\4996\4997\4998\4999\5000\5001\5002\5003\5004\5005\5006\5007\5008\5009\5010\5011\5012\5013\5014\5015\5016\5017\5018\5019\5020\5021\5022\5023\5024\5025\5026\5027\5028\5029\5030\5031\5032\5033\5034\5035\5036\5037\5038\5039\5040\5041\5042\5043\5044\5045\5046\5047\5048\5049\5050\5051\5052\5053\5054\5055\5056\5057\5058\5059\5060\5061\5062\5063\5064\5065\5066\5067\5068\5069\5070\5071\5072\5073\5074\5075\5076\5077\5078\5079\5080\5081\5082\5083\5084\5085\5086\5087\5088\5089\5090\5091\5092\5093\5094\5095\5096\5097\5098\5099\5100\5101\5102\5103\5104\5105\5106\5107\5108\5109\5110\5111\5112\5113\5114\5115\5116\5117\5118\5119\5120\5121\5122\5123\5124\5125\5126\5127\5128\5129\5130\5131\5132\5133\5134\5135\5136\5137\5138\5139\5140\5141\5142\5143\5144\5145\5146\5147\5148\5149\5150\5151\5152\5153\5154\5155\5156\5157\5158\5159\5160\5161\5162\5163\5164\5165\5166\5167\5168\5169\5170\5171\5172\5173\5174\5175\5176\5177\5178\5179\5180\5181\5182\5183\5184\5185\5186\5187\5188\5189\5190\5191\5192\5193\5194\5195\5196\5197\5198\5199\5200\5201\5202\5203\5204\5205\5206\5207\5208\5209\5210\5211\5212\5213\5214\5215\5216\5217\5218\5219\5220\5221\5222\5223\5224\5225\5226\5227\5228\5229\5230\5231\5232\5233\5234\5235\5236\5237\5238\5239\5240\5241\5242\5243\5244\5245\5246\5247\5248\5249\5250\5251\5252\5253\5254\5255\5256\5257\5258\5259\5260\5261\5262\5263\5264\5265\5266\5267\5268\5269\5270\5271\5272\5273\5274\5275\5276\5277\5278\5279\5280\5281\5282\5283\5284\5285\5286\5287\5288\5289\5290\5291\5292\5293\5294\5295\5296\5297\5298\5299\5300\5301\5302\5303\5304\5305\5306\5307\5308\5309\5310\5311\5312\5313\5314\5315\5316\5317\5318\5319\5320\5321\5322\5323\5324\5325\5326\5327\5328\5329\5330\5331\5332\5333\5334\5335\5336\5337\5338\5339\5340\5341\5342\5343\5344\5345\5346\5347\5348\5349\5350\5351\5352\5353\5354\5355\5356\5357\5358\5359\5360\5361\5362\5363\5364\5365\5366\5367\5368\5369\5370\5371\5372\5373\5374\5375\5376\5377\5378\5379\5380\5381\5382\5383\5384\5385\5386\5387\5388\5389\5390\5391\5392\5393\5394\5395\5396\5397\5398\5399\5400\5401\5402\5403\5404\5405\5406\5407\5408\5409\5410\5411\5412\5413\5414\5415\5416\5417\5418\5419\5420\5421\5422\5423\5424\5425\5426\5427\5428\5429\5430\5431\5432\5433\5434\5435\5436\5437\5438\5439\5440\5441\5442\5443\5444\5445\5446\5447\5448\5449\5450\5451\5452\5453\5454\5455\5456\5457\5458\5459\5460\5461\5462\5463\5464\5465\5466\5467\5468\5469\5470\5471\5472\5473\5474\5475\5476\5477\5478\5479\5480\5481\5482\5483\5484\5485\5486\5487\5488\5489\5490\5491\5492\5493\5494\5495\5496\5497\5498\5499\5500\5501\5502\5503\5504\5505\5506\5507\5508\5509\5510\5511\5512\5513\5514\5515\5516\5517\5518\5519\5520\5521\5522\5523\5524\5525\5526\5527\5528\5529\5530\5531\5532\5533\5534\5535\5536\5537\5538\5539\5540\5541\5542\5543\5544\5545\5546\5547\5548\5549\5550\5551\5552\5553\5554\5555\5556\5557\5558\5559\5560\5561\5562\5563\5564\5565\5566\5567\5568\5569\5570\5571\5572\5573\5574\5575\5576\5577\5578\5579\5580\5581\5582\5583\5584\5585\5586\5587\5588\5589\5590\5591\5592\5593\5594\5595\5596\5597\5598\5599\5600\5601\5602\5603\5604\5605\5606\5607\5608\5609\5610\5611\5612\5613\5614\5615\5616\5617\5618\5619\5620\5621\5622\5623\5624\5625\5626\5627\5628\5629\5630\5631\5632\5633\5634\5635\5636\5637\5638\5639\5640\5641\5642\5643\5644\5645\5646\5647\5648\5649\5650\5651\5652\5653\5654\5655\5656\5657\5658\5659\5660\5661\5662\5663\5664\5665\5666\5667\5668\5669\5670\5671\5672\5673\5674\5675\5676\5677\5678\5679\5680\5681\5682\5683\5684\5685\5686\5687\5688\5689\5690\5691\5692\5693\5694\5695\5696\5697\5698\5699\5700\5701\5702\5703\5704\5705\5706\5707\5708\5709\5710\5711\5712\5713\5714\5715\5716\5717\5718\5719\5720\5721\5722\5723\5724\5725\5726\5727\5728\5729\5730\5731\5732\5733\5734\5735\5736\5737\5738\5739\5740\5741\5742\5743\5744\5745\5746\5747\5748\5749\5750\5751\5752\5753\5754\5755\5756\5757\5758\5759\5760\5761\5762\5763\5764\5765\5766\5767\5768\5769\5770\5771\5772\5773\5774\5775\5776\5777\5778\5779\5780\5781\5782\5783\5784\5785\5786\5787\5788\5789\5790\5791\5792\5793\5794\5795\5796\5797\5798\5799\5800\5801\5802\5803\5804\5805\5806\5807\5808\5809\5810\5811\5812\5813\5814\5815\5816\5817\5818\5819\5820\5821\5822\5823\5824\5825\5826\5827\5828\5829\5830\5831\5832\5833\5834\5835\5836\5837\5838\5839\5840\5841\5842\5843\5844\5845\5846\5847\5848\5849\5850\5851\5852\5853\5854\5855\5856\5857\5858\5859\5860\5861\5862\5863\5864\5865\5866\5867\5868\5869\5870\5871\5872\5873\5874\5875\5876\5877\5878\5879\5880\5881\5882\5883\5884\5885\5886\5887\5888\5889\5890\5891\5892\5893\5894\5895\5896\5897\5898\5899\5900\5901\5902\5903\5904\5905\5906\5907\5908\5909\5910\5911\5912\5913\5914\5915\5916\5917\5918\5919\5920\5921\5922\5923\5924\5925\5926\5927\5928\5929\5930\5931\5932\5933\5934\5935\5936\5937\5938\5939\5940\5941\5942\5943\5944\5945\5946\5947\5948\5949\5950\5951\5952\5953\5954\5955\5956\5957\5958\5959\5960\5961\5962\5963\5964\5965\5966\5967\5968\5969\5970\5971\5972\5973\5974\5975\5976\5977\5978\5979\5980\5981\5982\5983\5984\5985\5986\5987\5988\5989\5990\5991\5992\5993\5994\5995\5996\5997\5998\5999\6000\6001\6002\6003\6004\6005\6006\6007\6008\6009\6010\6011\6012\6013\6014\6015\6016\6017\6018\6019\6020\6021\6022\6023\6024\6025\6026\6027\6028\6029\6030\6031\6032\6033\6034\6035\6036\6037\6038\6039\6040\6041\6042\6043\6044\6045\6046\6047\6048\6049\6050\6051\6052\6053\6054\6055\6056\6057\6058\6059\6060\6061\6062\6063\6064\6065\6066\6067\6068\6069\6070\6071\6072\6073\6074\6075\6076\6077\6078\6079\6080\6081\6082\6083\6084\6085\6086\6087\6088\6089\6090\6091\6092\6093\6094\6095\6096\6097\6098\6099\6100\6101\6102\6103\6104\6105\6106\6107\6108\6109\6110\6111\6112\6113\6114\6115\6116\6117\6118\6119\6120\6121\6122\6123\6124\6125\6126\6127\6128\6129\6130\6131\6132\6133\6134\6135\6136\6137\6138\6139\6140\6141\6142\6143\6144\6145\6146\6147\6148\6149\6150\6151\6152\6153\6154\6155\6156\6157\6158\6159\6160\6161\6162\6163\6164\6165\6166\6167\6168\6169\6170\6171\6172\6173\6174\6175\6176\6177\6178\6179\6180\6181\6182\6183\6184\6185\6186\6187\6188\6189\6190\6191\6192\6193\6194\6195\6196\6197\6198\6199\6200\6201\6202\6203\6204\6205\6206\6207\6208\6209\6210\6211\6212\6213\6214\6215\6216\6217\6218\6219\6220\6221\6222\6223\6224\6225\6226\6227\6228\6229\6230\6231\6232\6233\6234\6235\6236\6237\6238\6239\6240\6241\6242\6243\6244\6245\6246\6247\6248\6249\6250\6251\6252\6253\6254\6255\6256\6257\6258\6259\6260\6261\6262\6263\6264\6265\6266\6267\6268\6269\6270\6271\6272\6273\6274\6275\6276\6277\6278\6279\6280\6281\6282\6283\6284\6285\6286\6287\6288\6289\6290\6291\6292\6293\6294\6295\6296\6297\6298\6299\6300\6301\6302\6303\6304\6305\6306\6307\6308\6309\6310\6311\6312\6313\6314\6315\6316\6317\6318\6319\6320\6321\6322\6323\6324\6325\6326\6327\6328\6329\6330\6331\6332\6333\6334\6335\6336\6337\6338\6339\6340\6341\6342\6343\6344\6345\6346\6347\6348\6349\6350\6351\6352\6353\6354\6355\6356\6357\6358\6359\6360\6361\6362\6363\6364\6365\6366\6367\6368\6369\6370\6371\6372\6373\6374\6375\6376\6377\6378\6379\6380\6381\6382\6383\6384\6385\6386\6387\6388\6389\6390\6391\6392\6393\6394\6395\6396\6397\6398\6399\6400\6401\6402\6403\6404\6405\6406\6407\6408\6409\6410\6411\6412\6413\6414\6415\6416\6417\6418\6419\6420\6421\6422\6423\6424\6425\6426\6427\6428\6429\6430\6431\6432\6433\6434\6435\6436\6437\6438\6439\6440\6441\6442\6443\6444\6445\6446\6447\6448\6449\6450\6451\6452\6453\6454\6455\6456\6457\6458\6459\6460\6461\6462\6463\6464\6465\6466\6467\6468\6469\6470\6471\6472\6473\6474\6475\6476\6477\6478\6479\6480\6481\6482\6483\6484\6485\6486\6487\6488\6489\6490\6491\6492\6493\6494\6495\6496\6497\6498\6499\6500\6501\6502\6503\6504\6505\6506\6507\6508\6509\6510\6511\6512\6513\6514\6515\6516\6517\6518\6519\6520\6521\6522\6523\6524\6525\6526\6527\6528\6529\6530\6531\6532\6533\6534\6535\6536\6537\6538\6539\6540\6541\6542\6543\6544\6545\6546\6547\6548\6549\6550\6551\6552\6553\6554\6555\6556\6557\6558\6559\6560\6561\6562\6563\6564\6565\6566\6567\6568\6569\6570\6571\6572\6573\6574\6575\6576\6577\6578\6579\6580\6581\6582\6583\6584\6585\6586\6587\6588\6589\6590\6591\6592\6593\6594\6595\6596\6597\6598\6599\6600\6601\6602\6603\6604\6605\6606\6607\6608\6609\6610\6611\6612\6613\6614\6615\6616\6617\6618\6619\6620\6621\6622\6623\6624\6625\6626\6627\6628\6629\6630\6631\6632\6633\6634\6635\6636\6637\6638\6639\6640\6641\6642\6643\6644\6645\6646\6647\6648\6649\6650\6651\6652\6653\6654\6655\6656\6657\6658\6659\6660\6661\6662\6663\6664\6665\6666\6667\6668\6669\6670\6671\6672\6673\6674\6675\6676\6677\6678\6679\6680\6681\6682\6683\6684\6685\6686\6687\6688\6689\6690\6691\6692\6693\6694\6695\6696\6697\6698\6699\6700\6701\6702\6703\6704\6705\6706\6707\6708\6709\6710\6711\6712\6713\6714\6715\6716\6717\6718\6719\6720\6721\6722\6723\6724\6725\6726\6727\6728\6729\6730\6731\6732\6733\6734\6735\6736\6737\6738\6739\6740\6741\6742\6743\6744\6745\6746\6747\6748\6749\6750\6751\6752\6753\6754\6755\6756\6757\6758\6759\6760\6761\6762\6763\6764\6765\6766\6767\6768\6769\6770\6771\6772\6773\6774\6775\6776\6777\6778\6779\6780\6781\6782\6783\6784\6785\6786\6787\6788\6789\6790\6791\6792\6793\6794\6795\6796\6797\6798\6799\6800\6801\6802\6803\6804\6805\6806\6807\6808\6809\6810\6811\6812\6813\6814\6815\6816\6817\6818\6819\6820\6821\6822\6823\6824\6825\6826\6827\6828\6829\6830\6831\6832\6833\6834\6835\6836\6837\6838\6839\6840\6841\6842\6843\6844\6845\6846\6847\6848\6849\6850\6851\6852\6853\6854\6855\6856\6857\6858\6859\6860\6861\6862\6863\6864\6865\6866\6867\6868\6869\6870\6871\6872\6873\6874\6875\6876\6877\6878\6879\6880\6881\6882\6883\6884\6885\6886\6887\6888\6889\6890\6891\6892\6893\6894\6895\6896\6897\6898\6899\6900\6901\6902\6903\6904\6905\6906\6907\6908\6909\6910\6911\6912\6913\6914\6915\6916\6917\6918\6919\6920\6921\6922\6923\6924\6925\6926\6927\6928\6929\6930\6931\6932\6933\6934\6935\6936\6937\6938\6939\6940\6941\6942\6943\6944\6945\6946\6947\6948\6949\6950\6951\6952\6953\6954\6955\6956\6957\6958\6959\6960\6961\6962\6963\6964\6965\6966\6967\6968\6969\6970\6971\6972\6973\6974\6975\6976\6977\6978\6979\6980\6981\6982\6983\6984\6985\6986\6987\6988\6989\6990\6991\6992\6993\6994\6995\6996\6997\6998\6999\7000\7001\7002\7003\7004\7005\7006\7007\7008\7009\7010\7011\7012\7013\7014\7015\7016\7017\7018\7019\7020\7021\7022\7023\7024\7025\7026\7027\7028\7029\7030\7031\7032\7033\7034\7035\7036\7037\7038\7039\7040\7041\7042\7043\7044\7045\7046\7047\7048\7049\7050\7051\7052\7053\7054\7055\7056\7057\7058\7059\7060\7061\7062\7063\7064\7065\7066\7067\7068\7069\7070\7071\7072\7073\7074\7075\7076\7077\7078\7079\7080\7081\7082\7083\7084\7085\7086\7087\7088\7089\7090\7091\7092\7093\7094\7095\7096\7097\7098\7099\7100\7101\7102\7103\7104\7105\7106\7107\7108\7109\7110\7111\7112\7113\7114\7115\7116\7117\7118\7119\7120\7121\7122\7123\7124\7125\7126\7127\7128\7129\7130\7131\7132\7133\7134\7135\7136\7137\7138\7139\7140\7141\7142\7143\7144\7145\7146\7147\7148\7149\7150\7151\7152\7153\7154\7155\7156\7157\7158\7159\7160\7161\7162\7163\7164\7165\7166\7167\7168\7169\7170\7171\7172\7173\7174\7175\7176\7177\7178\7179\7180\7181\7182\7183\7184\7185\7186\7187\7188\7189\7190\7191\7192\7193\7194\7195\7196\7197\7198\7199\7200\7201\7202\7203\7204\7205\7206\7207\7208\7209\7210\7211\7212\7213\7214\7215\7216\7217\7218\7219\7220\7221\7222\7223\7224\7225\7226\7227\7228\7229\7230\7231\7232\7233\7234\7235\7236\7237\7238\7239\7240\7241\7242\7243\7244\7245\7246\7247\7248\7249\7250\7251\7252\7253\7254\7255\7256\7257\7258\7259\7260\7261\7262\7263\7264\7265\7266\7267\7268\7269\7270\7271\7272\7273\7274\7275\7276\7277\7278\7279\7280\7281\7282\7283\7284\7285\7286\7287\7288\7289\7290\7291\7292\7293\7294\7295\7296\7297\7298\7299\7300\7301\7302\7303\7304\7305\7306\7307\7308\7309\7310\7311\7312\7313\7314\7315\7316\7317\7318\7319\7320\7321\7322\7323\7324\7325\7326\7327\7328\7329\7330\7331\7332\7333\7334\7335\7336\7337\7338\7339\7340\7341\7342\7343\7344\7345\7346\7347\7348\7349\7350\7351\7352\7353\7354\7355\7356\7357\7358\7359\7360\7361\7362\7363\7364\7365\7366\7367\7368\7369\7370\7371\7372\7373\7374\7375\7376\7377\7378\7379\7380\7381\7382\7383\7384\7385\7386\7387\7388\7389\7390\7391\7392\7393\7394\7395\7396\7397\7398\7399\7400\7401\7402\7403\7404\7405\7406\7407\7408\7409\7410\7411\7412\7413\7414\7415\7416\7417\7418\7419\7420\7421\7422\7423\7424\7425\7426\7427\7428\7429\7430\7431\7432\7433\7434\7435\7436\7437\7438\7439\7440\7441\7442\7443\7444\7445\7446\7447\7448\7449\7450\7451\7452\7453\7454\7455\7456\7457\7458\7459\7460\7461\7462\7463\7464\7465\7466\7467\7468\7469\7470\7471\7472\7473\7474\7475\7476\7477\7478\7479\7480\7481\7482\7483\7484\7485\7486\7487\7488\7489\7490\7491\7492\7493\7494\7495\7496\7497\7498\7499\7500\7501\7502\7503\7504\7505\7506\7507\7508\7509\7510\7511\7512\7513\7514\7515\7516\7517\7518\7519\7520\7521\7522\7523\7524\7525\7526\7527\7528\7529\7530\7531\7532\7533\7534\7535\7536\7537\7538\7539\7540\7541\7542\7543\7544\7545\7546\7547\7548\7549\7550\7551\7552\7553\7554\7555\7556\7557\7558\7559\7560\7561\7562\7563\7564\7565\7566\7567\7568\7569\7570\7571\7572\7573\7574\7575\7576\7577\7578\7579\7580\7581\7582\7583\7584\7585\7586\7587\7588\7589\7590\7591\7592\7593\7594\7595\7596\7597\7598\7599\7600\7601\7602\7603\7604\7605\7606\7607\7608\7609\7610\7611\7612\7613\7614\7615\7616\7617\7618\7619\7620\7621\7622\7623\7624\7625\7626\7627\7628\7629\7630\7631\7632\7633\7634\7635\7636\7637\7638\7639\7640\7641\7642\7643\7644\7645\7646\7647\7648\7649\7650\7651\7652\7653\7654\7655\7656\7657\7658\7659\7660\7661\7662\7663\7664\7665\7666\7667\7668\7669\7670\7671\7672\7673\7674\7675\7676\7677\7678\7679\7680\7681\7682\7683\7684\7685\7686\7687\7688\7689\7690\7691\7692\7693\7694\7695\7696\7697\7698\7699\7700\7701\7702\7703\7704\7705\7706\7707\7708\7709\7710\7711\7712\7713\7714\7715\7716\7717\7718\7719\7720\7721\7722\7723\7724\7725\7726\7727\7728\7729\7730\7731\7732\7733\7734\7735\7736\7737\7738\7739\7740\7741\7742\7743\7744\7745\7746\7747\7748\7749\7750\7751\7752\7753\7754\7755\7756\7757\7758\7759\7760\7761\7762\7763\7764\7765\7766\7767\7768\7769\7770\7771\7772\7773\7774\7775\7776\7777\7778\7779\7780\7781\7782\7783\7784\7785\7786\7787\7788\7789\7790\7791\7792\7793\7794\7795\7796\7797\7798\7799\7800\7801\7802\7803\7804\7805\7806\7807\7808\7809\7810\7811\7812\7813\7814\7815\7816\7817\7818\7819\7820\7821\7822\7823\7824\7825\7826\7827\7828\7829\7830\7831\7832\7833\7834\7835\7836\7837\7838\7839\7840\7841\7842\7843\7844\7845\7846\7847\7848\7849\7850\7851\7852\7853\7854\7855\7856\7857\7858\7859\7860\7861\7862\7863\7864\7865\7866\7867\7868\7869\7870\7871\7872\7873\7874\7875\7876\7877\7878\7879\7880\7881\7882\7883\7884\7885\7886\7887\7888\7889\7890\7891\7892\7893\7894\7895\7896\7897\7898\7899\7900\7901\7902\7903\7904\7905\7906\7907\7908\7909\7910\7911\7912\7913\7914\7915\7916\7917\7918\7919\7920\7921\7922\7923\7924\7925\7926\7927\7928\7929\7930\7931\7932\7933\7934\7935\7936\7937\7938\7939\7940\7941\7942\7943\7944\7945\7946\7947\7948\7949\7950\7951\7952\7953\7954\7955\7956\7957\7958\7959\7960\7961\7962\7963\7964\7965\7966\7967\7968\7969\7970\7971\7972\7973\7974\7975\7976\7977\7978\7979\7980\7981\7982\7983\7984\7985\7986\7987\7988\7989\7990\7991\7992\7993\7994\7995\7996\7997\7998\7999\8000\8001\8002\8003\8004\8005\8006\8007\8008\8009\8010\8011\8012\8013\8014\8015\8016\8017\8018\8019\8020\8021\8022\8023\8024\8025\8026\8027\8028\8029\8030\8031\8032\8033\8034\8035\8036\8037\8038\8039\8040\8041\8042\8043\8044\8045\8046\8047\8048\8049\8050\8051\8052\8053\8054\8055\8056\8057\8058\8059\8060\8061\8062\8063\8064\8065\8066\8067\8068\8069\8070\8071\8072\8073\8074\8075\8076\8077\8078\8079\8080\8081\8082\8083\8084\8085\8086\8087\8088\8089\8090\8091\8092\8093\8094\8095\8096\8097\8098\8099\8100\8101\8102\8103\8104\8105\8106\8107\8108\8109\8110\8111\8112\8113\8114\8115\8116\8117\8118\8119\8120\8121\8122\8123\8124\8125\8126\8127\8128\8129\8130\8131\8132\8133\8134\8135\8136\8137\8138\8139\8140\8141\8142\8143\8144\8145\8146\8147\8148\8149\8150\8151\8152\8153\8154\8155\8156\8157\8158\8159\8160\8161\8162\8163\8164\8165\8166\8167\8168\8169\8170\8171\8172\8173\8174\8175\8176\8177\8178\8179\8180\8181\8182\8183\8184\8185\8186\8187\8188\8189\8190\8191\8192\8193\8194\8195\8196\8197\8198\8199\8200\8201\8202\8203\8204\8205\8206\8207\8208\8209\8210\8211\8212\8213\8214\8215\8216\8217\8218\8219\8220\8221\8222\8223\8224\8225\8226\8227\8228\8229\8230\8231\8232\8233\8234\8235\8236\8237\8238\8239\8240\8241\8242\8243\8244\8245\8246\8247\8248\8249\8250\8251\8252\8253\8254\8255\8256\8257\8258\8259\8260\8261\8262\8263\8264\8265\8266\8267\8268\8269\8270\8271\8272\8273\8274\8275\8276\8277\8278\8279\8280\8281\8282\8283\8284\8285\8286\8287\8288\8289\8290\8291\8292\8293\8294\8295\8296\8297\8298\8299\8300\8301\8302\8303\8304\8305\8306\8307\8308\8309\8310\8311\8312\8313\8314\8315\8316\8317\8318\8319\8320\8321\8322\8323\8324\8325\8326\8327\8328\8329\8330\8331\8332\8333\8334\8335\8336\8337\8338\8339\8340\8341\8342\8343\8344\8345\8346\8347\8348\8349\8350\8351\8352\8353\8354\8355\8356\8357\8358\8359\8360\8361\8362\8363\8364\8365\8366\8367\8368\8369\8370\8371\8372\8373\8374\8375\8376\8377\8378\8379\8380\8381\8382\8383\8384\8385\8386\8387\8388\8389\8390\8391\8392\8393\8394\8395\8396\8397\8398\8399\8400\8401\8402\8403\8404\8405\8406\8407\8408\8409\8410\8411\8412\8413\8414\8415\8416\8417\8418\8419\8420\8421\8422\8423\8424\8425\8426\8427\8428\8429\8430\8431\8432\8433\8434\8435\8436\8437\8438\8439\8440\8441\8442\8443\8444\8445\8446\8447\8448\8449\8450\8451\8452\8453\8454\8455\8456\8457\8458\8459\8460\8461\8462\8463\8464\8465\8466\8467\8468\8469\8470\8471\8472\8473\8474\8475\8476\8477\8478\8479\8480\8481\8482\8483\8484\8485\8486\8487\8488\8489\8490\8491\8492\8493\8494\8495\8496\8497\8498\8499\8500\8501\8502\8503\8504\8505\8506\8507\8508\8509\8510\8511\8512\8513\8514\8515\8516\8517\8518\8519\8520\8521\8522\8523\8524\8525\8526\8527\8528\8529\8530\8531\8532\8533\8534\8535\8536\8537\8538\8539\8540\8541\8542\8543\8544\8545\8546\8547\8548\8549\8550\8551\8552\8553\8554\8555\8556\8557\8558\8559\8560\8561\8562\8563\8564\8565\8566\8567\8568\8569\8570\8571\8572\8573\8574\8575\8576\8577\8578\8579\8580\8581\8582\8583\8584\8585\8586\8587\8588\8589\8590\8591\8592\8593\8594\8595\8596\8597\8598\8599\8600\8601\8602\8603\8604\8605\8606\8607\8608\8609\8610\8611\8612\8613\8614\8615\8616\8617\8618\8619\8620\8621\8622\8623\8624\8625\8626\8627\8628\8629\8630\8631\8632\8633\8634\8635\8636\8637\8638\8639\8640\8641\8642\8643\8644\8645\8646\8647\8648\8649\8650\8651\8652\8653\8654\8655\8656\8657\8658\8659\8660\8661\8662\8663\8664\8665\8666\8667\8668\8669\8670\8671\8672\8673\8674\8675\8676\8677\8678\8679\8680\8681\8682\8683\8684\8685\8686\8687\8688\8689\8690\8691\8692\8693\8694\8695\8696\8697\8698\8699\8700\8701\8702\8703\8704\8705\8706\8707\8708\8709\8710\8711\8712\8713\8714\8715\8716\8717\8718\8719\8720\8721\8722\8723\8724\8725\8726\8727\8728\8729\8730\8731\8732\8733\8734\8735\8736\8737\8738\8739\8740\8741\8742\8743\8744\8745\8746\8747\8748\8749\8750\8751\8752\8753\8754\8755\8756\8757\8758\8759\8760\8761\8762\8763\8764\8765\8766\8767\8768\8769\8770\8771\8772\8773\8774\8775\8776\8777\8778\8779\8780\8781\8782\8783\8784\8785\8786\8787\8788\8789\8790\8791\8792\8793\8794\8795\8796\8797\8798\8799\8800\8801\8802\8803\8804\8805\8806\8807\8808\8809\8810\8811\8812\8813\8814\8815\8816\8817\8818\8819\8820\8821\8822\8823\8824\8825\8826\8827\8828\8829\8830\8831\8832\8833\8834\8835\8836\8837\8838\8839\8840\8841\8842\8843\8844\8845\8846\8847\8848\8849\8850\8851\8852\8853\8854\8855\8856\8857\8858\8859\8860\8861\8862\8863\8864\8865\8866\8867\8868\8869\8870\8871\8872\8873\8874\8875\8876\8877\8878\8879\8880\8881\8882\8883\8884\8885\8886\8887\8888\8889\8890\8891\8892\8893\8894\8895\8896\8897\8898\8899\8900\8901\8902\8903\8904\8905\8906\8907\8908\8909\8910\8911\8912\8913\8914\8915\8916\8917\8918\8919\8920\8921\8922\8923\8924\8925\8926\8927\8928\8929\8930\8931\8932\8933\8934\8935\8936\8937\8938\8939\8940\8941\8942\8943\8944\8945\8946\8947\8948\8949\8950\8951\8952\8953\8954\8955\8956\8957\8958\8959\8960\8961\8962\8963\8964\8965\8966\8967\8968\8969\8970\8971\8972\8973\8974\8975\8976\8977\8978\8979\8980\8981\8982\8983\8984\8985\8986\8987\8988\8989\8990\8991\8992\8993\8994\8995\8996\8997\8998\8999\9000\9001\9002\9003\9004\9005\9006\9007\9008\9009\9010\9011\9012\9013\9014\9015\9016\9017\9018\9019\9020\9021\9022\9023\9024\9025\9026\9027\9028\9029\9030\9031\9032\9033\9034\9035\9036\9037\9038\9039\9040\9041\9042\9043\9044\9045\9046\9047\9048\9049\9050\9051\9052\9053\9054\9055\9056\9057\9058\9059\9060\9061\9062\9063\9064\9065\9066\9067\9068\9069\9070\9071\9072\9073\9074\9075\9076\9077\9078\9079\9080\9081\9082\9083\9084\9085\9086\9087\9088\9089\9090\9091\9092\9093\9094\9095\9096\9097\9098\9099\9100\9101\9102\9103\9104\9105\9106\9107\9108\9109\9110\9111\9112\9113\9114\9115\9116\9117\9118\9119\9120\9121\9122\9123\9124\9125\9126\9127\9128\9129\9130\9131\9132\9133\9134\9135\9136\9137\9138\9139\9140\9141\9142\9143\9144\9145\9146\9147\9148\9149\9150\9151\9152\9153\9154\9155\9156\9157\9158\9159\9160\9161\9162\9163\9164\9165\9166\9167\9168\9169\9170\9171\9172\9173\9174\9175\9176\9177\9178\9179\9180\9181\9182\9183\9184\9185\9186\9187\9188\9189\9190\9191\9192\9193\9194\9195\9196\9197\9198\9199\9200\9201\9202\9203\9204\9205\9206\9207\9208\9209\9210\9211\9212\9213\9214\9215\9216\9217\9218\9219\9220\9221\9222\9223\9224\9225\9226\9227\9228\9229\9230\9231\9232\9233\9234\9235\9236\9237\9238\9239\9240\9241\9242\9243\9244\9245\9246\9247\9248\9249\9250\9251\9252\9253\9254\9255\9256\9257\9258\9259\9260\9261\9262\9263\9264\9265\9266\9267\9268\9269\9270\9271\9272\9273\9274\9275\9276\9277\9278\9279\9280\9281\9282\9283\9284\9285\9286\9287\9288\9289\9290\9291\9292\9293\9294\9295\9296\9297\9298\9299\9300\9301\9302\9303\9304\9305\9306\9307\9308\9309\9310\9311\9312\9313\9314\9315\9316\9317\9318\9319\9320\9321\9322\9323\9324\9325\9326\9327\9328\9329\9330\9331\9332\9333\9334\9335\9336\9337\9338\9339\9340\9341\9342\9343\9344\9345\9346\9347\9348\9349\9350\9351\9352\9353\9354\9355\9356\9357\9358\9359\9360\9361\9362\9363\9364\9365\9366\9367\9368\9369\9370\9371\9372\9373\9374\9375\9376\9377\9378\9379\9380\9381\9382\9383\9384\9385\9386\9387\9388\9389\9390\9391\9392\9393\9394\9395\9396\9397\9398\9399\9400\9401\9402\9403\9404\9405\9406\9407\9408\9409\9410\9411\9412\9413\9414\9415\9416\9417\9418\9419\9420\9421\9422\9423\9424\9425\9426\9427\9428\9429\9430\9431\9432\9433\9434\9435\9436\9437\9438\9439\9440\9441\9442\9443\9444\9445\9446\9447\9448\9449\9450\9451\9452\9453\9454\9455\9456\9457\9458\9459\9460\9461\9462\9463\9464\9465\9466\9467\9468\9469\9470\9471\9472\9473\9474\9475\9476\9477\9478\9479\9480\9481\9482\9483\9484\9485\9486\9487\9488\9489\9490\9491\9492\9493\9494\9495\9496\9497\9498\9499\9500\9501\9502\9503\9504\9505\9506\9507\9508\9509\9510\9511\9512\9513\9514\9515\9516\9517\9518\9519\9520\9521\9522\9523\9524\9525\9526\9527\9528\9529\9530\9531\9532\9533\9534\9535\9536\9537\9538\9539\9540\9541\9542\9543\9544\9545\9546\9547\9548\9549\9550\9551\9552\9553\9554\9555\9556\9557\9558\9559\9560\9561\9562\9563\9564\9565\9566\9567\9568\9569\9570\9571\9572\9573\9574\9575\9576\9577\9578\9579\9580\9581\9582\9583\9584\9585\9586\9587\9588\9589\9590\9591\9592\9593\9594\9595\9596\9597\9598\9599\9600\9601\9602\9603\9604\9605\9606\9607\9608\9609\9610\9611\9612\9613\9614\9615\9616\9617\9618\9619\9620\9621\9622\9623\9624\9625\9626\9627\9628\9629\9630\9631\9632\9633\9634\9635\9636\9637\9638\9639\9640\9641\9642\9643\9644\9645\9646\9647\9648\9649\9650\9651\9652\9653\9654\9655\9656\9657\9658\9659\9660\9661\9662\9663\9664\9665\9666\9667\9668\9669\9670\9671\9672\9673\9674\9675\9676\9677\9678\9679\9680\9681\9682\9683\9684\9685\9686\9687\9688\9689\9690\9691\9692\9693\9694\9695\9696\9697\9698\9699\9700\9701\9702\9703\9704\9705\9706\9707\9708\9709\9710\9711\9712\9713\9714\9715\9716\9717\9718\9719\9720\9721\9722\9723\9724\9725\9726\9727\9728\9729\9730\9731\9732\9733\9734\9735\9736\9737\9738\9739\9740\9741\9742\9743\9744\9745\9746\9747\9748\9749\9750\9751\9752\9753\9754\9755\9756\9757\9758\9759\9760\9761\9762\9763\9764\9765\9766\9767\9768\9769\9770\9771\9772\9773\9774\9775\9776\9777\9778\9779\9780\9781\9782\9783\9784\9785\9786\9787\9788\9789\9790\9791\9792\9793\9794\9795\9796\9797\9798\9799\9800\9801\9802\9803\9804\9805\9806\9807\9808\9809\9810\9811\9812\9813\9814\9815\9816\9817\9818\9819\9820\9821\9822\9823\9824\9825\9826\9827\9828\9829\9830\9831\9832\9833\9834\9835\9836\9837\9838\9839\9840\9841\9842\9843\9844\9845\9846\9847\9848\9849\9850\9851\9852\9853\9854\9855\9856\9857\9858\9859\9860\9861\9862\9863\9864\9865\9866\9867\9868\9869\9870\9871\9872\9873\9874\9875\9876\9877\9878\9879\9880\9881\9882\9883\9884\9885\9886\9887\9888\9889\9890\9891\9892\9893\9894\9895\9896\9897\9898\9899\9900\9901\9902\9903\9904\9905\9906\9907\9908\9909\9910\9911\9912\9913\9914\9915\9916\9917\9918\9919\9920\9921\9922\9923\9924\9925\9926\9927\9928\9929\9930\9931\9932\9933\9934\9935\9936\9937\9938\9939\9940\9941\9942\9943\9944\9945\9946\9947\9948\9949\9950\9951\9952\9953\9954\9955\9956\9957\9958\9959\9960\9961\9962\9963\9964\9965\9966\9967\9968\9969\9970\9971\9972\9973\9974\9975\9976\9977\9978\9979\9980\9981\9982\9983\9984\9985\9986\9987\9988\9989\9990\9991\9992\9993\9994\9995\9996\9997\9998\9999\10000</X2Data>
   <Y1Data>1.03\1.02\1.02\1.01\1.01\1.01\1\0.998\0.994\0.991\0.987\0.983\0.979\0.976\0.972\0.968\0.965\0.961\0.958\0.954\0.951\0.948\0.944\0.941\0.938\0.934\0.931\0.928\0.925\0.921\0.918\0.915\0.912\0.909\0.906\0.903\0.9\0.897\0.894\0.891\0.888\0.885\0.882\0.879\0.877\0.874\0.871\0.868\0.865\0.863\0.86\0.857\0.854\0.852\0.849\0.846\0.844\0.841\0.839\0.836\0.834\0.831\0.828\0.826\0.823\0.821\0.818\0.816\0.814\0.811\0.809\0.806\0.804\0.801\0.799\0.797\0.794\0.792\0.79\0.787\0.785\0.783\0.781\0.778\0.776\0.774\0.772\0.769\0.767\0.765\0.763\0.761\0.759\0.756\0.754\0.752\0.75\0.748\0.746\0.744\0.742\0.74\0.738\0.736\0.734\0.732\0.73\0.728\0.726\0.724\0.722\0.72\0.718\0.716\0.714\0.712\0.71\0.708\0.706\0.705\0.703\0.701\0.699\0.697\0.695\0.694\0.692\0.69\0.688\0.687\0.685\0.683\0.681\0.68\0.678\0.676\0.675\0.673\0.671\0.67\0.668\0.666\0.665\0.663\0.661\0.66\0.658\0.657\0.655\0.654\0.652\0.65\0.649\0.647\0.646\0.644\0.643\0.641\0.64\0.639\0.637\0.636\0.634\0.633\0.631\0.63\0.629\0.627\0.626\0.625\0.623\0.622\0.621\0.619\0.618\0.617\0.615\0.614\0.613\0.611\0.61\0.609\0.608\0.607\0.605\0.604\0.603\0.602\0.601\0.599\0.598\0.597\0.596\0.595\0.594\0.593\0.591\0.59\0.589\0.588\0.587\0.586\0.585\0.584\0.583\0.582\0.581\0.58\0.579\0.578\0.577\0.576\0.575\0.574\0.573\0.572\0.571\0.57\0.569\0.568\0.567\0.566\0.565\0.565\0.564\0.563\0.562\0.561\0.56\0.559\0.558\0.558\0.557\0.556\0.555\0.554\0.553\0.553\0.552\0.551\0.55\0.55\0.549\0.548\0.547\0.547\0.546\0.545\0.544\0.544\0.543\0.542\0.541\0.541\0.54\0.539\0.539\0.538\0.537\0.537\0.536\0.535\0.535\0.534\0.533\0.533\0.532\0.531\0.531\0.53\0.53\0.529\0.528\0.528\0.527\0.527\0.526\0.525\0.525\0.524\0.524\0.523\0.522\0.522\0.521\0.521\0.52\0.52\0.519\0.519\0.518\0.518\0.517\0.517\0.516\0.516\0.515\0.515\0.514\0.514\0.513\0.513\0.512\0.512\0.511\0.511\0.51\0.51\0.509\0.509\0.508\0.508\0.507\0.507\0.506\0.506\0.506\0.505\0.505\0.504\0.504\0.503\0.503\0.502\0.502\0.502\0.501\0.501\0.5\0.5\0.5\0.499\0.499\0.498\0.498\0.498\0.497\0.497\0.496\0.496\0.496\0.495\0.495\0.494\0.494\0.494\0.493\0.493\0.493\0.492\0.492\0.492\0.491\0.491\0.49\0.49\0.49\0.489\0.489\0.489\0.488\0.488\0.488\0.487\0.487\0.487\0.486\0.486\0.486\0.485\0.485\0.485\0.484\0.484\0.484\0.483\0.483\0.483\0.482\0.482\0.482\0.481\0.481\0.481\0.48\0.48\0.48\0.48\0.479\0.479\0.479\0.478\0.478\0.478\0.477\0.477\0.477\0.477\0.476\0.476\0.476\0.475\0.475\0.475\0.475\0.474\0.474\0.474\0.473\0.473\0.473\0.473\0.472\0.472\0.472\0.471\0.471\0.471\0.471\0.47\0.47\0.47\0.469\0.469\0.469\0.469\0.468\0.468\0.468\0.468\0.467\0.467\0.467\0.467\0.466\0.466\0.466\0.465\0.465\0.465\0.465\0.464\0.464\0.464\0.464\0.463\0.463\0.463\0.463\0.462\0.462\0.462\0.462\0.461\0.461\0.461\0.461\0.46\0.46\0.46\0.46\0.459\0.459\0.459\0.459\0.458\0.458\0.458\0.458\0.457\0.457\0.457\0.457\0.456\0.456\0.456\0.456\0.456\0.455\0.455\0.455\0.455\0.454\0.454\0.454\0.454\0.453\0.453\0.453\0.453\0.452\0.452\0.452\0.452\0.452\0.451\0.451\0.451\0.451\0.45\0.45\0.45\0.45\0.449\0.449\0.449\0.449\0.449\0.448\0.448\0.448\0.448\0.447\0.447\0.447\0.447\0.447\0.446\0.446\0.446\0.446\0.445\0.445\0.445\0.445\0.445\0.444\0.444\0.444\0.444\0.443\0.443\0.443\0.443\0.443\0.442\0.442\0.442\0.442\0.442\0.441\0.441\0.441\0.441\0.44\0.44\0.44\0.44\0.44\0.439\0.439\0.439\0.439\0.439\0.438\0.438\0.438\0.438\0.437\0.437\0.437\0.437\0.437\0.436\0.436\0.436\0.436\0.436\0.435\0.435\0.435\0.435\0.435\0.434\0.434\0.434\0.434\0.434\0.433\0.433\0.433\0.433\0.432\0.432\0.432\0.432\0.432\0.431\0.431\0.431\0.431\0.431\0.43\0.43\0.43\0.43\0.43\0.429\0.429\0.429\0.429\0.429\0.428\0.428\0.428\0.428\0.428\0.427\0.427\0.427\0.427\0.427\0.426\0.426\0.426\0.426\0.426\0.425\0.425\0.425\0.425\0.425\0.424\0.424\0.424\0.424\0.424\0.424\0.423\0.423\0.423\0.423\0.423\0.422\0.422\0.422\0.422\0.422\0.421\0.421\0.421\0.421\0.421\0.42\0.42\0.42\0.42\0.42\0.419\0.419\0.419\0.419\0.419\0.419\0.418\0.418\0.418\0.418\0.418\0.417\0.417\0.417\0.417\0.417\0.416\0.416\0.416\0.416\0.416\0.416\0.415\0.415\0.415\0.415\0.415\0.414\0.414\0.414\0.414\0.414\0.414\0.413\0.413\0.413\0.413\0.413\0.412\0.412\0.412\0.412\0.412\0.412\0.411\0.411\0.411\0.411\0.411\0.41\0.41\0.41\0.41\0.41\0.41\0.409\0.409\0.409\0.409\0.409\0.408\0.408\0.408\0.408\0.408\0.408\0.407\0.407\0.407\0.407\0.407\0.407\0.406\0.406\0.406\0.406\0.406\0.406\0.405\0.405\0.405\0.405\0.405\0.404\0.404\0.404\0.404\0.404\0.404\0.403\0.403\0.403\0.403\0.403\0.403\0.402\0.402\0.402\0.402\0.402\0.402\0.401\0.401\0.401\0.401\0.401\0.401\0.4\0.4\0.4\0.4\0.4\0.4\0.399\0.399\0.399\0.399\0.399\0.399\0.398\0.398\0.398\0.398\0.398\0.398\0.397\0.397\0.397\0.397\0.397\0.397\0.396\0.396\0.396\0.396\0.396\0.396\0.395\0.395\0.395\0.395\0.395\0.395\0.395\0.394\0.394\0.394\0.394\0.394\0.394\0.393\0.393\0.393\0.393\0.393\0.393\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.391\0.391\0.391\0.391\0.391\0.391\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.389\0.389\0.389\0.389\0.389\0.389\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.386\0.386\0.386\0.386\0.386\0.386\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.383\0.383\0.383\0.383\0.383\0.383\0.383\0.382\0.382\0.382\0.382\0.382\0.382\0.382\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.38\0.38\0.38\0.38\0.38\0.38\0.38\0.379\0.379\0.379\0.379\0.379\0.379\0.379\0.379\0.378\0.378\0.378\0.378\0.378\0.378\0.378\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263</Y1Data>
   <Y1Name>Training loss</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.805\0.803\0.8\0.798\0.796\0.793\0.791\0.788\0.786\0.784\0.782\0.779\0.777\0.775\0.773\0.771\0.769\0.767\0.765\0.763\0.761\0.759\0.757\0.755\0.753\0.751\0.749\0.747\0.745\0.744\0.742\0.74\0.738\0.736\0.735\0.733\0.731\0.73\0.728\0.726\0.725\0.723\0.721\0.72\0.718\0.717\0.715\0.713\0.712\0.71\0.709\0.707\0.706\0.704\0.703\0.701\0.7\0.698\0.697\0.696\0.694\0.693\0.691\0.69\0.689\0.687\0.686\0.685\0.683\0.682\0.681\0.679\0.678\0.677\0.675\0.674\0.673\0.672\0.67\0.669\0.668\0.667\0.665\0.664\0.663\0.662\0.661\0.659\0.658\0.657\0.656\0.655\0.654\0.652\0.651\0.65\0.649\0.648\0.647\0.646\0.645\0.644\0.642\0.641\0.64\0.639\0.638\0.637\0.636\0.635\0.634\0.633\0.632\0.631\0.63\0.629\0.628\0.627\0.626\0.625\0.624\0.623\0.622\0.621\0.62\0.619\0.618\0.617\0.616\0.616\0.615\0.614\0.613\0.612\0.611\0.61\0.609\0.608\0.607\0.607\0.606\0.605\0.604\0.603\0.602\0.602\0.601\0.6\0.599\0.598\0.598\0.597\0.596\0.595\0.594\0.594\0.593\0.592\0.591\0.591\0.59\0.589\0.588\0.588\0.587\0.586\0.585\0.585\0.584\0.583\0.583\0.582\0.581\0.581\0.58\0.579\0.579\0.578\0.577\0.577\0.576\0.575\0.575\0.574\0.574\0.573\0.572\0.572\0.571\0.571\0.57\0.569\0.569\0.568\0.568\0.567\0.566\0.566\0.565\0.565\0.564\0.564\0.563\0.563\0.562\0.562\0.561\0.561\0.56\0.56\0.559\0.559\0.558\0.558\0.557\0.557\0.556\0.556\0.555\0.555\0.554\0.554\0.553\0.553\0.552\0.552\0.551\0.551\0.551\0.55\0.55\0.549\0.549\0.548\0.548\0.548\0.547\0.547\0.546\0.546\0.546\0.545\0.545\0.544\0.544\0.544\0.543\0.543\0.542\0.542\0.542\0.541\0.541\0.541\0.54\0.54\0.54\0.539\0.539\0.538\0.538\0.538\0.537\0.537\0.537\0.536\0.536\0.536\0.535\0.535\0.535\0.534\0.534\0.534\0.533\0.533\0.533\0.533\0.532\0.532\0.532\0.531\0.531\0.531\0.53\0.53\0.53\0.529\0.529\0.529\0.529\0.528\0.528\0.528\0.527\0.527\0.527\0.527\0.526\0.526\0.526\0.525\0.525\0.525\0.525\0.524\0.524\0.524\0.524\0.523\0.523\0.523\0.523\0.522\0.522\0.522\0.521\0.521\0.521\0.521\0.52\0.52\0.52\0.52\0.519\0.519\0.519\0.519\0.518\0.518\0.518\0.518\0.517\0.517\0.517\0.517\0.516\0.516\0.516\0.516\0.515\0.515\0.515\0.515\0.515\0.514\0.514\0.514\0.514\0.513\0.513\0.513\0.513\0.512\0.512\0.512\0.512\0.511\0.511\0.511\0.511\0.511\0.51\0.51\0.51\0.51\0.509\0.509\0.509\0.509\0.508\0.508\0.508\0.508\0.508\0.507\0.507\0.507\0.507\0.506\0.506\0.506\0.506\0.506\0.505\0.505\0.505\0.505\0.504\0.504\0.504\0.504\0.504\0.503\0.503\0.503\0.503\0.502\0.502\0.502\0.502\0.502\0.501\0.501\0.501\0.501\0.5\0.5\0.5\0.5\0.5\0.499\0.499\0.499\0.499\0.498\0.498\0.498\0.498\0.498\0.497\0.497\0.497\0.497\0.497\0.496\0.496\0.496\0.496\0.495\0.495\0.495\0.495\0.495\0.494\0.494\0.494\0.494\0.493\0.493\0.493\0.493\0.493\0.492\0.492\0.492\0.492\0.492\0.491\0.491\0.491\0.491\0.49\0.49\0.49\0.49\0.49\0.489\0.489\0.489\0.489\0.489\0.488\0.488\0.488\0.488\0.487\0.487\0.487\0.487\0.487\0.486\0.486\0.486\0.486\0.486\0.485\0.485\0.485\0.485\0.484\0.484\0.484\0.484\0.484\0.483\0.483\0.483\0.483\0.483\0.482\0.482\0.482\0.482\0.482\0.481\0.481\0.481\0.481\0.48\0.48\0.48\0.48\0.48\0.479\0.479\0.479\0.479\0.479\0.478\0.478\0.478\0.478\0.478\0.477\0.477\0.477\0.477\0.476\0.476\0.476\0.476\0.476\0.475\0.475\0.475\0.475\0.475\0.474\0.474\0.474\0.474\0.474\0.473\0.473\0.473\0.473\0.473\0.472\0.472\0.472\0.472\0.471\0.471\0.471\0.471\0.471\0.47\0.47\0.47\0.47\0.47\0.469\0.469\0.469\0.469\0.469\0.468\0.468\0.468\0.468\0.468\0.467\0.467\0.467\0.467\0.467\0.466\0.466\0.466\0.466\0.466\0.465\0.465\0.465\0.465\0.464\0.464\0.464\0.464\0.464\0.463\0.463\0.463\0.463\0.463\0.462\0.462\0.462\0.462\0.462\0.461\0.461\0.461\0.461\0.461\0.46\0.46\0.46\0.46\0.46\0.459\0.459\0.459\0.459\0.459\0.458\0.458\0.458\0.458\0.458\0.457\0.457\0.457\0.457\0.457\0.456\0.456\0.456\0.456\0.456\0.456\0.455\0.455\0.455\0.455\0.455\0.454\0.454\0.454\0.454\0.454\0.453\0.453\0.453\0.453\0.453\0.452\0.452\0.452\0.452\0.452\0.451\0.451\0.451\0.451\0.451\0.451\0.45\0.45\0.45\0.45\0.45\0.449\0.449\0.449\0.449\0.449\0.448\0.448\0.448\0.448\0.448\0.447\0.447\0.447\0.447\0.447\0.447\0.446\0.446\0.446\0.446\0.446\0.445\0.445\0.445\0.445\0.445\0.445\0.444\0.444\0.444\0.444\0.444\0.443\0.443\0.443\0.443\0.443\0.443\0.442\0.442\0.442\0.442\0.442\0.441\0.441\0.441\0.441\0.441\0.441\0.44\0.44\0.44\0.44\0.44\0.44\0.439\0.439\0.439\0.439\0.439\0.438\0.438\0.438\0.438\0.438\0.438\0.437\0.437\0.437\0.437\0.437\0.437\0.436\0.436\0.436\0.436\0.436\0.435\0.435\0.435\0.435\0.435\0.435\0.434\0.434\0.434\0.434\0.434\0.434\0.433\0.433\0.433\0.433\0.433\0.433\0.432\0.432\0.432\0.432\0.432\0.432\0.431\0.431\0.431\0.431\0.431\0.431\0.43\0.43\0.43\0.43\0.43\0.43\0.429\0.429\0.429\0.429\0.429\0.429\0.429\0.428\0.428\0.428\0.428\0.428\0.428\0.427\0.427\0.427\0.427\0.427\0.427\0.426\0.426\0.426\0.426\0.426\0.426\0.426\0.425\0.425\0.425\0.425\0.425\0.425\0.424\0.424\0.424\0.424\0.424\0.424\0.423\0.423\0.423\0.423\0.423\0.423\0.423\0.422\0.422\0.422\0.422\0.422\0.422\0.422\0.421\0.421\0.421\0.421\0.421\0.421\0.421\0.42\0.42\0.42\0.42\0.42\0.42\0.419\0.419\0.419\0.419\0.419\0.419\0.419\0.418\0.418\0.418\0.418\0.418\0.418\0.418\0.417\0.417\0.417\0.417\0.417\0.417\0.417\0.416\0.416\0.416\0.416\0.416\0.416\0.416\0.415\0.415\0.415\0.415\0.415\0.415\0.415\0.415\0.414\0.414\0.414\0.414\0.414\0.414\0.414\0.413\0.413\0.413\0.413\0.413\0.413\0.413\0.412\0.412\0.412\0.412\0.412\0.412\0.412\0.412\0.411\0.411\0.411\0.411\0.411\0.411\0.411\0.411\0.41\0.41\0.41\0.41\0.41\0.41\0.41\0.409\0.409\0.409\0.409\0.409\0.409\0.409\0.409\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.406\0.406\0.406\0.406\0.406\0.406\0.406\0.406\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.405\0.404\0.404\0.404\0.404\0.404\0.404\0.404\0.404\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.403\0.402\0.402\0.402\0.402\0.402\0.402\0.402\0.402\0.402\0.401\0.401\0.401\0.401\0.401\0.401\0.401\0.401\0.401\0.4\0.4\0.4\0.4\0.4\0.4\0.4\0.4\0.4\0.399\0.399\0.399\0.399\0.399\0.399\0.399\0.399\0.399\0.398\0.398\0.398\0.398\0.398\0.398\0.398\0.398\0.398\0.397\0.397\0.397\0.397\0.397\0.397\0.397\0.397\0.397\0.397\0.396\0.396\0.396\0.396\0.396\0.396\0.396\0.396\0.396\0.395\0.395\0.395\0.395\0.395\0.395\0.395\0.395\0.395\0.395\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.394\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.393\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.391\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.39\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.389\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.388\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.387\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.386\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.385\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.384\0.383\0.383\0.383\0.383\0.383\0.383\0.383\0.383\0.383\0.383\0.383\0.383\0.383\0.382\0.382\0.382\0.382\0.382\0.382\0.382\0.382\0.382\0.382\0.382\0.382\0.382\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.381\0.38\0.38\0.38\0.38\0.38\0.38\0.38\0.38\0.38\0.38\0.38\0.38\0.38\0.38\0.379\0.379\0.379\0.379\0.379\0.379\0.379\0.379\0.379\0.379\0.379\0.379\0.379\0.379\0.378\0.378\0.378\0.378\0.378\0.378\0.378\0.378\0.378\0.378\0.378\0.378\0.378\0.378\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.377\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.376\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.37\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.355\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.354\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.353\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.352\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277</Y2Data>
   <Y2Name>Selection loss</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>10001</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="kpO3Jj" Title="Stochastic gradient descent results">
   <Caption Id="Y5LwBu">In the next  some important results from training are shown.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.263
0.277
10000
00:00:04
Maximum number of epochs
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Gtn96Y" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="wZAlMk" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="l06A3K" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="faQqbT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00341034\112.717\6.93004\9.03617
2.18611e-5\0.722546\0.0444233\0.0579242
0.00218611\72.2546\4.44233\5.79242</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="aROlKB" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="mQrkCt">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00521851\307.915\6.87899\14.2971
1.53485e-5\0.905632\0.0202323\0.0420502
0.00153485\90.5632\2.02323\4.20502</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="lG2DKU" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="J68KLy">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00136185\23.2792\4.54695\3.79704
5.49132e-6\0.0938678\0.0183345\0.0153106
0.000549132\9.38678\1.83345\1.53106</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="fhx0ka" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="oSr3Ts">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00148392\41.9771\5.50507\4.29881
2.061e-5\0.583015\0.0764593\0.0597057
0.002061\58.3015\7.64593\5.97057</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="dEGliM" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="TJGxUG">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00161147\13.9657\0.742356\0.976234
9.47924e-5\0.821511\0.043668\0.0574255
0.00947924\82.1511\4.3668\5.74255</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="zppEOp" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="NI1qte" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="mWsmT9" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="4Hd4iq" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="beKYZ7" Title="Optimization algorithm">The adaptive moment estimation method (Adam) is used here for training.
It is based on adaptive estimates of lower-order moments. </Text>
  <DoubleLineChart Id="zZMFU7" Title="Adaptive moment estimation errors history">
   <Caption Id="h4Vw7s">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.944364, and the final value after 10000 epochs is 0.231889.
The initial value of the selection error is 0.75566, and the final value after 10000 epochs is 0.302078.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661\662\663\664\665\666\667\668\669\670\671\672\673\674\675\676\677\678\679\680\681\682\683\684\685\686\687\688\689\690\691\692\693\694\695\696\697\698\699\700\701\702\703\704\705\706\707\708\709\710\711\712\713\714\715\716\717\718\719\720\721\722\723\724\725\726\727\728\729\730\731\732\733\734\735\736\737\738\739\740\741\742\743\744\745\746\747\748\749\750\751\752\753\754\755\756\757\758\759\760\761\762\763\764\765\766\767\768\769\770\771\772\773\774\775\776\777\778\779\780\781\782\783\784\785\786\787\788\789\790\791\792\793\794\795\796\797\798\799\800\801\802\803\804\805\806\807\808\809\810\811\812\813\814\815\816\817\818\819\820\821\822\823\824\825\826\827\828\829\830\831\832\833\834\835\836\837\838\839\840\841\842\843\844\845\846\847\848\849\850\851\852\853\854\855\856\857\858\859\860\861\862\863\864\865\866\867\868\869\870\871\872\873\874\875\876\877\878\879\880\881\882\883\884\885\886\887\888\889\890\891\892\893\894\895\896\897\898\899\900\901\902\903\904\905\906\907\908\909\910\911\912\913\914\915\916\917\918\919\920\921\922\923\924\925\926\927\928\929\930\931\932\933\934\935\936\937\938\939\940\941\942\943\944\945\946\947\948\949\950\951\952\953\954\955\956\957\958\959\960\961\962\963\964\965\966\967\968\969\970\971\972\973\974\975\976\977\978\979\980\981\982\983\984\985\986\987\988\989\990\991\992\993\994\995\996\997\998\999\1000\1001\1002\1003\1004\1005\1006\1007\1008\1009\1010\1011\1012\1013\1014\1015\1016\1017\1018\1019\1020\1021\1022\1023\1024\1025\1026\1027\1028\1029\1030\1031\1032\1033\1034\1035\1036\1037\1038\1039\1040\1041\1042\1043\1044\1045\1046\1047\1048\1049\1050\1051\1052\1053\1054\1055\1056\1057\1058\1059\1060\1061\1062\1063\1064\1065\1066\1067\1068\1069\1070\1071\1072\1073\1074\1075\1076\1077\1078\1079\1080\1081\1082\1083\1084\1085\1086\1087\1088\1089\1090\1091\1092\1093\1094\1095\1096\1097\1098\1099\1100\1101\1102\1103\1104\1105\1106\1107\1108\1109\1110\1111\1112\1113\1114\1115\1116\1117\1118\1119\1120\1121\1122\1123\1124\1125\1126\1127\1128\1129\1130\1131\1132\1133\1134\1135\1136\1137\1138\1139\1140\1141\1142\1143\1144\1145\1146\1147\1148\1149\1150\1151\1152\1153\1154\1155\1156\1157\1158\1159\1160\1161\1162\1163\1164\1165\1166\1167\1168\1169\1170\1171\1172\1173\1174\1175\1176\1177\1178\1179\1180\1181\1182\1183\1184\1185\1186\1187\1188\1189\1190\1191\1192\1193\1194\1195\1196\1197\1198\1199\1200\1201\1202\1203\1204\1205\1206\1207\1208\1209\1210\1211\1212\1213\1214\1215\1216\1217\1218\1219\1220\1221\1222\1223\1224\1225\1226\1227\1228\1229\1230\1231\1232\1233\1234\1235\1236\1237\1238\1239\1240\1241\1242\1243\1244\1245\1246\1247\1248\1249\1250\1251\1252\1253\1254\1255\1256\1257\1258\1259\1260\1261\1262\1263\1264\1265\1266\1267\1268\1269\1270\1271\1272\1273\1274\1275\1276\1277\1278\1279\1280\1281\1282\1283\1284\1285\1286\1287\1288\1289\1290\1291\1292\1293\1294\1295\1296\1297\1298\1299\1300\1301\1302\1303\1304\1305\1306\1307\1308\1309\1310\1311\1312\1313\1314\1315\1316\1317\1318\1319\1320\1321\1322\1323\1324\1325\1326\1327\1328\1329\1330\1331\1332\1333\1334\1335\1336\1337\1338\1339\1340\1341\1342\1343\1344\1345\1346\1347\1348\1349\1350\1351\1352\1353\1354\1355\1356\1357\1358\1359\1360\1361\1362\1363\1364\1365\1366\1367\1368\1369\1370\1371\1372\1373\1374\1375\1376\1377\1378\1379\1380\1381\1382\1383\1384\1385\1386\1387\1388\1389\1390\1391\1392\1393\1394\1395\1396\1397\1398\1399\1400\1401\1402\1403\1404\1405\1406\1407\1408\1409\1410\1411\1412\1413\1414\1415\1416\1417\1418\1419\1420\1421\1422\1423\1424\1425\1426\1427\1428\1429\1430\1431\1432\1433\1434\1435\1436\1437\1438\1439\1440\1441\1442\1443\1444\1445\1446\1447\1448\1449\1450\1451\1452\1453\1454\1455\1456\1457\1458\1459\1460\1461\1462\1463\1464\1465\1466\1467\1468\1469\1470\1471\1472\1473\1474\1475\1476\1477\1478\1479\1480\1481\1482\1483\1484\1485\1486\1487\1488\1489\1490\1491\1492\1493\1494\1495\1496\1497\1498\1499\1500\1501\1502\1503\1504\1505\1506\1507\1508\1509\1510\1511\1512\1513\1514\1515\1516\1517\1518\1519\1520\1521\1522\1523\1524\1525\1526\1527\1528\1529\1530\1531\1532\1533\1534\1535\1536\1537\1538\1539\1540\1541\1542\1543\1544\1545\1546\1547\1548\1549\1550\1551\1552\1553\1554\1555\1556\1557\1558\1559\1560\1561\1562\1563\1564\1565\1566\1567\1568\1569\1570\1571\1572\1573\1574\1575\1576\1577\1578\1579\1580\1581\1582\1583\1584\1585\1586\1587\1588\1589\1590\1591\1592\1593\1594\1595\1596\1597\1598\1599\1600\1601\1602\1603\1604\1605\1606\1607\1608\1609\1610\1611\1612\1613\1614\1615\1616\1617\1618\1619\1620\1621\1622\1623\1624\1625\1626\1627\1628\1629\1630\1631\1632\1633\1634\1635\1636\1637\1638\1639\1640\1641\1642\1643\1644\1645\1646\1647\1648\1649\1650\1651\1652\1653\1654\1655\1656\1657\1658\1659\1660\1661\1662\1663\1664\1665\1666\1667\1668\1669\1670\1671\1672\1673\1674\1675\1676\1677\1678\1679\1680\1681\1682\1683\1684\1685\1686\1687\1688\1689\1690\1691\1692\1693\1694\1695\1696\1697\1698\1699\1700\1701\1702\1703\1704\1705\1706\1707\1708\1709\1710\1711\1712\1713\1714\1715\1716\1717\1718\1719\1720\1721\1722\1723\1724\1725\1726\1727\1728\1729\1730\1731\1732\1733\1734\1735\1736\1737\1738\1739\1740\1741\1742\1743\1744\1745\1746\1747\1748\1749\1750\1751\1752\1753\1754\1755\1756\1757\1758\1759\1760\1761\1762\1763\1764\1765\1766\1767\1768\1769\1770\1771\1772\1773\1774\1775\1776\1777\1778\1779\1780\1781\1782\1783\1784\1785\1786\1787\1788\1789\1790\1791\1792\1793\1794\1795\1796\1797\1798\1799\1800\1801\1802\1803\1804\1805\1806\1807\1808\1809\1810\1811\1812\1813\1814\1815\1816\1817\1818\1819\1820\1821\1822\1823\1824\1825\1826\1827\1828\1829\1830\1831\1832\1833\1834\1835\1836\1837\1838\1839\1840\1841\1842\1843\1844\1845\1846\1847\1848\1849\1850\1851\1852\1853\1854\1855\1856\1857\1858\1859\1860\1861\1862\1863\1864\1865\1866\1867\1868\1869\1870\1871\1872\1873\1874\1875\1876\1877\1878\1879\1880\1881\1882\1883\1884\1885\1886\1887\1888\1889\1890\1891\1892\1893\1894\1895\1896\1897\1898\1899\1900\1901\1902\1903\1904\1905\1906\1907\1908\1909\1910\1911\1912\1913\1914\1915\1916\1917\1918\1919\1920\1921\1922\1923\1924\1925\1926\1927\1928\1929\1930\1931\1932\1933\1934\1935\1936\1937\1938\1939\1940\1941\1942\1943\1944\1945\1946\1947\1948\1949\1950\1951\1952\1953\1954\1955\1956\1957\1958\1959\1960\1961\1962\1963\1964\1965\1966\1967\1968\1969\1970\1971\1972\1973\1974\1975\1976\1977\1978\1979\1980\1981\1982\1983\1984\1985\1986\1987\1988\1989\1990\1991\1992\1993\1994\1995\1996\1997\1998\1999\2000\2001\2002\2003\2004\2005\2006\2007\2008\2009\2010\2011\2012\2013\2014\2015\2016\2017\2018\2019\2020\2021\2022\2023\2024\2025\2026\2027\2028\2029\2030\2031\2032\2033\2034\2035\2036\2037\2038\2039\2040\2041\2042\2043\2044\2045\2046\2047\2048\2049\2050\2051\2052\2053\2054\2055\2056\2057\2058\2059\2060\2061\2062\2063\2064\2065\2066\2067\2068\2069\2070\2071\2072\2073\2074\2075\2076\2077\2078\2079\2080\2081\2082\2083\2084\2085\2086\2087\2088\2089\2090\2091\2092\2093\2094\2095\2096\2097\2098\2099\2100\2101\2102\2103\2104\2105\2106\2107\2108\2109\2110\2111\2112\2113\2114\2115\2116\2117\2118\2119\2120\2121\2122\2123\2124\2125\2126\2127\2128\2129\2130\2131\2132\2133\2134\2135\2136\2137\2138\2139\2140\2141\2142\2143\2144\2145\2146\2147\2148\2149\2150\2151\2152\2153\2154\2155\2156\2157\2158\2159\2160\2161\2162\2163\2164\2165\2166\2167\2168\2169\2170\2171\2172\2173\2174\2175\2176\2177\2178\2179\2180\2181\2182\2183\2184\2185\2186\2187\2188\2189\2190\2191\2192\2193\2194\2195\2196\2197\2198\2199\2200\2201\2202\2203\2204\2205\2206\2207\2208\2209\2210\2211\2212\2213\2214\2215\2216\2217\2218\2219\2220\2221\2222\2223\2224\2225\2226\2227\2228\2229\2230\2231\2232\2233\2234\2235\2236\2237\2238\2239\2240\2241\2242\2243\2244\2245\2246\2247\2248\2249\2250\2251\2252\2253\2254\2255\2256\2257\2258\2259\2260\2261\2262\2263\2264\2265\2266\2267\2268\2269\2270\2271\2272\2273\2274\2275\2276\2277\2278\2279\2280\2281\2282\2283\2284\2285\2286\2287\2288\2289\2290\2291\2292\2293\2294\2295\2296\2297\2298\2299\2300\2301\2302\2303\2304\2305\2306\2307\2308\2309\2310\2311\2312\2313\2314\2315\2316\2317\2318\2319\2320\2321\2322\2323\2324\2325\2326\2327\2328\2329\2330\2331\2332\2333\2334\2335\2336\2337\2338\2339\2340\2341\2342\2343\2344\2345\2346\2347\2348\2349\2350\2351\2352\2353\2354\2355\2356\2357\2358\2359\2360\2361\2362\2363\2364\2365\2366\2367\2368\2369\2370\2371\2372\2373\2374\2375\2376\2377\2378\2379\2380\2381\2382\2383\2384\2385\2386\2387\2388\2389\2390\2391\2392\2393\2394\2395\2396\2397\2398\2399\2400\2401\2402\2403\2404\2405\2406\2407\2408\2409\2410\2411\2412\2413\2414\2415\2416\2417\2418\2419\2420\2421\2422\2423\2424\2425\2426\2427\2428\2429\2430\2431\2432\2433\2434\2435\2436\2437\2438\2439\2440\2441\2442\2443\2444\2445\2446\2447\2448\2449\2450\2451\2452\2453\2454\2455\2456\2457\2458\2459\2460\2461\2462\2463\2464\2465\2466\2467\2468\2469\2470\2471\2472\2473\2474\2475\2476\2477\2478\2479\2480\2481\2482\2483\2484\2485\2486\2487\2488\2489\2490\2491\2492\2493\2494\2495\2496\2497\2498\2499\2500\2501\2502\2503\2504\2505\2506\2507\2508\2509\2510\2511\2512\2513\2514\2515\2516\2517\2518\2519\2520\2521\2522\2523\2524\2525\2526\2527\2528\2529\2530\2531\2532\2533\2534\2535\2536\2537\2538\2539\2540\2541\2542\2543\2544\2545\2546\2547\2548\2549\2550\2551\2552\2553\2554\2555\2556\2557\2558\2559\2560\2561\2562\2563\2564\2565\2566\2567\2568\2569\2570\2571\2572\2573\2574\2575\2576\2577\2578\2579\2580\2581\2582\2583\2584\2585\2586\2587\2588\2589\2590\2591\2592\2593\2594\2595\2596\2597\2598\2599\2600\2601\2602\2603\2604\2605\2606\2607\2608\2609\2610\2611\2612\2613\2614\2615\2616\2617\2618\2619\2620\2621\2622\2623\2624\2625\2626\2627\2628\2629\2630\2631\2632\2633\2634\2635\2636\2637\2638\2639\2640\2641\2642\2643\2644\2645\2646\2647\2648\2649\2650\2651\2652\2653\2654\2655\2656\2657\2658\2659\2660\2661\2662\2663\2664\2665\2666\2667\2668\2669\2670\2671\2672\2673\2674\2675\2676\2677\2678\2679\2680\2681\2682\2683\2684\2685\2686\2687\2688\2689\2690\2691\2692\2693\2694\2695\2696\2697\2698\2699\2700\2701\2702\2703\2704\2705\2706\2707\2708\2709\2710\2711\2712\2713\2714\2715\2716\2717\2718\2719\2720\2721\2722\2723\2724\2725\2726\2727\2728\2729\2730\2731\2732\2733\2734\2735\2736\2737\2738\2739\2740\2741\2742\2743\2744\2745\2746\2747\2748\2749\2750\2751\2752\2753\2754\2755\2756\2757\2758\2759\2760\2761\2762\2763\2764\2765\2766\2767\2768\2769\2770\2771\2772\2773\2774\2775\2776\2777\2778\2779\2780\2781\2782\2783\2784\2785\2786\2787\2788\2789\2790\2791\2792\2793\2794\2795\2796\2797\2798\2799\2800\2801\2802\2803\2804\2805\2806\2807\2808\2809\2810\2811\2812\2813\2814\2815\2816\2817\2818\2819\2820\2821\2822\2823\2824\2825\2826\2827\2828\2829\2830\2831\2832\2833\2834\2835\2836\2837\2838\2839\2840\2841\2842\2843\2844\2845\2846\2847\2848\2849\2850\2851\2852\2853\2854\2855\2856\2857\2858\2859\2860\2861\2862\2863\2864\2865\2866\2867\2868\2869\2870\2871\2872\2873\2874\2875\2876\2877\2878\2879\2880\2881\2882\2883\2884\2885\2886\2887\2888\2889\2890\2891\2892\2893\2894\2895\2896\2897\2898\2899\2900\2901\2902\2903\2904\2905\2906\2907\2908\2909\2910\2911\2912\2913\2914\2915\2916\2917\2918\2919\2920\2921\2922\2923\2924\2925\2926\2927\2928\2929\2930\2931\2932\2933\2934\2935\2936\2937\2938\2939\2940\2941\2942\2943\2944\2945\2946\2947\2948\2949\2950\2951\2952\2953\2954\2955\2956\2957\2958\2959\2960\2961\2962\2963\2964\2965\2966\2967\2968\2969\2970\2971\2972\2973\2974\2975\2976\2977\2978\2979\2980\2981\2982\2983\2984\2985\2986\2987\2988\2989\2990\2991\2992\2993\2994\2995\2996\2997\2998\2999\3000\3001\3002\3003\3004\3005\3006\3007\3008\3009\3010\3011\3012\3013\3014\3015\3016\3017\3018\3019\3020\3021\3022\3023\3024\3025\3026\3027\3028\3029\3030\3031\3032\3033\3034\3035\3036\3037\3038\3039\3040\3041\3042\3043\3044\3045\3046\3047\3048\3049\3050\3051\3052\3053\3054\3055\3056\3057\3058\3059\3060\3061\3062\3063\3064\3065\3066\3067\3068\3069\3070\3071\3072\3073\3074\3075\3076\3077\3078\3079\3080\3081\3082\3083\3084\3085\3086\3087\3088\3089\3090\3091\3092\3093\3094\3095\3096\3097\3098\3099\3100\3101\3102\3103\3104\3105\3106\3107\3108\3109\3110\3111\3112\3113\3114\3115\3116\3117\3118\3119\3120\3121\3122\3123\3124\3125\3126\3127\3128\3129\3130\3131\3132\3133\3134\3135\3136\3137\3138\3139\3140\3141\3142\3143\3144\3145\3146\3147\3148\3149\3150\3151\3152\3153\3154\3155\3156\3157\3158\3159\3160\3161\3162\3163\3164\3165\3166\3167\3168\3169\3170\3171\3172\3173\3174\3175\3176\3177\3178\3179\3180\3181\3182\3183\3184\3185\3186\3187\3188\3189\3190\3191\3192\3193\3194\3195\3196\3197\3198\3199\3200\3201\3202\3203\3204\3205\3206\3207\3208\3209\3210\3211\3212\3213\3214\3215\3216\3217\3218\3219\3220\3221\3222\3223\3224\3225\3226\3227\3228\3229\3230\3231\3232\3233\3234\3235\3236\3237\3238\3239\3240\3241\3242\3243\3244\3245\3246\3247\3248\3249\3250\3251\3252\3253\3254\3255\3256\3257\3258\3259\3260\3261\3262\3263\3264\3265\3266\3267\3268\3269\3270\3271\3272\3273\3274\3275\3276\3277\3278\3279\3280\3281\3282\3283\3284\3285\3286\3287\3288\3289\3290\3291\3292\3293\3294\3295\3296\3297\3298\3299\3300\3301\3302\3303\3304\3305\3306\3307\3308\3309\3310\3311\3312\3313\3314\3315\3316\3317\3318\3319\3320\3321\3322\3323\3324\3325\3326\3327\3328\3329\3330\3331\3332\3333\3334\3335\3336\3337\3338\3339\3340\3341\3342\3343\3344\3345\3346\3347\3348\3349\3350\3351\3352\3353\3354\3355\3356\3357\3358\3359\3360\3361\3362\3363\3364\3365\3366\3367\3368\3369\3370\3371\3372\3373\3374\3375\3376\3377\3378\3379\3380\3381\3382\3383\3384\3385\3386\3387\3388\3389\3390\3391\3392\3393\3394\3395\3396\3397\3398\3399\3400\3401\3402\3403\3404\3405\3406\3407\3408\3409\3410\3411\3412\3413\3414\3415\3416\3417\3418\3419\3420\3421\3422\3423\3424\3425\3426\3427\3428\3429\3430\3431\3432\3433\3434\3435\3436\3437\3438\3439\3440\3441\3442\3443\3444\3445\3446\3447\3448\3449\3450\3451\3452\3453\3454\3455\3456\3457\3458\3459\3460\3461\3462\3463\3464\3465\3466\3467\3468\3469\3470\3471\3472\3473\3474\3475\3476\3477\3478\3479\3480\3481\3482\3483\3484\3485\3486\3487\3488\3489\3490\3491\3492\3493\3494\3495\3496\3497\3498\3499\3500\3501\3502\3503\3504\3505\3506\3507\3508\3509\3510\3511\3512\3513\3514\3515\3516\3517\3518\3519\3520\3521\3522\3523\3524\3525\3526\3527\3528\3529\3530\3531\3532\3533\3534\3535\3536\3537\3538\3539\3540\3541\3542\3543\3544\3545\3546\3547\3548\3549\3550\3551\3552\3553\3554\3555\3556\3557\3558\3559\3560\3561\3562\3563\3564\3565\3566\3567\3568\3569\3570\3571\3572\3573\3574\3575\3576\3577\3578\3579\3580\3581\3582\3583\3584\3585\3586\3587\3588\3589\3590\3591\3592\3593\3594\3595\3596\3597\3598\3599\3600\3601\3602\3603\3604\3605\3606\3607\3608\3609\3610\3611\3612\3613\3614\3615\3616\3617\3618\3619\3620\3621\3622\3623\3624\3625\3626\3627\3628\3629\3630\3631\3632\3633\3634\3635\3636\3637\3638\3639\3640\3641\3642\3643\3644\3645\3646\3647\3648\3649\3650\3651\3652\3653\3654\3655\3656\3657\3658\3659\3660\3661\3662\3663\3664\3665\3666\3667\3668\3669\3670\3671\3672\3673\3674\3675\3676\3677\3678\3679\3680\3681\3682\3683\3684\3685\3686\3687\3688\3689\3690\3691\3692\3693\3694\3695\3696\3697\3698\3699\3700\3701\3702\3703\3704\3705\3706\3707\3708\3709\3710\3711\3712\3713\3714\3715\3716\3717\3718\3719\3720\3721\3722\3723\3724\3725\3726\3727\3728\3729\3730\3731\3732\3733\3734\3735\3736\3737\3738\3739\3740\3741\3742\3743\3744\3745\3746\3747\3748\3749\3750\3751\3752\3753\3754\3755\3756\3757\3758\3759\3760\3761\3762\3763\3764\3765\3766\3767\3768\3769\3770\3771\3772\3773\3774\3775\3776\3777\3778\3779\3780\3781\3782\3783\3784\3785\3786\3787\3788\3789\3790\3791\3792\3793\3794\3795\3796\3797\3798\3799\3800\3801\3802\3803\3804\3805\3806\3807\3808\3809\3810\3811\3812\3813\3814\3815\3816\3817\3818\3819\3820\3821\3822\3823\3824\3825\3826\3827\3828\3829\3830\3831\3832\3833\3834\3835\3836\3837\3838\3839\3840\3841\3842\3843\3844\3845\3846\3847\3848\3849\3850\3851\3852\3853\3854\3855\3856\3857\3858\3859\3860\3861\3862\3863\3864\3865\3866\3867\3868\3869\3870\3871\3872\3873\3874\3875\3876\3877\3878\3879\3880\3881\3882\3883\3884\3885\3886\3887\3888\3889\3890\3891\3892\3893\3894\3895\3896\3897\3898\3899\3900\3901\3902\3903\3904\3905\3906\3907\3908\3909\3910\3911\3912\3913\3914\3915\3916\3917\3918\3919\3920\3921\3922\3923\3924\3925\3926\3927\3928\3929\3930\3931\3932\3933\3934\3935\3936\3937\3938\3939\3940\3941\3942\3943\3944\3945\3946\3947\3948\3949\3950\3951\3952\3953\3954\3955\3956\3957\3958\3959\3960\3961\3962\3963\3964\3965\3966\3967\3968\3969\3970\3971\3972\3973\3974\3975\3976\3977\3978\3979\3980\3981\3982\3983\3984\3985\3986\3987\3988\3989\3990\3991\3992\3993\3994\3995\3996\3997\3998\3999\4000\4001\4002\4003\4004\4005\4006\4007\4008\4009\4010\4011\4012\4013\4014\4015\4016\4017\4018\4019\4020\4021\4022\4023\4024\4025\4026\4027\4028\4029\4030\4031\4032\4033\4034\4035\4036\4037\4038\4039\4040\4041\4042\4043\4044\4045\4046\4047\4048\4049\4050\4051\4052\4053\4054\4055\4056\4057\4058\4059\4060\4061\4062\4063\4064\4065\4066\4067\4068\4069\4070\4071\4072\4073\4074\4075\4076\4077\4078\4079\4080\4081\4082\4083\4084\4085\4086\4087\4088\4089\4090\4091\4092\4093\4094\4095\4096\4097\4098\4099\4100\4101\4102\4103\4104\4105\4106\4107\4108\4109\4110\4111\4112\4113\4114\4115\4116\4117\4118\4119\4120\4121\4122\4123\4124\4125\4126\4127\4128\4129\4130\4131\4132\4133\4134\4135\4136\4137\4138\4139\4140\4141\4142\4143\4144\4145\4146\4147\4148\4149\4150\4151\4152\4153\4154\4155\4156\4157\4158\4159\4160\4161\4162\4163\4164\4165\4166\4167\4168\4169\4170\4171\4172\4173\4174\4175\4176\4177\4178\4179\4180\4181\4182\4183\4184\4185\4186\4187\4188\4189\4190\4191\4192\4193\4194\4195\4196\4197\4198\4199\4200\4201\4202\4203\4204\4205\4206\4207\4208\4209\4210\4211\4212\4213\4214\4215\4216\4217\4218\4219\4220\4221\4222\4223\4224\4225\4226\4227\4228\4229\4230\4231\4232\4233\4234\4235\4236\4237\4238\4239\4240\4241\4242\4243\4244\4245\4246\4247\4248\4249\4250\4251\4252\4253\4254\4255\4256\4257\4258\4259\4260\4261\4262\4263\4264\4265\4266\4267\4268\4269\4270\4271\4272\4273\4274\4275\4276\4277\4278\4279\4280\4281\4282\4283\4284\4285\4286\4287\4288\4289\4290\4291\4292\4293\4294\4295\4296\4297\4298\4299\4300\4301\4302\4303\4304\4305\4306\4307\4308\4309\4310\4311\4312\4313\4314\4315\4316\4317\4318\4319\4320\4321\4322\4323\4324\4325\4326\4327\4328\4329\4330\4331\4332\4333\4334\4335\4336\4337\4338\4339\4340\4341\4342\4343\4344\4345\4346\4347\4348\4349\4350\4351\4352\4353\4354\4355\4356\4357\4358\4359\4360\4361\4362\4363\4364\4365\4366\4367\4368\4369\4370\4371\4372\4373\4374\4375\4376\4377\4378\4379\4380\4381\4382\4383\4384\4385\4386\4387\4388\4389\4390\4391\4392\4393\4394\4395\4396\4397\4398\4399\4400\4401\4402\4403\4404\4405\4406\4407\4408\4409\4410\4411\4412\4413\4414\4415\4416\4417\4418\4419\4420\4421\4422\4423\4424\4425\4426\4427\4428\4429\4430\4431\4432\4433\4434\4435\4436\4437\4438\4439\4440\4441\4442\4443\4444\4445\4446\4447\4448\4449\4450\4451\4452\4453\4454\4455\4456\4457\4458\4459\4460\4461\4462\4463\4464\4465\4466\4467\4468\4469\4470\4471\4472\4473\4474\4475\4476\4477\4478\4479\4480\4481\4482\4483\4484\4485\4486\4487\4488\4489\4490\4491\4492\4493\4494\4495\4496\4497\4498\4499\4500\4501\4502\4503\4504\4505\4506\4507\4508\4509\4510\4511\4512\4513\4514\4515\4516\4517\4518\4519\4520\4521\4522\4523\4524\4525\4526\4527\4528\4529\4530\4531\4532\4533\4534\4535\4536\4537\4538\4539\4540\4541\4542\4543\4544\4545\4546\4547\4548\4549\4550\4551\4552\4553\4554\4555\4556\4557\4558\4559\4560\4561\4562\4563\4564\4565\4566\4567\4568\4569\4570\4571\4572\4573\4574\4575\4576\4577\4578\4579\4580\4581\4582\4583\4584\4585\4586\4587\4588\4589\4590\4591\4592\4593\4594\4595\4596\4597\4598\4599\4600\4601\4602\4603\4604\4605\4606\4607\4608\4609\4610\4611\4612\4613\4614\4615\4616\4617\4618\4619\4620\4621\4622\4623\4624\4625\4626\4627\4628\4629\4630\4631\4632\4633\4634\4635\4636\4637\4638\4639\4640\4641\4642\4643\4644\4645\4646\4647\4648\4649\4650\4651\4652\4653\4654\4655\4656\4657\4658\4659\4660\4661\4662\4663\4664\4665\4666\4667\4668\4669\4670\4671\4672\4673\4674\4675\4676\4677\4678\4679\4680\4681\4682\4683\4684\4685\4686\4687\4688\4689\4690\4691\4692\4693\4694\4695\4696\4697\4698\4699\4700\4701\4702\4703\4704\4705\4706\4707\4708\4709\4710\4711\4712\4713\4714\4715\4716\4717\4718\4719\4720\4721\4722\4723\4724\4725\4726\4727\4728\4729\4730\4731\4732\4733\4734\4735\4736\4737\4738\4739\4740\4741\4742\4743\4744\4745\4746\4747\4748\4749\4750\4751\4752\4753\4754\4755\4756\4757\4758\4759\4760\4761\4762\4763\4764\4765\4766\4767\4768\4769\4770\4771\4772\4773\4774\4775\4776\4777\4778\4779\4780\4781\4782\4783\4784\4785\4786\4787\4788\4789\4790\4791\4792\4793\4794\4795\4796\4797\4798\4799\4800\4801\4802\4803\4804\4805\4806\4807\4808\4809\4810\4811\4812\4813\4814\4815\4816\4817\4818\4819\4820\4821\4822\4823\4824\4825\4826\4827\4828\4829\4830\4831\4832\4833\4834\4835\4836\4837\4838\4839\4840\4841\4842\4843\4844\4845\4846\4847\4848\4849\4850\4851\4852\4853\4854\4855\4856\4857\4858\4859\4860\4861\4862\4863\4864\4865\4866\4867\4868\4869\4870\4871\4872\4873\4874\4875\4876\4877\4878\4879\4880\4881\4882\4883\4884\4885\4886\4887\4888\4889\4890\4891\4892\4893\4894\4895\4896\4897\4898\4899\4900\4901\4902\4903\4904\4905\4906\4907\4908\4909\4910\4911\4912\4913\4914\4915\4916\4917\4918\4919\4920\4921\4922\4923\4924\4925\4926\4927\4928\4929\4930\4931\4932\4933\4934\4935\4936\4937\4938\4939\4940\4941\4942\4943\4944\4945\4946\4947\4948\4949\4950\4951\4952\4953\4954\4955\4956\4957\4958\4959\4960\4961\4962\4963\4964\4965\4966\4967\4968\4969\4970\4971\4972\4973\4974\4975\4976\4977\4978\4979\4980\4981\4982\4983\4984\4985\4986\4987\4988\4989\4990\4991\4992\4993\4994\4995\4996\4997\4998\4999\5000\5001\5002\5003\5004\5005\5006\5007\5008\5009\5010\5011\5012\5013\5014\5015\5016\5017\5018\5019\5020\5021\5022\5023\5024\5025\5026\5027\5028\5029\5030\5031\5032\5033\5034\5035\5036\5037\5038\5039\5040\5041\5042\5043\5044\5045\5046\5047\5048\5049\5050\5051\5052\5053\5054\5055\5056\5057\5058\5059\5060\5061\5062\5063\5064\5065\5066\5067\5068\5069\5070\5071\5072\5073\5074\5075\5076\5077\5078\5079\5080\5081\5082\5083\5084\5085\5086\5087\5088\5089\5090\5091\5092\5093\5094\5095\5096\5097\5098\5099\5100\5101\5102\5103\5104\5105\5106\5107\5108\5109\5110\5111\5112\5113\5114\5115\5116\5117\5118\5119\5120\5121\5122\5123\5124\5125\5126\5127\5128\5129\5130\5131\5132\5133\5134\5135\5136\5137\5138\5139\5140\5141\5142\5143\5144\5145\5146\5147\5148\5149\5150\5151\5152\5153\5154\5155\5156\5157\5158\5159\5160\5161\5162\5163\5164\5165\5166\5167\5168\5169\5170\5171\5172\5173\5174\5175\5176\5177\5178\5179\5180\5181\5182\5183\5184\5185\5186\5187\5188\5189\5190\5191\5192\5193\5194\5195\5196\5197\5198\5199\5200\5201\5202\5203\5204\5205\5206\5207\5208\5209\5210\5211\5212\5213\5214\5215\5216\5217\5218\5219\5220\5221\5222\5223\5224\5225\5226\5227\5228\5229\5230\5231\5232\5233\5234\5235\5236\5237\5238\5239\5240\5241\5242\5243\5244\5245\5246\5247\5248\5249\5250\5251\5252\5253\5254\5255\5256\5257\5258\5259\5260\5261\5262\5263\5264\5265\5266\5267\5268\5269\5270\5271\5272\5273\5274\5275\5276\5277\5278\5279\5280\5281\5282\5283\5284\5285\5286\5287\5288\5289\5290\5291\5292\5293\5294\5295\5296\5297\5298\5299\5300\5301\5302\5303\5304\5305\5306\5307\5308\5309\5310\5311\5312\5313\5314\5315\5316\5317\5318\5319\5320\5321\5322\5323\5324\5325\5326\5327\5328\5329\5330\5331\5332\5333\5334\5335\5336\5337\5338\5339\5340\5341\5342\5343\5344\5345\5346\5347\5348\5349\5350\5351\5352\5353\5354\5355\5356\5357\5358\5359\5360\5361\5362\5363\5364\5365\5366\5367\5368\5369\5370\5371\5372\5373\5374\5375\5376\5377\5378\5379\5380\5381\5382\5383\5384\5385\5386\5387\5388\5389\5390\5391\5392\5393\5394\5395\5396\5397\5398\5399\5400\5401\5402\5403\5404\5405\5406\5407\5408\5409\5410\5411\5412\5413\5414\5415\5416\5417\5418\5419\5420\5421\5422\5423\5424\5425\5426\5427\5428\5429\5430\5431\5432\5433\5434\5435\5436\5437\5438\5439\5440\5441\5442\5443\5444\5445\5446\5447\5448\5449\5450\5451\5452\5453\5454\5455\5456\5457\5458\5459\5460\5461\5462\5463\5464\5465\5466\5467\5468\5469\5470\5471\5472\5473\5474\5475\5476\5477\5478\5479\5480\5481\5482\5483\5484\5485\5486\5487\5488\5489\5490\5491\5492\5493\5494\5495\5496\5497\5498\5499\5500\5501\5502\5503\5504\5505\5506\5507\5508\5509\5510\5511\5512\5513\5514\5515\5516\5517\5518\5519\5520\5521\5522\5523\5524\5525\5526\5527\5528\5529\5530\5531\5532\5533\5534\5535\5536\5537\5538\5539\5540\5541\5542\5543\5544\5545\5546\5547\5548\5549\5550\5551\5552\5553\5554\5555\5556\5557\5558\5559\5560\5561\5562\5563\5564\5565\5566\5567\5568\5569\5570\5571\5572\5573\5574\5575\5576\5577\5578\5579\5580\5581\5582\5583\5584\5585\5586\5587\5588\5589\5590\5591\5592\5593\5594\5595\5596\5597\5598\5599\5600\5601\5602\5603\5604\5605\5606\5607\5608\5609\5610\5611\5612\5613\5614\5615\5616\5617\5618\5619\5620\5621\5622\5623\5624\5625\5626\5627\5628\5629\5630\5631\5632\5633\5634\5635\5636\5637\5638\5639\5640\5641\5642\5643\5644\5645\5646\5647\5648\5649\5650\5651\5652\5653\5654\5655\5656\5657\5658\5659\5660\5661\5662\5663\5664\5665\5666\5667\5668\5669\5670\5671\5672\5673\5674\5675\5676\5677\5678\5679\5680\5681\5682\5683\5684\5685\5686\5687\5688\5689\5690\5691\5692\5693\5694\5695\5696\5697\5698\5699\5700\5701\5702\5703\5704\5705\5706\5707\5708\5709\5710\5711\5712\5713\5714\5715\5716\5717\5718\5719\5720\5721\5722\5723\5724\5725\5726\5727\5728\5729\5730\5731\5732\5733\5734\5735\5736\5737\5738\5739\5740\5741\5742\5743\5744\5745\5746\5747\5748\5749\5750\5751\5752\5753\5754\5755\5756\5757\5758\5759\5760\5761\5762\5763\5764\5765\5766\5767\5768\5769\5770\5771\5772\5773\5774\5775\5776\5777\5778\5779\5780\5781\5782\5783\5784\5785\5786\5787\5788\5789\5790\5791\5792\5793\5794\5795\5796\5797\5798\5799\5800\5801\5802\5803\5804\5805\5806\5807\5808\5809\5810\5811\5812\5813\5814\5815\5816\5817\5818\5819\5820\5821\5822\5823\5824\5825\5826\5827\5828\5829\5830\5831\5832\5833\5834\5835\5836\5837\5838\5839\5840\5841\5842\5843\5844\5845\5846\5847\5848\5849\5850\5851\5852\5853\5854\5855\5856\5857\5858\5859\5860\5861\5862\5863\5864\5865\5866\5867\5868\5869\5870\5871\5872\5873\5874\5875\5876\5877\5878\5879\5880\5881\5882\5883\5884\5885\5886\5887\5888\5889\5890\5891\5892\5893\5894\5895\5896\5897\5898\5899\5900\5901\5902\5903\5904\5905\5906\5907\5908\5909\5910\5911\5912\5913\5914\5915\5916\5917\5918\5919\5920\5921\5922\5923\5924\5925\5926\5927\5928\5929\5930\5931\5932\5933\5934\5935\5936\5937\5938\5939\5940\5941\5942\5943\5944\5945\5946\5947\5948\5949\5950\5951\5952\5953\5954\5955\5956\5957\5958\5959\5960\5961\5962\5963\5964\5965\5966\5967\5968\5969\5970\5971\5972\5973\5974\5975\5976\5977\5978\5979\5980\5981\5982\5983\5984\5985\5986\5987\5988\5989\5990\5991\5992\5993\5994\5995\5996\5997\5998\5999\6000\6001\6002\6003\6004\6005\6006\6007\6008\6009\6010\6011\6012\6013\6014\6015\6016\6017\6018\6019\6020\6021\6022\6023\6024\6025\6026\6027\6028\6029\6030\6031\6032\6033\6034\6035\6036\6037\6038\6039\6040\6041\6042\6043\6044\6045\6046\6047\6048\6049\6050\6051\6052\6053\6054\6055\6056\6057\6058\6059\6060\6061\6062\6063\6064\6065\6066\6067\6068\6069\6070\6071\6072\6073\6074\6075\6076\6077\6078\6079\6080\6081\6082\6083\6084\6085\6086\6087\6088\6089\6090\6091\6092\6093\6094\6095\6096\6097\6098\6099\6100\6101\6102\6103\6104\6105\6106\6107\6108\6109\6110\6111\6112\6113\6114\6115\6116\6117\6118\6119\6120\6121\6122\6123\6124\6125\6126\6127\6128\6129\6130\6131\6132\6133\6134\6135\6136\6137\6138\6139\6140\6141\6142\6143\6144\6145\6146\6147\6148\6149\6150\6151\6152\6153\6154\6155\6156\6157\6158\6159\6160\6161\6162\6163\6164\6165\6166\6167\6168\6169\6170\6171\6172\6173\6174\6175\6176\6177\6178\6179\6180\6181\6182\6183\6184\6185\6186\6187\6188\6189\6190\6191\6192\6193\6194\6195\6196\6197\6198\6199\6200\6201\6202\6203\6204\6205\6206\6207\6208\6209\6210\6211\6212\6213\6214\6215\6216\6217\6218\6219\6220\6221\6222\6223\6224\6225\6226\6227\6228\6229\6230\6231\6232\6233\6234\6235\6236\6237\6238\6239\6240\6241\6242\6243\6244\6245\6246\6247\6248\6249\6250\6251\6252\6253\6254\6255\6256\6257\6258\6259\6260\6261\6262\6263\6264\6265\6266\6267\6268\6269\6270\6271\6272\6273\6274\6275\6276\6277\6278\6279\6280\6281\6282\6283\6284\6285\6286\6287\6288\6289\6290\6291\6292\6293\6294\6295\6296\6297\6298\6299\6300\6301\6302\6303\6304\6305\6306\6307\6308\6309\6310\6311\6312\6313\6314\6315\6316\6317\6318\6319\6320\6321\6322\6323\6324\6325\6326\6327\6328\6329\6330\6331\6332\6333\6334\6335\6336\6337\6338\6339\6340\6341\6342\6343\6344\6345\6346\6347\6348\6349\6350\6351\6352\6353\6354\6355\6356\6357\6358\6359\6360\6361\6362\6363\6364\6365\6366\6367\6368\6369\6370\6371\6372\6373\6374\6375\6376\6377\6378\6379\6380\6381\6382\6383\6384\6385\6386\6387\6388\6389\6390\6391\6392\6393\6394\6395\6396\6397\6398\6399\6400\6401\6402\6403\6404\6405\6406\6407\6408\6409\6410\6411\6412\6413\6414\6415\6416\6417\6418\6419\6420\6421\6422\6423\6424\6425\6426\6427\6428\6429\6430\6431\6432\6433\6434\6435\6436\6437\6438\6439\6440\6441\6442\6443\6444\6445\6446\6447\6448\6449\6450\6451\6452\6453\6454\6455\6456\6457\6458\6459\6460\6461\6462\6463\6464\6465\6466\6467\6468\6469\6470\6471\6472\6473\6474\6475\6476\6477\6478\6479\6480\6481\6482\6483\6484\6485\6486\6487\6488\6489\6490\6491\6492\6493\6494\6495\6496\6497\6498\6499\6500\6501\6502\6503\6504\6505\6506\6507\6508\6509\6510\6511\6512\6513\6514\6515\6516\6517\6518\6519\6520\6521\6522\6523\6524\6525\6526\6527\6528\6529\6530\6531\6532\6533\6534\6535\6536\6537\6538\6539\6540\6541\6542\6543\6544\6545\6546\6547\6548\6549\6550\6551\6552\6553\6554\6555\6556\6557\6558\6559\6560\6561\6562\6563\6564\6565\6566\6567\6568\6569\6570\6571\6572\6573\6574\6575\6576\6577\6578\6579\6580\6581\6582\6583\6584\6585\6586\6587\6588\6589\6590\6591\6592\6593\6594\6595\6596\6597\6598\6599\6600\6601\6602\6603\6604\6605\6606\6607\6608\6609\6610\6611\6612\6613\6614\6615\6616\6617\6618\6619\6620\6621\6622\6623\6624\6625\6626\6627\6628\6629\6630\6631\6632\6633\6634\6635\6636\6637\6638\6639\6640\6641\6642\6643\6644\6645\6646\6647\6648\6649\6650\6651\6652\6653\6654\6655\6656\6657\6658\6659\6660\6661\6662\6663\6664\6665\6666\6667\6668\6669\6670\6671\6672\6673\6674\6675\6676\6677\6678\6679\6680\6681\6682\6683\6684\6685\6686\6687\6688\6689\6690\6691\6692\6693\6694\6695\6696\6697\6698\6699\6700\6701\6702\6703\6704\6705\6706\6707\6708\6709\6710\6711\6712\6713\6714\6715\6716\6717\6718\6719\6720\6721\6722\6723\6724\6725\6726\6727\6728\6729\6730\6731\6732\6733\6734\6735\6736\6737\6738\6739\6740\6741\6742\6743\6744\6745\6746\6747\6748\6749\6750\6751\6752\6753\6754\6755\6756\6757\6758\6759\6760\6761\6762\6763\6764\6765\6766\6767\6768\6769\6770\6771\6772\6773\6774\6775\6776\6777\6778\6779\6780\6781\6782\6783\6784\6785\6786\6787\6788\6789\6790\6791\6792\6793\6794\6795\6796\6797\6798\6799\6800\6801\6802\6803\6804\6805\6806\6807\6808\6809\6810\6811\6812\6813\6814\6815\6816\6817\6818\6819\6820\6821\6822\6823\6824\6825\6826\6827\6828\6829\6830\6831\6832\6833\6834\6835\6836\6837\6838\6839\6840\6841\6842\6843\6844\6845\6846\6847\6848\6849\6850\6851\6852\6853\6854\6855\6856\6857\6858\6859\6860\6861\6862\6863\6864\6865\6866\6867\6868\6869\6870\6871\6872\6873\6874\6875\6876\6877\6878\6879\6880\6881\6882\6883\6884\6885\6886\6887\6888\6889\6890\6891\6892\6893\6894\6895\6896\6897\6898\6899\6900\6901\6902\6903\6904\6905\6906\6907\6908\6909\6910\6911\6912\6913\6914\6915\6916\6917\6918\6919\6920\6921\6922\6923\6924\6925\6926\6927\6928\6929\6930\6931\6932\6933\6934\6935\6936\6937\6938\6939\6940\6941\6942\6943\6944\6945\6946\6947\6948\6949\6950\6951\6952\6953\6954\6955\6956\6957\6958\6959\6960\6961\6962\6963\6964\6965\6966\6967\6968\6969\6970\6971\6972\6973\6974\6975\6976\6977\6978\6979\6980\6981\6982\6983\6984\6985\6986\6987\6988\6989\6990\6991\6992\6993\6994\6995\6996\6997\6998\6999\7000\7001\7002\7003\7004\7005\7006\7007\7008\7009\7010\7011\7012\7013\7014\7015\7016\7017\7018\7019\7020\7021\7022\7023\7024\7025\7026\7027\7028\7029\7030\7031\7032\7033\7034\7035\7036\7037\7038\7039\7040\7041\7042\7043\7044\7045\7046\7047\7048\7049\7050\7051\7052\7053\7054\7055\7056\7057\7058\7059\7060\7061\7062\7063\7064\7065\7066\7067\7068\7069\7070\7071\7072\7073\7074\7075\7076\7077\7078\7079\7080\7081\7082\7083\7084\7085\7086\7087\7088\7089\7090\7091\7092\7093\7094\7095\7096\7097\7098\7099\7100\7101\7102\7103\7104\7105\7106\7107\7108\7109\7110\7111\7112\7113\7114\7115\7116\7117\7118\7119\7120\7121\7122\7123\7124\7125\7126\7127\7128\7129\7130\7131\7132\7133\7134\7135\7136\7137\7138\7139\7140\7141\7142\7143\7144\7145\7146\7147\7148\7149\7150\7151\7152\7153\7154\7155\7156\7157\7158\7159\7160\7161\7162\7163\7164\7165\7166\7167\7168\7169\7170\7171\7172\7173\7174\7175\7176\7177\7178\7179\7180\7181\7182\7183\7184\7185\7186\7187\7188\7189\7190\7191\7192\7193\7194\7195\7196\7197\7198\7199\7200\7201\7202\7203\7204\7205\7206\7207\7208\7209\7210\7211\7212\7213\7214\7215\7216\7217\7218\7219\7220\7221\7222\7223\7224\7225\7226\7227\7228\7229\7230\7231\7232\7233\7234\7235\7236\7237\7238\7239\7240\7241\7242\7243\7244\7245\7246\7247\7248\7249\7250\7251\7252\7253\7254\7255\7256\7257\7258\7259\7260\7261\7262\7263\7264\7265\7266\7267\7268\7269\7270\7271\7272\7273\7274\7275\7276\7277\7278\7279\7280\7281\7282\7283\7284\7285\7286\7287\7288\7289\7290\7291\7292\7293\7294\7295\7296\7297\7298\7299\7300\7301\7302\7303\7304\7305\7306\7307\7308\7309\7310\7311\7312\7313\7314\7315\7316\7317\7318\7319\7320\7321\7322\7323\7324\7325\7326\7327\7328\7329\7330\7331\7332\7333\7334\7335\7336\7337\7338\7339\7340\7341\7342\7343\7344\7345\7346\7347\7348\7349\7350\7351\7352\7353\7354\7355\7356\7357\7358\7359\7360\7361\7362\7363\7364\7365\7366\7367\7368\7369\7370\7371\7372\7373\7374\7375\7376\7377\7378\7379\7380\7381\7382\7383\7384\7385\7386\7387\7388\7389\7390\7391\7392\7393\7394\7395\7396\7397\7398\7399\7400\7401\7402\7403\7404\7405\7406\7407\7408\7409\7410\7411\7412\7413\7414\7415\7416\7417\7418\7419\7420\7421\7422\7423\7424\7425\7426\7427\7428\7429\7430\7431\7432\7433\7434\7435\7436\7437\7438\7439\7440\7441\7442\7443\7444\7445\7446\7447\7448\7449\7450\7451\7452\7453\7454\7455\7456\7457\7458\7459\7460\7461\7462\7463\7464\7465\7466\7467\7468\7469\7470\7471\7472\7473\7474\7475\7476\7477\7478\7479\7480\7481\7482\7483\7484\7485\7486\7487\7488\7489\7490\7491\7492\7493\7494\7495\7496\7497\7498\7499\7500\7501\7502\7503\7504\7505\7506\7507\7508\7509\7510\7511\7512\7513\7514\7515\7516\7517\7518\7519\7520\7521\7522\7523\7524\7525\7526\7527\7528\7529\7530\7531\7532\7533\7534\7535\7536\7537\7538\7539\7540\7541\7542\7543\7544\7545\7546\7547\7548\7549\7550\7551\7552\7553\7554\7555\7556\7557\7558\7559\7560\7561\7562\7563\7564\7565\7566\7567\7568\7569\7570\7571\7572\7573\7574\7575\7576\7577\7578\7579\7580\7581\7582\7583\7584\7585\7586\7587\7588\7589\7590\7591\7592\7593\7594\7595\7596\7597\7598\7599\7600\7601\7602\7603\7604\7605\7606\7607\7608\7609\7610\7611\7612\7613\7614\7615\7616\7617\7618\7619\7620\7621\7622\7623\7624\7625\7626\7627\7628\7629\7630\7631\7632\7633\7634\7635\7636\7637\7638\7639\7640\7641\7642\7643\7644\7645\7646\7647\7648\7649\7650\7651\7652\7653\7654\7655\7656\7657\7658\7659\7660\7661\7662\7663\7664\7665\7666\7667\7668\7669\7670\7671\7672\7673\7674\7675\7676\7677\7678\7679\7680\7681\7682\7683\7684\7685\7686\7687\7688\7689\7690\7691\7692\7693\7694\7695\7696\7697\7698\7699\7700\7701\7702\7703\7704\7705\7706\7707\7708\7709\7710\7711\7712\7713\7714\7715\7716\7717\7718\7719\7720\7721\7722\7723\7724\7725\7726\7727\7728\7729\7730\7731\7732\7733\7734\7735\7736\7737\7738\7739\7740\7741\7742\7743\7744\7745\7746\7747\7748\7749\7750\7751\7752\7753\7754\7755\7756\7757\7758\7759\7760\7761\7762\7763\7764\7765\7766\7767\7768\7769\7770\7771\7772\7773\7774\7775\7776\7777\7778\7779\7780\7781\7782\7783\7784\7785\7786\7787\7788\7789\7790\7791\7792\7793\7794\7795\7796\7797\7798\7799\7800\7801\7802\7803\7804\7805\7806\7807\7808\7809\7810\7811\7812\7813\7814\7815\7816\7817\7818\7819\7820\7821\7822\7823\7824\7825\7826\7827\7828\7829\7830\7831\7832\7833\7834\7835\7836\7837\7838\7839\7840\7841\7842\7843\7844\7845\7846\7847\7848\7849\7850\7851\7852\7853\7854\7855\7856\7857\7858\7859\7860\7861\7862\7863\7864\7865\7866\7867\7868\7869\7870\7871\7872\7873\7874\7875\7876\7877\7878\7879\7880\7881\7882\7883\7884\7885\7886\7887\7888\7889\7890\7891\7892\7893\7894\7895\7896\7897\7898\7899\7900\7901\7902\7903\7904\7905\7906\7907\7908\7909\7910\7911\7912\7913\7914\7915\7916\7917\7918\7919\7920\7921\7922\7923\7924\7925\7926\7927\7928\7929\7930\7931\7932\7933\7934\7935\7936\7937\7938\7939\7940\7941\7942\7943\7944\7945\7946\7947\7948\7949\7950\7951\7952\7953\7954\7955\7956\7957\7958\7959\7960\7961\7962\7963\7964\7965\7966\7967\7968\7969\7970\7971\7972\7973\7974\7975\7976\7977\7978\7979\7980\7981\7982\7983\7984\7985\7986\7987\7988\7989\7990\7991\7992\7993\7994\7995\7996\7997\7998\7999\8000\8001\8002\8003\8004\8005\8006\8007\8008\8009\8010\8011\8012\8013\8014\8015\8016\8017\8018\8019\8020\8021\8022\8023\8024\8025\8026\8027\8028\8029\8030\8031\8032\8033\8034\8035\8036\8037\8038\8039\8040\8041\8042\8043\8044\8045\8046\8047\8048\8049\8050\8051\8052\8053\8054\8055\8056\8057\8058\8059\8060\8061\8062\8063\8064\8065\8066\8067\8068\8069\8070\8071\8072\8073\8074\8075\8076\8077\8078\8079\8080\8081\8082\8083\8084\8085\8086\8087\8088\8089\8090\8091\8092\8093\8094\8095\8096\8097\8098\8099\8100\8101\8102\8103\8104\8105\8106\8107\8108\8109\8110\8111\8112\8113\8114\8115\8116\8117\8118\8119\8120\8121\8122\8123\8124\8125\8126\8127\8128\8129\8130\8131\8132\8133\8134\8135\8136\8137\8138\8139\8140\8141\8142\8143\8144\8145\8146\8147\8148\8149\8150\8151\8152\8153\8154\8155\8156\8157\8158\8159\8160\8161\8162\8163\8164\8165\8166\8167\8168\8169\8170\8171\8172\8173\8174\8175\8176\8177\8178\8179\8180\8181\8182\8183\8184\8185\8186\8187\8188\8189\8190\8191\8192\8193\8194\8195\8196\8197\8198\8199\8200\8201\8202\8203\8204\8205\8206\8207\8208\8209\8210\8211\8212\8213\8214\8215\8216\8217\8218\8219\8220\8221\8222\8223\8224\8225\8226\8227\8228\8229\8230\8231\8232\8233\8234\8235\8236\8237\8238\8239\8240\8241\8242\8243\8244\8245\8246\8247\8248\8249\8250\8251\8252\8253\8254\8255\8256\8257\8258\8259\8260\8261\8262\8263\8264\8265\8266\8267\8268\8269\8270\8271\8272\8273\8274\8275\8276\8277\8278\8279\8280\8281\8282\8283\8284\8285\8286\8287\8288\8289\8290\8291\8292\8293\8294\8295\8296\8297\8298\8299\8300\8301\8302\8303\8304\8305\8306\8307\8308\8309\8310\8311\8312\8313\8314\8315\8316\8317\8318\8319\8320\8321\8322\8323\8324\8325\8326\8327\8328\8329\8330\8331\8332\8333\8334\8335\8336\8337\8338\8339\8340\8341\8342\8343\8344\8345\8346\8347\8348\8349\8350\8351\8352\8353\8354\8355\8356\8357\8358\8359\8360\8361\8362\8363\8364\8365\8366\8367\8368\8369\8370\8371\8372\8373\8374\8375\8376\8377\8378\8379\8380\8381\8382\8383\8384\8385\8386\8387\8388\8389\8390\8391\8392\8393\8394\8395\8396\8397\8398\8399\8400\8401\8402\8403\8404\8405\8406\8407\8408\8409\8410\8411\8412\8413\8414\8415\8416\8417\8418\8419\8420\8421\8422\8423\8424\8425\8426\8427\8428\8429\8430\8431\8432\8433\8434\8435\8436\8437\8438\8439\8440\8441\8442\8443\8444\8445\8446\8447\8448\8449\8450\8451\8452\8453\8454\8455\8456\8457\8458\8459\8460\8461\8462\8463\8464\8465\8466\8467\8468\8469\8470\8471\8472\8473\8474\8475\8476\8477\8478\8479\8480\8481\8482\8483\8484\8485\8486\8487\8488\8489\8490\8491\8492\8493\8494\8495\8496\8497\8498\8499\8500\8501\8502\8503\8504\8505\8506\8507\8508\8509\8510\8511\8512\8513\8514\8515\8516\8517\8518\8519\8520\8521\8522\8523\8524\8525\8526\8527\8528\8529\8530\8531\8532\8533\8534\8535\8536\8537\8538\8539\8540\8541\8542\8543\8544\8545\8546\8547\8548\8549\8550\8551\8552\8553\8554\8555\8556\8557\8558\8559\8560\8561\8562\8563\8564\8565\8566\8567\8568\8569\8570\8571\8572\8573\8574\8575\8576\8577\8578\8579\8580\8581\8582\8583\8584\8585\8586\8587\8588\8589\8590\8591\8592\8593\8594\8595\8596\8597\8598\8599\8600\8601\8602\8603\8604\8605\8606\8607\8608\8609\8610\8611\8612\8613\8614\8615\8616\8617\8618\8619\8620\8621\8622\8623\8624\8625\8626\8627\8628\8629\8630\8631\8632\8633\8634\8635\8636\8637\8638\8639\8640\8641\8642\8643\8644\8645\8646\8647\8648\8649\8650\8651\8652\8653\8654\8655\8656\8657\8658\8659\8660\8661\8662\8663\8664\8665\8666\8667\8668\8669\8670\8671\8672\8673\8674\8675\8676\8677\8678\8679\8680\8681\8682\8683\8684\8685\8686\8687\8688\8689\8690\8691\8692\8693\8694\8695\8696\8697\8698\8699\8700\8701\8702\8703\8704\8705\8706\8707\8708\8709\8710\8711\8712\8713\8714\8715\8716\8717\8718\8719\8720\8721\8722\8723\8724\8725\8726\8727\8728\8729\8730\8731\8732\8733\8734\8735\8736\8737\8738\8739\8740\8741\8742\8743\8744\8745\8746\8747\8748\8749\8750\8751\8752\8753\8754\8755\8756\8757\8758\8759\8760\8761\8762\8763\8764\8765\8766\8767\8768\8769\8770\8771\8772\8773\8774\8775\8776\8777\8778\8779\8780\8781\8782\8783\8784\8785\8786\8787\8788\8789\8790\8791\8792\8793\8794\8795\8796\8797\8798\8799\8800\8801\8802\8803\8804\8805\8806\8807\8808\8809\8810\8811\8812\8813\8814\8815\8816\8817\8818\8819\8820\8821\8822\8823\8824\8825\8826\8827\8828\8829\8830\8831\8832\8833\8834\8835\8836\8837\8838\8839\8840\8841\8842\8843\8844\8845\8846\8847\8848\8849\8850\8851\8852\8853\8854\8855\8856\8857\8858\8859\8860\8861\8862\8863\8864\8865\8866\8867\8868\8869\8870\8871\8872\8873\8874\8875\8876\8877\8878\8879\8880\8881\8882\8883\8884\8885\8886\8887\8888\8889\8890\8891\8892\8893\8894\8895\8896\8897\8898\8899\8900\8901\8902\8903\8904\8905\8906\8907\8908\8909\8910\8911\8912\8913\8914\8915\8916\8917\8918\8919\8920\8921\8922\8923\8924\8925\8926\8927\8928\8929\8930\8931\8932\8933\8934\8935\8936\8937\8938\8939\8940\8941\8942\8943\8944\8945\8946\8947\8948\8949\8950\8951\8952\8953\8954\8955\8956\8957\8958\8959\8960\8961\8962\8963\8964\8965\8966\8967\8968\8969\8970\8971\8972\8973\8974\8975\8976\8977\8978\8979\8980\8981\8982\8983\8984\8985\8986\8987\8988\8989\8990\8991\8992\8993\8994\8995\8996\8997\8998\8999\9000\9001\9002\9003\9004\9005\9006\9007\9008\9009\9010\9011\9012\9013\9014\9015\9016\9017\9018\9019\9020\9021\9022\9023\9024\9025\9026\9027\9028\9029\9030\9031\9032\9033\9034\9035\9036\9037\9038\9039\9040\9041\9042\9043\9044\9045\9046\9047\9048\9049\9050\9051\9052\9053\9054\9055\9056\9057\9058\9059\9060\9061\9062\9063\9064\9065\9066\9067\9068\9069\9070\9071\9072\9073\9074\9075\9076\9077\9078\9079\9080\9081\9082\9083\9084\9085\9086\9087\9088\9089\9090\9091\9092\9093\9094\9095\9096\9097\9098\9099\9100\9101\9102\9103\9104\9105\9106\9107\9108\9109\9110\9111\9112\9113\9114\9115\9116\9117\9118\9119\9120\9121\9122\9123\9124\9125\9126\9127\9128\9129\9130\9131\9132\9133\9134\9135\9136\9137\9138\9139\9140\9141\9142\9143\9144\9145\9146\9147\9148\9149\9150\9151\9152\9153\9154\9155\9156\9157\9158\9159\9160\9161\9162\9163\9164\9165\9166\9167\9168\9169\9170\9171\9172\9173\9174\9175\9176\9177\9178\9179\9180\9181\9182\9183\9184\9185\9186\9187\9188\9189\9190\9191\9192\9193\9194\9195\9196\9197\9198\9199\9200\9201\9202\9203\9204\9205\9206\9207\9208\9209\9210\9211\9212\9213\9214\9215\9216\9217\9218\9219\9220\9221\9222\9223\9224\9225\9226\9227\9228\9229\9230\9231\9232\9233\9234\9235\9236\9237\9238\9239\9240\9241\9242\9243\9244\9245\9246\9247\9248\9249\9250\9251\9252\9253\9254\9255\9256\9257\9258\9259\9260\9261\9262\9263\9264\9265\9266\9267\9268\9269\9270\9271\9272\9273\9274\9275\9276\9277\9278\9279\9280\9281\9282\9283\9284\9285\9286\9287\9288\9289\9290\9291\9292\9293\9294\9295\9296\9297\9298\9299\9300\9301\9302\9303\9304\9305\9306\9307\9308\9309\9310\9311\9312\9313\9314\9315\9316\9317\9318\9319\9320\9321\9322\9323\9324\9325\9326\9327\9328\9329\9330\9331\9332\9333\9334\9335\9336\9337\9338\9339\9340\9341\9342\9343\9344\9345\9346\9347\9348\9349\9350\9351\9352\9353\9354\9355\9356\9357\9358\9359\9360\9361\9362\9363\9364\9365\9366\9367\9368\9369\9370\9371\9372\9373\9374\9375\9376\9377\9378\9379\9380\9381\9382\9383\9384\9385\9386\9387\9388\9389\9390\9391\9392\9393\9394\9395\9396\9397\9398\9399\9400\9401\9402\9403\9404\9405\9406\9407\9408\9409\9410\9411\9412\9413\9414\9415\9416\9417\9418\9419\9420\9421\9422\9423\9424\9425\9426\9427\9428\9429\9430\9431\9432\9433\9434\9435\9436\9437\9438\9439\9440\9441\9442\9443\9444\9445\9446\9447\9448\9449\9450\9451\9452\9453\9454\9455\9456\9457\9458\9459\9460\9461\9462\9463\9464\9465\9466\9467\9468\9469\9470\9471\9472\9473\9474\9475\9476\9477\9478\9479\9480\9481\9482\9483\9484\9485\9486\9487\9488\9489\9490\9491\9492\9493\9494\9495\9496\9497\9498\9499\9500\9501\9502\9503\9504\9505\9506\9507\9508\9509\9510\9511\9512\9513\9514\9515\9516\9517\9518\9519\9520\9521\9522\9523\9524\9525\9526\9527\9528\9529\9530\9531\9532\9533\9534\9535\9536\9537\9538\9539\9540\9541\9542\9543\9544\9545\9546\9547\9548\9549\9550\9551\9552\9553\9554\9555\9556\9557\9558\9559\9560\9561\9562\9563\9564\9565\9566\9567\9568\9569\9570\9571\9572\9573\9574\9575\9576\9577\9578\9579\9580\9581\9582\9583\9584\9585\9586\9587\9588\9589\9590\9591\9592\9593\9594\9595\9596\9597\9598\9599\9600\9601\9602\9603\9604\9605\9606\9607\9608\9609\9610\9611\9612\9613\9614\9615\9616\9617\9618\9619\9620\9621\9622\9623\9624\9625\9626\9627\9628\9629\9630\9631\9632\9633\9634\9635\9636\9637\9638\9639\9640\9641\9642\9643\9644\9645\9646\9647\9648\9649\9650\9651\9652\9653\9654\9655\9656\9657\9658\9659\9660\9661\9662\9663\9664\9665\9666\9667\9668\9669\9670\9671\9672\9673\9674\9675\9676\9677\9678\9679\9680\9681\9682\9683\9684\9685\9686\9687\9688\9689\9690\9691\9692\9693\9694\9695\9696\9697\9698\9699\9700\9701\9702\9703\9704\9705\9706\9707\9708\9709\9710\9711\9712\9713\9714\9715\9716\9717\9718\9719\9720\9721\9722\9723\9724\9725\9726\9727\9728\9729\9730\9731\9732\9733\9734\9735\9736\9737\9738\9739\9740\9741\9742\9743\9744\9745\9746\9747\9748\9749\9750\9751\9752\9753\9754\9755\9756\9757\9758\9759\9760\9761\9762\9763\9764\9765\9766\9767\9768\9769\9770\9771\9772\9773\9774\9775\9776\9777\9778\9779\9780\9781\9782\9783\9784\9785\9786\9787\9788\9789\9790\9791\9792\9793\9794\9795\9796\9797\9798\9799\9800\9801\9802\9803\9804\9805\9806\9807\9808\9809\9810\9811\9812\9813\9814\9815\9816\9817\9818\9819\9820\9821\9822\9823\9824\9825\9826\9827\9828\9829\9830\9831\9832\9833\9834\9835\9836\9837\9838\9839\9840\9841\9842\9843\9844\9845\9846\9847\9848\9849\9850\9851\9852\9853\9854\9855\9856\9857\9858\9859\9860\9861\9862\9863\9864\9865\9866\9867\9868\9869\9870\9871\9872\9873\9874\9875\9876\9877\9878\9879\9880\9881\9882\9883\9884\9885\9886\9887\9888\9889\9890\9891\9892\9893\9894\9895\9896\9897\9898\9899\9900\9901\9902\9903\9904\9905\9906\9907\9908\9909\9910\9911\9912\9913\9914\9915\9916\9917\9918\9919\9920\9921\9922\9923\9924\9925\9926\9927\9928\9929\9930\9931\9932\9933\9934\9935\9936\9937\9938\9939\9940\9941\9942\9943\9944\9945\9946\9947\9948\9949\9950\9951\9952\9953\9954\9955\9956\9957\9958\9959\9960\9961\9962\9963\9964\9965\9966\9967\9968\9969\9970\9971\9972\9973\9974\9975\9976\9977\9978\9979\9980\9981\9982\9983\9984\9985\9986\9987\9988\9989\9990\9991\9992\9993\9994\9995\9996\9997\9998\9999\10000</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661\662\663\664\665\666\667\668\669\670\671\672\673\674\675\676\677\678\679\680\681\682\683\684\685\686\687\688\689\690\691\692\693\694\695\696\697\698\699\700\701\702\703\704\705\706\707\708\709\710\711\712\713\714\715\716\717\718\719\720\721\722\723\724\725\726\727\728\729\730\731\732\733\734\735\736\737\738\739\740\741\742\743\744\745\746\747\748\749\750\751\752\753\754\755\756\757\758\759\760\761\762\763\764\765\766\767\768\769\770\771\772\773\774\775\776\777\778\779\780\781\782\783\784\785\786\787\788\789\790\791\792\793\794\795\796\797\798\799\800\801\802\803\804\805\806\807\808\809\810\811\812\813\814\815\816\817\818\819\820\821\822\823\824\825\826\827\828\829\830\831\832\833\834\835\836\837\838\839\840\841\842\843\844\845\846\847\848\849\850\851\852\853\854\855\856\857\858\859\860\861\862\863\864\865\866\867\868\869\870\871\872\873\874\875\876\877\878\879\880\881\882\883\884\885\886\887\888\889\890\891\892\893\894\895\896\897\898\899\900\901\902\903\904\905\906\907\908\909\910\911\912\913\914\915\916\917\918\919\920\921\922\923\924\925\926\927\928\929\930\931\932\933\934\935\936\937\938\939\940\941\942\943\944\945\946\947\948\949\950\951\952\953\954\955\956\957\958\959\960\961\962\963\964\965\966\967\968\969\970\971\972\973\974\975\976\977\978\979\980\981\982\983\984\985\986\987\988\989\990\991\992\993\994\995\996\997\998\999\1000\1001\1002\1003\1004\1005\1006\1007\1008\1009\1010\1011\1012\1013\1014\1015\1016\1017\1018\1019\1020\1021\1022\1023\1024\1025\1026\1027\1028\1029\1030\1031\1032\1033\1034\1035\1036\1037\1038\1039\1040\1041\1042\1043\1044\1045\1046\1047\1048\1049\1050\1051\1052\1053\1054\1055\1056\1057\1058\1059\1060\1061\1062\1063\1064\1065\1066\1067\1068\1069\1070\1071\1072\1073\1074\1075\1076\1077\1078\1079\1080\1081\1082\1083\1084\1085\1086\1087\1088\1089\1090\1091\1092\1093\1094\1095\1096\1097\1098\1099\1100\1101\1102\1103\1104\1105\1106\1107\1108\1109\1110\1111\1112\1113\1114\1115\1116\1117\1118\1119\1120\1121\1122\1123\1124\1125\1126\1127\1128\1129\1130\1131\1132\1133\1134\1135\1136\1137\1138\1139\1140\1141\1142\1143\1144\1145\1146\1147\1148\1149\1150\1151\1152\1153\1154\1155\1156\1157\1158\1159\1160\1161\1162\1163\1164\1165\1166\1167\1168\1169\1170\1171\1172\1173\1174\1175\1176\1177\1178\1179\1180\1181\1182\1183\1184\1185\1186\1187\1188\1189\1190\1191\1192\1193\1194\1195\1196\1197\1198\1199\1200\1201\1202\1203\1204\1205\1206\1207\1208\1209\1210\1211\1212\1213\1214\1215\1216\1217\1218\1219\1220\1221\1222\1223\1224\1225\1226\1227\1228\1229\1230\1231\1232\1233\1234\1235\1236\1237\1238\1239\1240\1241\1242\1243\1244\1245\1246\1247\1248\1249\1250\1251\1252\1253\1254\1255\1256\1257\1258\1259\1260\1261\1262\1263\1264\1265\1266\1267\1268\1269\1270\1271\1272\1273\1274\1275\1276\1277\1278\1279\1280\1281\1282\1283\1284\1285\1286\1287\1288\1289\1290\1291\1292\1293\1294\1295\1296\1297\1298\1299\1300\1301\1302\1303\1304\1305\1306\1307\1308\1309\1310\1311\1312\1313\1314\1315\1316\1317\1318\1319\1320\1321\1322\1323\1324\1325\1326\1327\1328\1329\1330\1331\1332\1333\1334\1335\1336\1337\1338\1339\1340\1341\1342\1343\1344\1345\1346\1347\1348\1349\1350\1351\1352\1353\1354\1355\1356\1357\1358\1359\1360\1361\1362\1363\1364\1365\1366\1367\1368\1369\1370\1371\1372\1373\1374\1375\1376\1377\1378\1379\1380\1381\1382\1383\1384\1385\1386\1387\1388\1389\1390\1391\1392\1393\1394\1395\1396\1397\1398\1399\1400\1401\1402\1403\1404\1405\1406\1407\1408\1409\1410\1411\1412\1413\1414\1415\1416\1417\1418\1419\1420\1421\1422\1423\1424\1425\1426\1427\1428\1429\1430\1431\1432\1433\1434\1435\1436\1437\1438\1439\1440\1441\1442\1443\1444\1445\1446\1447\1448\1449\1450\1451\1452\1453\1454\1455\1456\1457\1458\1459\1460\1461\1462\1463\1464\1465\1466\1467\1468\1469\1470\1471\1472\1473\1474\1475\1476\1477\1478\1479\1480\1481\1482\1483\1484\1485\1486\1487\1488\1489\1490\1491\1492\1493\1494\1495\1496\1497\1498\1499\1500\1501\1502\1503\1504\1505\1506\1507\1508\1509\1510\1511\1512\1513\1514\1515\1516\1517\1518\1519\1520\1521\1522\1523\1524\1525\1526\1527\1528\1529\1530\1531\1532\1533\1534\1535\1536\1537\1538\1539\1540\1541\1542\1543\1544\1545\1546\1547\1548\1549\1550\1551\1552\1553\1554\1555\1556\1557\1558\1559\1560\1561\1562\1563\1564\1565\1566\1567\1568\1569\1570\1571\1572\1573\1574\1575\1576\1577\1578\1579\1580\1581\1582\1583\1584\1585\1586\1587\1588\1589\1590\1591\1592\1593\1594\1595\1596\1597\1598\1599\1600\1601\1602\1603\1604\1605\1606\1607\1608\1609\1610\1611\1612\1613\1614\1615\1616\1617\1618\1619\1620\1621\1622\1623\1624\1625\1626\1627\1628\1629\1630\1631\1632\1633\1634\1635\1636\1637\1638\1639\1640\1641\1642\1643\1644\1645\1646\1647\1648\1649\1650\1651\1652\1653\1654\1655\1656\1657\1658\1659\1660\1661\1662\1663\1664\1665\1666\1667\1668\1669\1670\1671\1672\1673\1674\1675\1676\1677\1678\1679\1680\1681\1682\1683\1684\1685\1686\1687\1688\1689\1690\1691\1692\1693\1694\1695\1696\1697\1698\1699\1700\1701\1702\1703\1704\1705\1706\1707\1708\1709\1710\1711\1712\1713\1714\1715\1716\1717\1718\1719\1720\1721\1722\1723\1724\1725\1726\1727\1728\1729\1730\1731\1732\1733\1734\1735\1736\1737\1738\1739\1740\1741\1742\1743\1744\1745\1746\1747\1748\1749\1750\1751\1752\1753\1754\1755\1756\1757\1758\1759\1760\1761\1762\1763\1764\1765\1766\1767\1768\1769\1770\1771\1772\1773\1774\1775\1776\1777\1778\1779\1780\1781\1782\1783\1784\1785\1786\1787\1788\1789\1790\1791\1792\1793\1794\1795\1796\1797\1798\1799\1800\1801\1802\1803\1804\1805\1806\1807\1808\1809\1810\1811\1812\1813\1814\1815\1816\1817\1818\1819\1820\1821\1822\1823\1824\1825\1826\1827\1828\1829\1830\1831\1832\1833\1834\1835\1836\1837\1838\1839\1840\1841\1842\1843\1844\1845\1846\1847\1848\1849\1850\1851\1852\1853\1854\1855\1856\1857\1858\1859\1860\1861\1862\1863\1864\1865\1866\1867\1868\1869\1870\1871\1872\1873\1874\1875\1876\1877\1878\1879\1880\1881\1882\1883\1884\1885\1886\1887\1888\1889\1890\1891\1892\1893\1894\1895\1896\1897\1898\1899\1900\1901\1902\1903\1904\1905\1906\1907\1908\1909\1910\1911\1912\1913\1914\1915\1916\1917\1918\1919\1920\1921\1922\1923\1924\1925\1926\1927\1928\1929\1930\1931\1932\1933\1934\1935\1936\1937\1938\1939\1940\1941\1942\1943\1944\1945\1946\1947\1948\1949\1950\1951\1952\1953\1954\1955\1956\1957\1958\1959\1960\1961\1962\1963\1964\1965\1966\1967\1968\1969\1970\1971\1972\1973\1974\1975\1976\1977\1978\1979\1980\1981\1982\1983\1984\1985\1986\1987\1988\1989\1990\1991\1992\1993\1994\1995\1996\1997\1998\1999\2000\2001\2002\2003\2004\2005\2006\2007\2008\2009\2010\2011\2012\2013\2014\2015\2016\2017\2018\2019\2020\2021\2022\2023\2024\2025\2026\2027\2028\2029\2030\2031\2032\2033\2034\2035\2036\2037\2038\2039\2040\2041\2042\2043\2044\2045\2046\2047\2048\2049\2050\2051\2052\2053\2054\2055\2056\2057\2058\2059\2060\2061\2062\2063\2064\2065\2066\2067\2068\2069\2070\2071\2072\2073\2074\2075\2076\2077\2078\2079\2080\2081\2082\2083\2084\2085\2086\2087\2088\2089\2090\2091\2092\2093\2094\2095\2096\2097\2098\2099\2100\2101\2102\2103\2104\2105\2106\2107\2108\2109\2110\2111\2112\2113\2114\2115\2116\2117\2118\2119\2120\2121\2122\2123\2124\2125\2126\2127\2128\2129\2130\2131\2132\2133\2134\2135\2136\2137\2138\2139\2140\2141\2142\2143\2144\2145\2146\2147\2148\2149\2150\2151\2152\2153\2154\2155\2156\2157\2158\2159\2160\2161\2162\2163\2164\2165\2166\2167\2168\2169\2170\2171\2172\2173\2174\2175\2176\2177\2178\2179\2180\2181\2182\2183\2184\2185\2186\2187\2188\2189\2190\2191\2192\2193\2194\2195\2196\2197\2198\2199\2200\2201\2202\2203\2204\2205\2206\2207\2208\2209\2210\2211\2212\2213\2214\2215\2216\2217\2218\2219\2220\2221\2222\2223\2224\2225\2226\2227\2228\2229\2230\2231\2232\2233\2234\2235\2236\2237\2238\2239\2240\2241\2242\2243\2244\2245\2246\2247\2248\2249\2250\2251\2252\2253\2254\2255\2256\2257\2258\2259\2260\2261\2262\2263\2264\2265\2266\2267\2268\2269\2270\2271\2272\2273\2274\2275\2276\2277\2278\2279\2280\2281\2282\2283\2284\2285\2286\2287\2288\2289\2290\2291\2292\2293\2294\2295\2296\2297\2298\2299\2300\2301\2302\2303\2304\2305\2306\2307\2308\2309\2310\2311\2312\2313\2314\2315\2316\2317\2318\2319\2320\2321\2322\2323\2324\2325\2326\2327\2328\2329\2330\2331\2332\2333\2334\2335\2336\2337\2338\2339\2340\2341\2342\2343\2344\2345\2346\2347\2348\2349\2350\2351\2352\2353\2354\2355\2356\2357\2358\2359\2360\2361\2362\2363\2364\2365\2366\2367\2368\2369\2370\2371\2372\2373\2374\2375\2376\2377\2378\2379\2380\2381\2382\2383\2384\2385\2386\2387\2388\2389\2390\2391\2392\2393\2394\2395\2396\2397\2398\2399\2400\2401\2402\2403\2404\2405\2406\2407\2408\2409\2410\2411\2412\2413\2414\2415\2416\2417\2418\2419\2420\2421\2422\2423\2424\2425\2426\2427\2428\2429\2430\2431\2432\2433\2434\2435\2436\2437\2438\2439\2440\2441\2442\2443\2444\2445\2446\2447\2448\2449\2450\2451\2452\2453\2454\2455\2456\2457\2458\2459\2460\2461\2462\2463\2464\2465\2466\2467\2468\2469\2470\2471\2472\2473\2474\2475\2476\2477\2478\2479\2480\2481\2482\2483\2484\2485\2486\2487\2488\2489\2490\2491\2492\2493\2494\2495\2496\2497\2498\2499\2500\2501\2502\2503\2504\2505\2506\2507\2508\2509\2510\2511\2512\2513\2514\2515\2516\2517\2518\2519\2520\2521\2522\2523\2524\2525\2526\2527\2528\2529\2530\2531\2532\2533\2534\2535\2536\2537\2538\2539\2540\2541\2542\2543\2544\2545\2546\2547\2548\2549\2550\2551\2552\2553\2554\2555\2556\2557\2558\2559\2560\2561\2562\2563\2564\2565\2566\2567\2568\2569\2570\2571\2572\2573\2574\2575\2576\2577\2578\2579\2580\2581\2582\2583\2584\2585\2586\2587\2588\2589\2590\2591\2592\2593\2594\2595\2596\2597\2598\2599\2600\2601\2602\2603\2604\2605\2606\2607\2608\2609\2610\2611\2612\2613\2614\2615\2616\2617\2618\2619\2620\2621\2622\2623\2624\2625\2626\2627\2628\2629\2630\2631\2632\2633\2634\2635\2636\2637\2638\2639\2640\2641\2642\2643\2644\2645\2646\2647\2648\2649\2650\2651\2652\2653\2654\2655\2656\2657\2658\2659\2660\2661\2662\2663\2664\2665\2666\2667\2668\2669\2670\2671\2672\2673\2674\2675\2676\2677\2678\2679\2680\2681\2682\2683\2684\2685\2686\2687\2688\2689\2690\2691\2692\2693\2694\2695\2696\2697\2698\2699\2700\2701\2702\2703\2704\2705\2706\2707\2708\2709\2710\2711\2712\2713\2714\2715\2716\2717\2718\2719\2720\2721\2722\2723\2724\2725\2726\2727\2728\2729\2730\2731\2732\2733\2734\2735\2736\2737\2738\2739\2740\2741\2742\2743\2744\2745\2746\2747\2748\2749\2750\2751\2752\2753\2754\2755\2756\2757\2758\2759\2760\2761\2762\2763\2764\2765\2766\2767\2768\2769\2770\2771\2772\2773\2774\2775\2776\2777\2778\2779\2780\2781\2782\2783\2784\2785\2786\2787\2788\2789\2790\2791\2792\2793\2794\2795\2796\2797\2798\2799\2800\2801\2802\2803\2804\2805\2806\2807\2808\2809\2810\2811\2812\2813\2814\2815\2816\2817\2818\2819\2820\2821\2822\2823\2824\2825\2826\2827\2828\2829\2830\2831\2832\2833\2834\2835\2836\2837\2838\2839\2840\2841\2842\2843\2844\2845\2846\2847\2848\2849\2850\2851\2852\2853\2854\2855\2856\2857\2858\2859\2860\2861\2862\2863\2864\2865\2866\2867\2868\2869\2870\2871\2872\2873\2874\2875\2876\2877\2878\2879\2880\2881\2882\2883\2884\2885\2886\2887\2888\2889\2890\2891\2892\2893\2894\2895\2896\2897\2898\2899\2900\2901\2902\2903\2904\2905\2906\2907\2908\2909\2910\2911\2912\2913\2914\2915\2916\2917\2918\2919\2920\2921\2922\2923\2924\2925\2926\2927\2928\2929\2930\2931\2932\2933\2934\2935\2936\2937\2938\2939\2940\2941\2942\2943\2944\2945\2946\2947\2948\2949\2950\2951\2952\2953\2954\2955\2956\2957\2958\2959\2960\2961\2962\2963\2964\2965\2966\2967\2968\2969\2970\2971\2972\2973\2974\2975\2976\2977\2978\2979\2980\2981\2982\2983\2984\2985\2986\2987\2988\2989\2990\2991\2992\2993\2994\2995\2996\2997\2998\2999\3000\3001\3002\3003\3004\3005\3006\3007\3008\3009\3010\3011\3012\3013\3014\3015\3016\3017\3018\3019\3020\3021\3022\3023\3024\3025\3026\3027\3028\3029\3030\3031\3032\3033\3034\3035\3036\3037\3038\3039\3040\3041\3042\3043\3044\3045\3046\3047\3048\3049\3050\3051\3052\3053\3054\3055\3056\3057\3058\3059\3060\3061\3062\3063\3064\3065\3066\3067\3068\3069\3070\3071\3072\3073\3074\3075\3076\3077\3078\3079\3080\3081\3082\3083\3084\3085\3086\3087\3088\3089\3090\3091\3092\3093\3094\3095\3096\3097\3098\3099\3100\3101\3102\3103\3104\3105\3106\3107\3108\3109\3110\3111\3112\3113\3114\3115\3116\3117\3118\3119\3120\3121\3122\3123\3124\3125\3126\3127\3128\3129\3130\3131\3132\3133\3134\3135\3136\3137\3138\3139\3140\3141\3142\3143\3144\3145\3146\3147\3148\3149\3150\3151\3152\3153\3154\3155\3156\3157\3158\3159\3160\3161\3162\3163\3164\3165\3166\3167\3168\3169\3170\3171\3172\3173\3174\3175\3176\3177\3178\3179\3180\3181\3182\3183\3184\3185\3186\3187\3188\3189\3190\3191\3192\3193\3194\3195\3196\3197\3198\3199\3200\3201\3202\3203\3204\3205\3206\3207\3208\3209\3210\3211\3212\3213\3214\3215\3216\3217\3218\3219\3220\3221\3222\3223\3224\3225\3226\3227\3228\3229\3230\3231\3232\3233\3234\3235\3236\3237\3238\3239\3240\3241\3242\3243\3244\3245\3246\3247\3248\3249\3250\3251\3252\3253\3254\3255\3256\3257\3258\3259\3260\3261\3262\3263\3264\3265\3266\3267\3268\3269\3270\3271\3272\3273\3274\3275\3276\3277\3278\3279\3280\3281\3282\3283\3284\3285\3286\3287\3288\3289\3290\3291\3292\3293\3294\3295\3296\3297\3298\3299\3300\3301\3302\3303\3304\3305\3306\3307\3308\3309\3310\3311\3312\3313\3314\3315\3316\3317\3318\3319\3320\3321\3322\3323\3324\3325\3326\3327\3328\3329\3330\3331\3332\3333\3334\3335\3336\3337\3338\3339\3340\3341\3342\3343\3344\3345\3346\3347\3348\3349\3350\3351\3352\3353\3354\3355\3356\3357\3358\3359\3360\3361\3362\3363\3364\3365\3366\3367\3368\3369\3370\3371\3372\3373\3374\3375\3376\3377\3378\3379\3380\3381\3382\3383\3384\3385\3386\3387\3388\3389\3390\3391\3392\3393\3394\3395\3396\3397\3398\3399\3400\3401\3402\3403\3404\3405\3406\3407\3408\3409\3410\3411\3412\3413\3414\3415\3416\3417\3418\3419\3420\3421\3422\3423\3424\3425\3426\3427\3428\3429\3430\3431\3432\3433\3434\3435\3436\3437\3438\3439\3440\3441\3442\3443\3444\3445\3446\3447\3448\3449\3450\3451\3452\3453\3454\3455\3456\3457\3458\3459\3460\3461\3462\3463\3464\3465\3466\3467\3468\3469\3470\3471\3472\3473\3474\3475\3476\3477\3478\3479\3480\3481\3482\3483\3484\3485\3486\3487\3488\3489\3490\3491\3492\3493\3494\3495\3496\3497\3498\3499\3500\3501\3502\3503\3504\3505\3506\3507\3508\3509\3510\3511\3512\3513\3514\3515\3516\3517\3518\3519\3520\3521\3522\3523\3524\3525\3526\3527\3528\3529\3530\3531\3532\3533\3534\3535\3536\3537\3538\3539\3540\3541\3542\3543\3544\3545\3546\3547\3548\3549\3550\3551\3552\3553\3554\3555\3556\3557\3558\3559\3560\3561\3562\3563\3564\3565\3566\3567\3568\3569\3570\3571\3572\3573\3574\3575\3576\3577\3578\3579\3580\3581\3582\3583\3584\3585\3586\3587\3588\3589\3590\3591\3592\3593\3594\3595\3596\3597\3598\3599\3600\3601\3602\3603\3604\3605\3606\3607\3608\3609\3610\3611\3612\3613\3614\3615\3616\3617\3618\3619\3620\3621\3622\3623\3624\3625\3626\3627\3628\3629\3630\3631\3632\3633\3634\3635\3636\3637\3638\3639\3640\3641\3642\3643\3644\3645\3646\3647\3648\3649\3650\3651\3652\3653\3654\3655\3656\3657\3658\3659\3660\3661\3662\3663\3664\3665\3666\3667\3668\3669\3670\3671\3672\3673\3674\3675\3676\3677\3678\3679\3680\3681\3682\3683\3684\3685\3686\3687\3688\3689\3690\3691\3692\3693\3694\3695\3696\3697\3698\3699\3700\3701\3702\3703\3704\3705\3706\3707\3708\3709\3710\3711\3712\3713\3714\3715\3716\3717\3718\3719\3720\3721\3722\3723\3724\3725\3726\3727\3728\3729\3730\3731\3732\3733\3734\3735\3736\3737\3738\3739\3740\3741\3742\3743\3744\3745\3746\3747\3748\3749\3750\3751\3752\3753\3754\3755\3756\3757\3758\3759\3760\3761\3762\3763\3764\3765\3766\3767\3768\3769\3770\3771\3772\3773\3774\3775\3776\3777\3778\3779\3780\3781\3782\3783\3784\3785\3786\3787\3788\3789\3790\3791\3792\3793\3794\3795\3796\3797\3798\3799\3800\3801\3802\3803\3804\3805\3806\3807\3808\3809\3810\3811\3812\3813\3814\3815\3816\3817\3818\3819\3820\3821\3822\3823\3824\3825\3826\3827\3828\3829\3830\3831\3832\3833\3834\3835\3836\3837\3838\3839\3840\3841\3842\3843\3844\3845\3846\3847\3848\3849\3850\3851\3852\3853\3854\3855\3856\3857\3858\3859\3860\3861\3862\3863\3864\3865\3866\3867\3868\3869\3870\3871\3872\3873\3874\3875\3876\3877\3878\3879\3880\3881\3882\3883\3884\3885\3886\3887\3888\3889\3890\3891\3892\3893\3894\3895\3896\3897\3898\3899\3900\3901\3902\3903\3904\3905\3906\3907\3908\3909\3910\3911\3912\3913\3914\3915\3916\3917\3918\3919\3920\3921\3922\3923\3924\3925\3926\3927\3928\3929\3930\3931\3932\3933\3934\3935\3936\3937\3938\3939\3940\3941\3942\3943\3944\3945\3946\3947\3948\3949\3950\3951\3952\3953\3954\3955\3956\3957\3958\3959\3960\3961\3962\3963\3964\3965\3966\3967\3968\3969\3970\3971\3972\3973\3974\3975\3976\3977\3978\3979\3980\3981\3982\3983\3984\3985\3986\3987\3988\3989\3990\3991\3992\3993\3994\3995\3996\3997\3998\3999\4000\4001\4002\4003\4004\4005\4006\4007\4008\4009\4010\4011\4012\4013\4014\4015\4016\4017\4018\4019\4020\4021\4022\4023\4024\4025\4026\4027\4028\4029\4030\4031\4032\4033\4034\4035\4036\4037\4038\4039\4040\4041\4042\4043\4044\4045\4046\4047\4048\4049\4050\4051\4052\4053\4054\4055\4056\4057\4058\4059\4060\4061\4062\4063\4064\4065\4066\4067\4068\4069\4070\4071\4072\4073\4074\4075\4076\4077\4078\4079\4080\4081\4082\4083\4084\4085\4086\4087\4088\4089\4090\4091\4092\4093\4094\4095\4096\4097\4098\4099\4100\4101\4102\4103\4104\4105\4106\4107\4108\4109\4110\4111\4112\4113\4114\4115\4116\4117\4118\4119\4120\4121\4122\4123\4124\4125\4126\4127\4128\4129\4130\4131\4132\4133\4134\4135\4136\4137\4138\4139\4140\4141\4142\4143\4144\4145\4146\4147\4148\4149\4150\4151\4152\4153\4154\4155\4156\4157\4158\4159\4160\4161\4162\4163\4164\4165\4166\4167\4168\4169\4170\4171\4172\4173\4174\4175\4176\4177\4178\4179\4180\4181\4182\4183\4184\4185\4186\4187\4188\4189\4190\4191\4192\4193\4194\4195\4196\4197\4198\4199\4200\4201\4202\4203\4204\4205\4206\4207\4208\4209\4210\4211\4212\4213\4214\4215\4216\4217\4218\4219\4220\4221\4222\4223\4224\4225\4226\4227\4228\4229\4230\4231\4232\4233\4234\4235\4236\4237\4238\4239\4240\4241\4242\4243\4244\4245\4246\4247\4248\4249\4250\4251\4252\4253\4254\4255\4256\4257\4258\4259\4260\4261\4262\4263\4264\4265\4266\4267\4268\4269\4270\4271\4272\4273\4274\4275\4276\4277\4278\4279\4280\4281\4282\4283\4284\4285\4286\4287\4288\4289\4290\4291\4292\4293\4294\4295\4296\4297\4298\4299\4300\4301\4302\4303\4304\4305\4306\4307\4308\4309\4310\4311\4312\4313\4314\4315\4316\4317\4318\4319\4320\4321\4322\4323\4324\4325\4326\4327\4328\4329\4330\4331\4332\4333\4334\4335\4336\4337\4338\4339\4340\4341\4342\4343\4344\4345\4346\4347\4348\4349\4350\4351\4352\4353\4354\4355\4356\4357\4358\4359\4360\4361\4362\4363\4364\4365\4366\4367\4368\4369\4370\4371\4372\4373\4374\4375\4376\4377\4378\4379\4380\4381\4382\4383\4384\4385\4386\4387\4388\4389\4390\4391\4392\4393\4394\4395\4396\4397\4398\4399\4400\4401\4402\4403\4404\4405\4406\4407\4408\4409\4410\4411\4412\4413\4414\4415\4416\4417\4418\4419\4420\4421\4422\4423\4424\4425\4426\4427\4428\4429\4430\4431\4432\4433\4434\4435\4436\4437\4438\4439\4440\4441\4442\4443\4444\4445\4446\4447\4448\4449\4450\4451\4452\4453\4454\4455\4456\4457\4458\4459\4460\4461\4462\4463\4464\4465\4466\4467\4468\4469\4470\4471\4472\4473\4474\4475\4476\4477\4478\4479\4480\4481\4482\4483\4484\4485\4486\4487\4488\4489\4490\4491\4492\4493\4494\4495\4496\4497\4498\4499\4500\4501\4502\4503\4504\4505\4506\4507\4508\4509\4510\4511\4512\4513\4514\4515\4516\4517\4518\4519\4520\4521\4522\4523\4524\4525\4526\4527\4528\4529\4530\4531\4532\4533\4534\4535\4536\4537\4538\4539\4540\4541\4542\4543\4544\4545\4546\4547\4548\4549\4550\4551\4552\4553\4554\4555\4556\4557\4558\4559\4560\4561\4562\4563\4564\4565\4566\4567\4568\4569\4570\4571\4572\4573\4574\4575\4576\4577\4578\4579\4580\4581\4582\4583\4584\4585\4586\4587\4588\4589\4590\4591\4592\4593\4594\4595\4596\4597\4598\4599\4600\4601\4602\4603\4604\4605\4606\4607\4608\4609\4610\4611\4612\4613\4614\4615\4616\4617\4618\4619\4620\4621\4622\4623\4624\4625\4626\4627\4628\4629\4630\4631\4632\4633\4634\4635\4636\4637\4638\4639\4640\4641\4642\4643\4644\4645\4646\4647\4648\4649\4650\4651\4652\4653\4654\4655\4656\4657\4658\4659\4660\4661\4662\4663\4664\4665\4666\4667\4668\4669\4670\4671\4672\4673\4674\4675\4676\4677\4678\4679\4680\4681\4682\4683\4684\4685\4686\4687\4688\4689\4690\4691\4692\4693\4694\4695\4696\4697\4698\4699\4700\4701\4702\4703\4704\4705\4706\4707\4708\4709\4710\4711\4712\4713\4714\4715\4716\4717\4718\4719\4720\4721\4722\4723\4724\4725\4726\4727\4728\4729\4730\4731\4732\4733\4734\4735\4736\4737\4738\4739\4740\4741\4742\4743\4744\4745\4746\4747\4748\4749\4750\4751\4752\4753\4754\4755\4756\4757\4758\4759\4760\4761\4762\4763\4764\4765\4766\4767\4768\4769\4770\4771\4772\4773\4774\4775\4776\4777\4778\4779\4780\4781\4782\4783\4784\4785\4786\4787\4788\4789\4790\4791\4792\4793\4794\4795\4796\4797\4798\4799\4800\4801\4802\4803\4804\4805\4806\4807\4808\4809\4810\4811\4812\4813\4814\4815\4816\4817\4818\4819\4820\4821\4822\4823\4824\4825\4826\4827\4828\4829\4830\4831\4832\4833\4834\4835\4836\4837\4838\4839\4840\4841\4842\4843\4844\4845\4846\4847\4848\4849\4850\4851\4852\4853\4854\4855\4856\4857\4858\4859\4860\4861\4862\4863\4864\4865\4866\4867\4868\4869\4870\4871\4872\4873\4874\4875\4876\4877\4878\4879\4880\4881\4882\4883\4884\4885\4886\4887\4888\4889\4890\4891\4892\4893\4894\4895\4896\4897\4898\4899\4900\4901\4902\4903\4904\4905\4906\4907\4908\4909\4910\4911\4912\4913\4914\4915\4916\4917\4918\4919\4920\4921\4922\4923\4924\4925\4926\4927\4928\4929\4930\4931\4932\4933\4934\4935\4936\4937\4938\4939\4940\4941\4942\4943\4944\4945\4946\4947\4948\4949\4950\4951\4952\4953\4954\4955\4956\4957\4958\4959\4960\4961\4962\4963\4964\4965\4966\4967\4968\4969\4970\4971\4972\4973\4974\4975\4976\4977\4978\4979\4980\4981\4982\4983\4984\4985\4986\4987\4988\4989\4990\4991\4992\4993\4994\4995\4996\4997\4998\4999\5000\5001\5002\5003\5004\5005\5006\5007\5008\5009\5010\5011\5012\5013\5014\5015\5016\5017\5018\5019\5020\5021\5022\5023\5024\5025\5026\5027\5028\5029\5030\5031\5032\5033\5034\5035\5036\5037\5038\5039\5040\5041\5042\5043\5044\5045\5046\5047\5048\5049\5050\5051\5052\5053\5054\5055\5056\5057\5058\5059\5060\5061\5062\5063\5064\5065\5066\5067\5068\5069\5070\5071\5072\5073\5074\5075\5076\5077\5078\5079\5080\5081\5082\5083\5084\5085\5086\5087\5088\5089\5090\5091\5092\5093\5094\5095\5096\5097\5098\5099\5100\5101\5102\5103\5104\5105\5106\5107\5108\5109\5110\5111\5112\5113\5114\5115\5116\5117\5118\5119\5120\5121\5122\5123\5124\5125\5126\5127\5128\5129\5130\5131\5132\5133\5134\5135\5136\5137\5138\5139\5140\5141\5142\5143\5144\5145\5146\5147\5148\5149\5150\5151\5152\5153\5154\5155\5156\5157\5158\5159\5160\5161\5162\5163\5164\5165\5166\5167\5168\5169\5170\5171\5172\5173\5174\5175\5176\5177\5178\5179\5180\5181\5182\5183\5184\5185\5186\5187\5188\5189\5190\5191\5192\5193\5194\5195\5196\5197\5198\5199\5200\5201\5202\5203\5204\5205\5206\5207\5208\5209\5210\5211\5212\5213\5214\5215\5216\5217\5218\5219\5220\5221\5222\5223\5224\5225\5226\5227\5228\5229\5230\5231\5232\5233\5234\5235\5236\5237\5238\5239\5240\5241\5242\5243\5244\5245\5246\5247\5248\5249\5250\5251\5252\5253\5254\5255\5256\5257\5258\5259\5260\5261\5262\5263\5264\5265\5266\5267\5268\5269\5270\5271\5272\5273\5274\5275\5276\5277\5278\5279\5280\5281\5282\5283\5284\5285\5286\5287\5288\5289\5290\5291\5292\5293\5294\5295\5296\5297\5298\5299\5300\5301\5302\5303\5304\5305\5306\5307\5308\5309\5310\5311\5312\5313\5314\5315\5316\5317\5318\5319\5320\5321\5322\5323\5324\5325\5326\5327\5328\5329\5330\5331\5332\5333\5334\5335\5336\5337\5338\5339\5340\5341\5342\5343\5344\5345\5346\5347\5348\5349\5350\5351\5352\5353\5354\5355\5356\5357\5358\5359\5360\5361\5362\5363\5364\5365\5366\5367\5368\5369\5370\5371\5372\5373\5374\5375\5376\5377\5378\5379\5380\5381\5382\5383\5384\5385\5386\5387\5388\5389\5390\5391\5392\5393\5394\5395\5396\5397\5398\5399\5400\5401\5402\5403\5404\5405\5406\5407\5408\5409\5410\5411\5412\5413\5414\5415\5416\5417\5418\5419\5420\5421\5422\5423\5424\5425\5426\5427\5428\5429\5430\5431\5432\5433\5434\5435\5436\5437\5438\5439\5440\5441\5442\5443\5444\5445\5446\5447\5448\5449\5450\5451\5452\5453\5454\5455\5456\5457\5458\5459\5460\5461\5462\5463\5464\5465\5466\5467\5468\5469\5470\5471\5472\5473\5474\5475\5476\5477\5478\5479\5480\5481\5482\5483\5484\5485\5486\5487\5488\5489\5490\5491\5492\5493\5494\5495\5496\5497\5498\5499\5500\5501\5502\5503\5504\5505\5506\5507\5508\5509\5510\5511\5512\5513\5514\5515\5516\5517\5518\5519\5520\5521\5522\5523\5524\5525\5526\5527\5528\5529\5530\5531\5532\5533\5534\5535\5536\5537\5538\5539\5540\5541\5542\5543\5544\5545\5546\5547\5548\5549\5550\5551\5552\5553\5554\5555\5556\5557\5558\5559\5560\5561\5562\5563\5564\5565\5566\5567\5568\5569\5570\5571\5572\5573\5574\5575\5576\5577\5578\5579\5580\5581\5582\5583\5584\5585\5586\5587\5588\5589\5590\5591\5592\5593\5594\5595\5596\5597\5598\5599\5600\5601\5602\5603\5604\5605\5606\5607\5608\5609\5610\5611\5612\5613\5614\5615\5616\5617\5618\5619\5620\5621\5622\5623\5624\5625\5626\5627\5628\5629\5630\5631\5632\5633\5634\5635\5636\5637\5638\5639\5640\5641\5642\5643\5644\5645\5646\5647\5648\5649\5650\5651\5652\5653\5654\5655\5656\5657\5658\5659\5660\5661\5662\5663\5664\5665\5666\5667\5668\5669\5670\5671\5672\5673\5674\5675\5676\5677\5678\5679\5680\5681\5682\5683\5684\5685\5686\5687\5688\5689\5690\5691\5692\5693\5694\5695\5696\5697\5698\5699\5700\5701\5702\5703\5704\5705\5706\5707\5708\5709\5710\5711\5712\5713\5714\5715\5716\5717\5718\5719\5720\5721\5722\5723\5724\5725\5726\5727\5728\5729\5730\5731\5732\5733\5734\5735\5736\5737\5738\5739\5740\5741\5742\5743\5744\5745\5746\5747\5748\5749\5750\5751\5752\5753\5754\5755\5756\5757\5758\5759\5760\5761\5762\5763\5764\5765\5766\5767\5768\5769\5770\5771\5772\5773\5774\5775\5776\5777\5778\5779\5780\5781\5782\5783\5784\5785\5786\5787\5788\5789\5790\5791\5792\5793\5794\5795\5796\5797\5798\5799\5800\5801\5802\5803\5804\5805\5806\5807\5808\5809\5810\5811\5812\5813\5814\5815\5816\5817\5818\5819\5820\5821\5822\5823\5824\5825\5826\5827\5828\5829\5830\5831\5832\5833\5834\5835\5836\5837\5838\5839\5840\5841\5842\5843\5844\5845\5846\5847\5848\5849\5850\5851\5852\5853\5854\5855\5856\5857\5858\5859\5860\5861\5862\5863\5864\5865\5866\5867\5868\5869\5870\5871\5872\5873\5874\5875\5876\5877\5878\5879\5880\5881\5882\5883\5884\5885\5886\5887\5888\5889\5890\5891\5892\5893\5894\5895\5896\5897\5898\5899\5900\5901\5902\5903\5904\5905\5906\5907\5908\5909\5910\5911\5912\5913\5914\5915\5916\5917\5918\5919\5920\5921\5922\5923\5924\5925\5926\5927\5928\5929\5930\5931\5932\5933\5934\5935\5936\5937\5938\5939\5940\5941\5942\5943\5944\5945\5946\5947\5948\5949\5950\5951\5952\5953\5954\5955\5956\5957\5958\5959\5960\5961\5962\5963\5964\5965\5966\5967\5968\5969\5970\5971\5972\5973\5974\5975\5976\5977\5978\5979\5980\5981\5982\5983\5984\5985\5986\5987\5988\5989\5990\5991\5992\5993\5994\5995\5996\5997\5998\5999\6000\6001\6002\6003\6004\6005\6006\6007\6008\6009\6010\6011\6012\6013\6014\6015\6016\6017\6018\6019\6020\6021\6022\6023\6024\6025\6026\6027\6028\6029\6030\6031\6032\6033\6034\6035\6036\6037\6038\6039\6040\6041\6042\6043\6044\6045\6046\6047\6048\6049\6050\6051\6052\6053\6054\6055\6056\6057\6058\6059\6060\6061\6062\6063\6064\6065\6066\6067\6068\6069\6070\6071\6072\6073\6074\6075\6076\6077\6078\6079\6080\6081\6082\6083\6084\6085\6086\6087\6088\6089\6090\6091\6092\6093\6094\6095\6096\6097\6098\6099\6100\6101\6102\6103\6104\6105\6106\6107\6108\6109\6110\6111\6112\6113\6114\6115\6116\6117\6118\6119\6120\6121\6122\6123\6124\6125\6126\6127\6128\6129\6130\6131\6132\6133\6134\6135\6136\6137\6138\6139\6140\6141\6142\6143\6144\6145\6146\6147\6148\6149\6150\6151\6152\6153\6154\6155\6156\6157\6158\6159\6160\6161\6162\6163\6164\6165\6166\6167\6168\6169\6170\6171\6172\6173\6174\6175\6176\6177\6178\6179\6180\6181\6182\6183\6184\6185\6186\6187\6188\6189\6190\6191\6192\6193\6194\6195\6196\6197\6198\6199\6200\6201\6202\6203\6204\6205\6206\6207\6208\6209\6210\6211\6212\6213\6214\6215\6216\6217\6218\6219\6220\6221\6222\6223\6224\6225\6226\6227\6228\6229\6230\6231\6232\6233\6234\6235\6236\6237\6238\6239\6240\6241\6242\6243\6244\6245\6246\6247\6248\6249\6250\6251\6252\6253\6254\6255\6256\6257\6258\6259\6260\6261\6262\6263\6264\6265\6266\6267\6268\6269\6270\6271\6272\6273\6274\6275\6276\6277\6278\6279\6280\6281\6282\6283\6284\6285\6286\6287\6288\6289\6290\6291\6292\6293\6294\6295\6296\6297\6298\6299\6300\6301\6302\6303\6304\6305\6306\6307\6308\6309\6310\6311\6312\6313\6314\6315\6316\6317\6318\6319\6320\6321\6322\6323\6324\6325\6326\6327\6328\6329\6330\6331\6332\6333\6334\6335\6336\6337\6338\6339\6340\6341\6342\6343\6344\6345\6346\6347\6348\6349\6350\6351\6352\6353\6354\6355\6356\6357\6358\6359\6360\6361\6362\6363\6364\6365\6366\6367\6368\6369\6370\6371\6372\6373\6374\6375\6376\6377\6378\6379\6380\6381\6382\6383\6384\6385\6386\6387\6388\6389\6390\6391\6392\6393\6394\6395\6396\6397\6398\6399\6400\6401\6402\6403\6404\6405\6406\6407\6408\6409\6410\6411\6412\6413\6414\6415\6416\6417\6418\6419\6420\6421\6422\6423\6424\6425\6426\6427\6428\6429\6430\6431\6432\6433\6434\6435\6436\6437\6438\6439\6440\6441\6442\6443\6444\6445\6446\6447\6448\6449\6450\6451\6452\6453\6454\6455\6456\6457\6458\6459\6460\6461\6462\6463\6464\6465\6466\6467\6468\6469\6470\6471\6472\6473\6474\6475\6476\6477\6478\6479\6480\6481\6482\6483\6484\6485\6486\6487\6488\6489\6490\6491\6492\6493\6494\6495\6496\6497\6498\6499\6500\6501\6502\6503\6504\6505\6506\6507\6508\6509\6510\6511\6512\6513\6514\6515\6516\6517\6518\6519\6520\6521\6522\6523\6524\6525\6526\6527\6528\6529\6530\6531\6532\6533\6534\6535\6536\6537\6538\6539\6540\6541\6542\6543\6544\6545\6546\6547\6548\6549\6550\6551\6552\6553\6554\6555\6556\6557\6558\6559\6560\6561\6562\6563\6564\6565\6566\6567\6568\6569\6570\6571\6572\6573\6574\6575\6576\6577\6578\6579\6580\6581\6582\6583\6584\6585\6586\6587\6588\6589\6590\6591\6592\6593\6594\6595\6596\6597\6598\6599\6600\6601\6602\6603\6604\6605\6606\6607\6608\6609\6610\6611\6612\6613\6614\6615\6616\6617\6618\6619\6620\6621\6622\6623\6624\6625\6626\6627\6628\6629\6630\6631\6632\6633\6634\6635\6636\6637\6638\6639\6640\6641\6642\6643\6644\6645\6646\6647\6648\6649\6650\6651\6652\6653\6654\6655\6656\6657\6658\6659\6660\6661\6662\6663\6664\6665\6666\6667\6668\6669\6670\6671\6672\6673\6674\6675\6676\6677\6678\6679\6680\6681\6682\6683\6684\6685\6686\6687\6688\6689\6690\6691\6692\6693\6694\6695\6696\6697\6698\6699\6700\6701\6702\6703\6704\6705\6706\6707\6708\6709\6710\6711\6712\6713\6714\6715\6716\6717\6718\6719\6720\6721\6722\6723\6724\6725\6726\6727\6728\6729\6730\6731\6732\6733\6734\6735\6736\6737\6738\6739\6740\6741\6742\6743\6744\6745\6746\6747\6748\6749\6750\6751\6752\6753\6754\6755\6756\6757\6758\6759\6760\6761\6762\6763\6764\6765\6766\6767\6768\6769\6770\6771\6772\6773\6774\6775\6776\6777\6778\6779\6780\6781\6782\6783\6784\6785\6786\6787\6788\6789\6790\6791\6792\6793\6794\6795\6796\6797\6798\6799\6800\6801\6802\6803\6804\6805\6806\6807\6808\6809\6810\6811\6812\6813\6814\6815\6816\6817\6818\6819\6820\6821\6822\6823\6824\6825\6826\6827\6828\6829\6830\6831\6832\6833\6834\6835\6836\6837\6838\6839\6840\6841\6842\6843\6844\6845\6846\6847\6848\6849\6850\6851\6852\6853\6854\6855\6856\6857\6858\6859\6860\6861\6862\6863\6864\6865\6866\6867\6868\6869\6870\6871\6872\6873\6874\6875\6876\6877\6878\6879\6880\6881\6882\6883\6884\6885\6886\6887\6888\6889\6890\6891\6892\6893\6894\6895\6896\6897\6898\6899\6900\6901\6902\6903\6904\6905\6906\6907\6908\6909\6910\6911\6912\6913\6914\6915\6916\6917\6918\6919\6920\6921\6922\6923\6924\6925\6926\6927\6928\6929\6930\6931\6932\6933\6934\6935\6936\6937\6938\6939\6940\6941\6942\6943\6944\6945\6946\6947\6948\6949\6950\6951\6952\6953\6954\6955\6956\6957\6958\6959\6960\6961\6962\6963\6964\6965\6966\6967\6968\6969\6970\6971\6972\6973\6974\6975\6976\6977\6978\6979\6980\6981\6982\6983\6984\6985\6986\6987\6988\6989\6990\6991\6992\6993\6994\6995\6996\6997\6998\6999\7000\7001\7002\7003\7004\7005\7006\7007\7008\7009\7010\7011\7012\7013\7014\7015\7016\7017\7018\7019\7020\7021\7022\7023\7024\7025\7026\7027\7028\7029\7030\7031\7032\7033\7034\7035\7036\7037\7038\7039\7040\7041\7042\7043\7044\7045\7046\7047\7048\7049\7050\7051\7052\7053\7054\7055\7056\7057\7058\7059\7060\7061\7062\7063\7064\7065\7066\7067\7068\7069\7070\7071\7072\7073\7074\7075\7076\7077\7078\7079\7080\7081\7082\7083\7084\7085\7086\7087\7088\7089\7090\7091\7092\7093\7094\7095\7096\7097\7098\7099\7100\7101\7102\7103\7104\7105\7106\7107\7108\7109\7110\7111\7112\7113\7114\7115\7116\7117\7118\7119\7120\7121\7122\7123\7124\7125\7126\7127\7128\7129\7130\7131\7132\7133\7134\7135\7136\7137\7138\7139\7140\7141\7142\7143\7144\7145\7146\7147\7148\7149\7150\7151\7152\7153\7154\7155\7156\7157\7158\7159\7160\7161\7162\7163\7164\7165\7166\7167\7168\7169\7170\7171\7172\7173\7174\7175\7176\7177\7178\7179\7180\7181\7182\7183\7184\7185\7186\7187\7188\7189\7190\7191\7192\7193\7194\7195\7196\7197\7198\7199\7200\7201\7202\7203\7204\7205\7206\7207\7208\7209\7210\7211\7212\7213\7214\7215\7216\7217\7218\7219\7220\7221\7222\7223\7224\7225\7226\7227\7228\7229\7230\7231\7232\7233\7234\7235\7236\7237\7238\7239\7240\7241\7242\7243\7244\7245\7246\7247\7248\7249\7250\7251\7252\7253\7254\7255\7256\7257\7258\7259\7260\7261\7262\7263\7264\7265\7266\7267\7268\7269\7270\7271\7272\7273\7274\7275\7276\7277\7278\7279\7280\7281\7282\7283\7284\7285\7286\7287\7288\7289\7290\7291\7292\7293\7294\7295\7296\7297\7298\7299\7300\7301\7302\7303\7304\7305\7306\7307\7308\7309\7310\7311\7312\7313\7314\7315\7316\7317\7318\7319\7320\7321\7322\7323\7324\7325\7326\7327\7328\7329\7330\7331\7332\7333\7334\7335\7336\7337\7338\7339\7340\7341\7342\7343\7344\7345\7346\7347\7348\7349\7350\7351\7352\7353\7354\7355\7356\7357\7358\7359\7360\7361\7362\7363\7364\7365\7366\7367\7368\7369\7370\7371\7372\7373\7374\7375\7376\7377\7378\7379\7380\7381\7382\7383\7384\7385\7386\7387\7388\7389\7390\7391\7392\7393\7394\7395\7396\7397\7398\7399\7400\7401\7402\7403\7404\7405\7406\7407\7408\7409\7410\7411\7412\7413\7414\7415\7416\7417\7418\7419\7420\7421\7422\7423\7424\7425\7426\7427\7428\7429\7430\7431\7432\7433\7434\7435\7436\7437\7438\7439\7440\7441\7442\7443\7444\7445\7446\7447\7448\7449\7450\7451\7452\7453\7454\7455\7456\7457\7458\7459\7460\7461\7462\7463\7464\7465\7466\7467\7468\7469\7470\7471\7472\7473\7474\7475\7476\7477\7478\7479\7480\7481\7482\7483\7484\7485\7486\7487\7488\7489\7490\7491\7492\7493\7494\7495\7496\7497\7498\7499\7500\7501\7502\7503\7504\7505\7506\7507\7508\7509\7510\7511\7512\7513\7514\7515\7516\7517\7518\7519\7520\7521\7522\7523\7524\7525\7526\7527\7528\7529\7530\7531\7532\7533\7534\7535\7536\7537\7538\7539\7540\7541\7542\7543\7544\7545\7546\7547\7548\7549\7550\7551\7552\7553\7554\7555\7556\7557\7558\7559\7560\7561\7562\7563\7564\7565\7566\7567\7568\7569\7570\7571\7572\7573\7574\7575\7576\7577\7578\7579\7580\7581\7582\7583\7584\7585\7586\7587\7588\7589\7590\7591\7592\7593\7594\7595\7596\7597\7598\7599\7600\7601\7602\7603\7604\7605\7606\7607\7608\7609\7610\7611\7612\7613\7614\7615\7616\7617\7618\7619\7620\7621\7622\7623\7624\7625\7626\7627\7628\7629\7630\7631\7632\7633\7634\7635\7636\7637\7638\7639\7640\7641\7642\7643\7644\7645\7646\7647\7648\7649\7650\7651\7652\7653\7654\7655\7656\7657\7658\7659\7660\7661\7662\7663\7664\7665\7666\7667\7668\7669\7670\7671\7672\7673\7674\7675\7676\7677\7678\7679\7680\7681\7682\7683\7684\7685\7686\7687\7688\7689\7690\7691\7692\7693\7694\7695\7696\7697\7698\7699\7700\7701\7702\7703\7704\7705\7706\7707\7708\7709\7710\7711\7712\7713\7714\7715\7716\7717\7718\7719\7720\7721\7722\7723\7724\7725\7726\7727\7728\7729\7730\7731\7732\7733\7734\7735\7736\7737\7738\7739\7740\7741\7742\7743\7744\7745\7746\7747\7748\7749\7750\7751\7752\7753\7754\7755\7756\7757\7758\7759\7760\7761\7762\7763\7764\7765\7766\7767\7768\7769\7770\7771\7772\7773\7774\7775\7776\7777\7778\7779\7780\7781\7782\7783\7784\7785\7786\7787\7788\7789\7790\7791\7792\7793\7794\7795\7796\7797\7798\7799\7800\7801\7802\7803\7804\7805\7806\7807\7808\7809\7810\7811\7812\7813\7814\7815\7816\7817\7818\7819\7820\7821\7822\7823\7824\7825\7826\7827\7828\7829\7830\7831\7832\7833\7834\7835\7836\7837\7838\7839\7840\7841\7842\7843\7844\7845\7846\7847\7848\7849\7850\7851\7852\7853\7854\7855\7856\7857\7858\7859\7860\7861\7862\7863\7864\7865\7866\7867\7868\7869\7870\7871\7872\7873\7874\7875\7876\7877\7878\7879\7880\7881\7882\7883\7884\7885\7886\7887\7888\7889\7890\7891\7892\7893\7894\7895\7896\7897\7898\7899\7900\7901\7902\7903\7904\7905\7906\7907\7908\7909\7910\7911\7912\7913\7914\7915\7916\7917\7918\7919\7920\7921\7922\7923\7924\7925\7926\7927\7928\7929\7930\7931\7932\7933\7934\7935\7936\7937\7938\7939\7940\7941\7942\7943\7944\7945\7946\7947\7948\7949\7950\7951\7952\7953\7954\7955\7956\7957\7958\7959\7960\7961\7962\7963\7964\7965\7966\7967\7968\7969\7970\7971\7972\7973\7974\7975\7976\7977\7978\7979\7980\7981\7982\7983\7984\7985\7986\7987\7988\7989\7990\7991\7992\7993\7994\7995\7996\7997\7998\7999\8000\8001\8002\8003\8004\8005\8006\8007\8008\8009\8010\8011\8012\8013\8014\8015\8016\8017\8018\8019\8020\8021\8022\8023\8024\8025\8026\8027\8028\8029\8030\8031\8032\8033\8034\8035\8036\8037\8038\8039\8040\8041\8042\8043\8044\8045\8046\8047\8048\8049\8050\8051\8052\8053\8054\8055\8056\8057\8058\8059\8060\8061\8062\8063\8064\8065\8066\8067\8068\8069\8070\8071\8072\8073\8074\8075\8076\8077\8078\8079\8080\8081\8082\8083\8084\8085\8086\8087\8088\8089\8090\8091\8092\8093\8094\8095\8096\8097\8098\8099\8100\8101\8102\8103\8104\8105\8106\8107\8108\8109\8110\8111\8112\8113\8114\8115\8116\8117\8118\8119\8120\8121\8122\8123\8124\8125\8126\8127\8128\8129\8130\8131\8132\8133\8134\8135\8136\8137\8138\8139\8140\8141\8142\8143\8144\8145\8146\8147\8148\8149\8150\8151\8152\8153\8154\8155\8156\8157\8158\8159\8160\8161\8162\8163\8164\8165\8166\8167\8168\8169\8170\8171\8172\8173\8174\8175\8176\8177\8178\8179\8180\8181\8182\8183\8184\8185\8186\8187\8188\8189\8190\8191\8192\8193\8194\8195\8196\8197\8198\8199\8200\8201\8202\8203\8204\8205\8206\8207\8208\8209\8210\8211\8212\8213\8214\8215\8216\8217\8218\8219\8220\8221\8222\8223\8224\8225\8226\8227\8228\8229\8230\8231\8232\8233\8234\8235\8236\8237\8238\8239\8240\8241\8242\8243\8244\8245\8246\8247\8248\8249\8250\8251\8252\8253\8254\8255\8256\8257\8258\8259\8260\8261\8262\8263\8264\8265\8266\8267\8268\8269\8270\8271\8272\8273\8274\8275\8276\8277\8278\8279\8280\8281\8282\8283\8284\8285\8286\8287\8288\8289\8290\8291\8292\8293\8294\8295\8296\8297\8298\8299\8300\8301\8302\8303\8304\8305\8306\8307\8308\8309\8310\8311\8312\8313\8314\8315\8316\8317\8318\8319\8320\8321\8322\8323\8324\8325\8326\8327\8328\8329\8330\8331\8332\8333\8334\8335\8336\8337\8338\8339\8340\8341\8342\8343\8344\8345\8346\8347\8348\8349\8350\8351\8352\8353\8354\8355\8356\8357\8358\8359\8360\8361\8362\8363\8364\8365\8366\8367\8368\8369\8370\8371\8372\8373\8374\8375\8376\8377\8378\8379\8380\8381\8382\8383\8384\8385\8386\8387\8388\8389\8390\8391\8392\8393\8394\8395\8396\8397\8398\8399\8400\8401\8402\8403\8404\8405\8406\8407\8408\8409\8410\8411\8412\8413\8414\8415\8416\8417\8418\8419\8420\8421\8422\8423\8424\8425\8426\8427\8428\8429\8430\8431\8432\8433\8434\8435\8436\8437\8438\8439\8440\8441\8442\8443\8444\8445\8446\8447\8448\8449\8450\8451\8452\8453\8454\8455\8456\8457\8458\8459\8460\8461\8462\8463\8464\8465\8466\8467\8468\8469\8470\8471\8472\8473\8474\8475\8476\8477\8478\8479\8480\8481\8482\8483\8484\8485\8486\8487\8488\8489\8490\8491\8492\8493\8494\8495\8496\8497\8498\8499\8500\8501\8502\8503\8504\8505\8506\8507\8508\8509\8510\8511\8512\8513\8514\8515\8516\8517\8518\8519\8520\8521\8522\8523\8524\8525\8526\8527\8528\8529\8530\8531\8532\8533\8534\8535\8536\8537\8538\8539\8540\8541\8542\8543\8544\8545\8546\8547\8548\8549\8550\8551\8552\8553\8554\8555\8556\8557\8558\8559\8560\8561\8562\8563\8564\8565\8566\8567\8568\8569\8570\8571\8572\8573\8574\8575\8576\8577\8578\8579\8580\8581\8582\8583\8584\8585\8586\8587\8588\8589\8590\8591\8592\8593\8594\8595\8596\8597\8598\8599\8600\8601\8602\8603\8604\8605\8606\8607\8608\8609\8610\8611\8612\8613\8614\8615\8616\8617\8618\8619\8620\8621\8622\8623\8624\8625\8626\8627\8628\8629\8630\8631\8632\8633\8634\8635\8636\8637\8638\8639\8640\8641\8642\8643\8644\8645\8646\8647\8648\8649\8650\8651\8652\8653\8654\8655\8656\8657\8658\8659\8660\8661\8662\8663\8664\8665\8666\8667\8668\8669\8670\8671\8672\8673\8674\8675\8676\8677\8678\8679\8680\8681\8682\8683\8684\8685\8686\8687\8688\8689\8690\8691\8692\8693\8694\8695\8696\8697\8698\8699\8700\8701\8702\8703\8704\8705\8706\8707\8708\8709\8710\8711\8712\8713\8714\8715\8716\8717\8718\8719\8720\8721\8722\8723\8724\8725\8726\8727\8728\8729\8730\8731\8732\8733\8734\8735\8736\8737\8738\8739\8740\8741\8742\8743\8744\8745\8746\8747\8748\8749\8750\8751\8752\8753\8754\8755\8756\8757\8758\8759\8760\8761\8762\8763\8764\8765\8766\8767\8768\8769\8770\8771\8772\8773\8774\8775\8776\8777\8778\8779\8780\8781\8782\8783\8784\8785\8786\8787\8788\8789\8790\8791\8792\8793\8794\8795\8796\8797\8798\8799\8800\8801\8802\8803\8804\8805\8806\8807\8808\8809\8810\8811\8812\8813\8814\8815\8816\8817\8818\8819\8820\8821\8822\8823\8824\8825\8826\8827\8828\8829\8830\8831\8832\8833\8834\8835\8836\8837\8838\8839\8840\8841\8842\8843\8844\8845\8846\8847\8848\8849\8850\8851\8852\8853\8854\8855\8856\8857\8858\8859\8860\8861\8862\8863\8864\8865\8866\8867\8868\8869\8870\8871\8872\8873\8874\8875\8876\8877\8878\8879\8880\8881\8882\8883\8884\8885\8886\8887\8888\8889\8890\8891\8892\8893\8894\8895\8896\8897\8898\8899\8900\8901\8902\8903\8904\8905\8906\8907\8908\8909\8910\8911\8912\8913\8914\8915\8916\8917\8918\8919\8920\8921\8922\8923\8924\8925\8926\8927\8928\8929\8930\8931\8932\8933\8934\8935\8936\8937\8938\8939\8940\8941\8942\8943\8944\8945\8946\8947\8948\8949\8950\8951\8952\8953\8954\8955\8956\8957\8958\8959\8960\8961\8962\8963\8964\8965\8966\8967\8968\8969\8970\8971\8972\8973\8974\8975\8976\8977\8978\8979\8980\8981\8982\8983\8984\8985\8986\8987\8988\8989\8990\8991\8992\8993\8994\8995\8996\8997\8998\8999\9000\9001\9002\9003\9004\9005\9006\9007\9008\9009\9010\9011\9012\9013\9014\9015\9016\9017\9018\9019\9020\9021\9022\9023\9024\9025\9026\9027\9028\9029\9030\9031\9032\9033\9034\9035\9036\9037\9038\9039\9040\9041\9042\9043\9044\9045\9046\9047\9048\9049\9050\9051\9052\9053\9054\9055\9056\9057\9058\9059\9060\9061\9062\9063\9064\9065\9066\9067\9068\9069\9070\9071\9072\9073\9074\9075\9076\9077\9078\9079\9080\9081\9082\9083\9084\9085\9086\9087\9088\9089\9090\9091\9092\9093\9094\9095\9096\9097\9098\9099\9100\9101\9102\9103\9104\9105\9106\9107\9108\9109\9110\9111\9112\9113\9114\9115\9116\9117\9118\9119\9120\9121\9122\9123\9124\9125\9126\9127\9128\9129\9130\9131\9132\9133\9134\9135\9136\9137\9138\9139\9140\9141\9142\9143\9144\9145\9146\9147\9148\9149\9150\9151\9152\9153\9154\9155\9156\9157\9158\9159\9160\9161\9162\9163\9164\9165\9166\9167\9168\9169\9170\9171\9172\9173\9174\9175\9176\9177\9178\9179\9180\9181\9182\9183\9184\9185\9186\9187\9188\9189\9190\9191\9192\9193\9194\9195\9196\9197\9198\9199\9200\9201\9202\9203\9204\9205\9206\9207\9208\9209\9210\9211\9212\9213\9214\9215\9216\9217\9218\9219\9220\9221\9222\9223\9224\9225\9226\9227\9228\9229\9230\9231\9232\9233\9234\9235\9236\9237\9238\9239\9240\9241\9242\9243\9244\9245\9246\9247\9248\9249\9250\9251\9252\9253\9254\9255\9256\9257\9258\9259\9260\9261\9262\9263\9264\9265\9266\9267\9268\9269\9270\9271\9272\9273\9274\9275\9276\9277\9278\9279\9280\9281\9282\9283\9284\9285\9286\9287\9288\9289\9290\9291\9292\9293\9294\9295\9296\9297\9298\9299\9300\9301\9302\9303\9304\9305\9306\9307\9308\9309\9310\9311\9312\9313\9314\9315\9316\9317\9318\9319\9320\9321\9322\9323\9324\9325\9326\9327\9328\9329\9330\9331\9332\9333\9334\9335\9336\9337\9338\9339\9340\9341\9342\9343\9344\9345\9346\9347\9348\9349\9350\9351\9352\9353\9354\9355\9356\9357\9358\9359\9360\9361\9362\9363\9364\9365\9366\9367\9368\9369\9370\9371\9372\9373\9374\9375\9376\9377\9378\9379\9380\9381\9382\9383\9384\9385\9386\9387\9388\9389\9390\9391\9392\9393\9394\9395\9396\9397\9398\9399\9400\9401\9402\9403\9404\9405\9406\9407\9408\9409\9410\9411\9412\9413\9414\9415\9416\9417\9418\9419\9420\9421\9422\9423\9424\9425\9426\9427\9428\9429\9430\9431\9432\9433\9434\9435\9436\9437\9438\9439\9440\9441\9442\9443\9444\9445\9446\9447\9448\9449\9450\9451\9452\9453\9454\9455\9456\9457\9458\9459\9460\9461\9462\9463\9464\9465\9466\9467\9468\9469\9470\9471\9472\9473\9474\9475\9476\9477\9478\9479\9480\9481\9482\9483\9484\9485\9486\9487\9488\9489\9490\9491\9492\9493\9494\9495\9496\9497\9498\9499\9500\9501\9502\9503\9504\9505\9506\9507\9508\9509\9510\9511\9512\9513\9514\9515\9516\9517\9518\9519\9520\9521\9522\9523\9524\9525\9526\9527\9528\9529\9530\9531\9532\9533\9534\9535\9536\9537\9538\9539\9540\9541\9542\9543\9544\9545\9546\9547\9548\9549\9550\9551\9552\9553\9554\9555\9556\9557\9558\9559\9560\9561\9562\9563\9564\9565\9566\9567\9568\9569\9570\9571\9572\9573\9574\9575\9576\9577\9578\9579\9580\9581\9582\9583\9584\9585\9586\9587\9588\9589\9590\9591\9592\9593\9594\9595\9596\9597\9598\9599\9600\9601\9602\9603\9604\9605\9606\9607\9608\9609\9610\9611\9612\9613\9614\9615\9616\9617\9618\9619\9620\9621\9622\9623\9624\9625\9626\9627\9628\9629\9630\9631\9632\9633\9634\9635\9636\9637\9638\9639\9640\9641\9642\9643\9644\9645\9646\9647\9648\9649\9650\9651\9652\9653\9654\9655\9656\9657\9658\9659\9660\9661\9662\9663\9664\9665\9666\9667\9668\9669\9670\9671\9672\9673\9674\9675\9676\9677\9678\9679\9680\9681\9682\9683\9684\9685\9686\9687\9688\9689\9690\9691\9692\9693\9694\9695\9696\9697\9698\9699\9700\9701\9702\9703\9704\9705\9706\9707\9708\9709\9710\9711\9712\9713\9714\9715\9716\9717\9718\9719\9720\9721\9722\9723\9724\9725\9726\9727\9728\9729\9730\9731\9732\9733\9734\9735\9736\9737\9738\9739\9740\9741\9742\9743\9744\9745\9746\9747\9748\9749\9750\9751\9752\9753\9754\9755\9756\9757\9758\9759\9760\9761\9762\9763\9764\9765\9766\9767\9768\9769\9770\9771\9772\9773\9774\9775\9776\9777\9778\9779\9780\9781\9782\9783\9784\9785\9786\9787\9788\9789\9790\9791\9792\9793\9794\9795\9796\9797\9798\9799\9800\9801\9802\9803\9804\9805\9806\9807\9808\9809\9810\9811\9812\9813\9814\9815\9816\9817\9818\9819\9820\9821\9822\9823\9824\9825\9826\9827\9828\9829\9830\9831\9832\9833\9834\9835\9836\9837\9838\9839\9840\9841\9842\9843\9844\9845\9846\9847\9848\9849\9850\9851\9852\9853\9854\9855\9856\9857\9858\9859\9860\9861\9862\9863\9864\9865\9866\9867\9868\9869\9870\9871\9872\9873\9874\9875\9876\9877\9878\9879\9880\9881\9882\9883\9884\9885\9886\9887\9888\9889\9890\9891\9892\9893\9894\9895\9896\9897\9898\9899\9900\9901\9902\9903\9904\9905\9906\9907\9908\9909\9910\9911\9912\9913\9914\9915\9916\9917\9918\9919\9920\9921\9922\9923\9924\9925\9926\9927\9928\9929\9930\9931\9932\9933\9934\9935\9936\9937\9938\9939\9940\9941\9942\9943\9944\9945\9946\9947\9948\9949\9950\9951\9952\9953\9954\9955\9956\9957\9958\9959\9960\9961\9962\9963\9964\9965\9966\9967\9968\9969\9970\9971\9972\9973\9974\9975\9976\9977\9978\9979\9980\9981\9982\9983\9984\9985\9986\9987\9988\9989\9990\9991\9992\9993\9994\9995\9996\9997\9998\9999\10000</X2Data>
   <Y1Data>0.944\0.935\0.923\0.909\0.895\0.88\0.865\0.85\0.836\0.823\0.81\0.798\0.786\0.775\0.764\0.754\0.744\0.735\0.727\0.718\0.71\0.703\0.695\0.688\0.681\0.675\0.668\0.662\0.656\0.651\0.645\0.64\0.635\0.63\0.625\0.621\0.616\0.612\0.608\0.604\0.6\0.597\0.593\0.59\0.586\0.583\0.58\0.577\0.574\0.571\0.569\0.566\0.563\0.561\0.559\0.556\0.554\0.552\0.549\0.547\0.545\0.543\0.541\0.539\0.537\0.535\0.533\0.531\0.53\0.528\0.526\0.524\0.523\0.521\0.519\0.518\0.516\0.514\0.513\0.511\0.51\0.508\0.507\0.505\0.504\0.503\0.501\0.5\0.498\0.497\0.496\0.494\0.493\0.492\0.49\0.489\0.488\0.487\0.485\0.484\0.483\0.482\0.48\0.479\0.478\0.477\0.476\0.474\0.473\0.472\0.471\0.47\0.469\0.468\0.467\0.465\0.464\0.463\0.462\0.461\0.46\0.459\0.458\0.457\0.456\0.455\0.454\0.453\0.452\0.451\0.45\0.449\0.448\0.447\0.446\0.445\0.444\0.443\0.442\0.441\0.44\0.439\0.438\0.437\0.437\0.436\0.435\0.434\0.433\0.432\0.431\0.43\0.43\0.429\0.428\0.427\0.426\0.425\0.425\0.424\0.423\0.422\0.421\0.421\0.42\0.419\0.418\0.417\0.417\0.416\0.415\0.414\0.414\0.413\0.412\0.411\0.411\0.41\0.409\0.408\0.408\0.407\0.406\0.406\0.405\0.404\0.403\0.403\0.402\0.401\0.401\0.4\0.399\0.399\0.398\0.397\0.397\0.396\0.395\0.395\0.394\0.393\0.393\0.392\0.391\0.391\0.39\0.39\0.389\0.388\0.388\0.387\0.387\0.386\0.385\0.385\0.384\0.384\0.383\0.382\0.382\0.381\0.381\0.38\0.379\0.379\0.378\0.378\0.377\0.377\0.376\0.376\0.375\0.374\0.374\0.373\0.373\0.372\0.372\0.371\0.371\0.37\0.37\0.369\0.369\0.368\0.368\0.367\0.367\0.366\0.366\0.365\0.365\0.364\0.364\0.363\0.363\0.362\0.362\0.361\0.361\0.36\0.36\0.36\0.359\0.359\0.358\0.358\0.357\0.357\0.356\0.356\0.356\0.355\0.355\0.354\0.354\0.354\0.353\0.353\0.352\0.352\0.351\0.351\0.351\0.35\0.35\0.35\0.349\0.349\0.348\0.348\0.348\0.347\0.347\0.347\0.346\0.346\0.345\0.345\0.345\0.344\0.344\0.344\0.343\0.343\0.343\0.342\0.342\0.342\0.341\0.341\0.341\0.34\0.34\0.34\0.339\0.339\0.339\0.338\0.338\0.338\0.338\0.337\0.337\0.337\0.336\0.336\0.336\0.335\0.335\0.335\0.335\0.334\0.334\0.334\0.333\0.333\0.333\0.333\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.328\0.328\0.328\0.328\0.327\0.327\0.327\0.327\0.326\0.326\0.326\0.326\0.326\0.325\0.325\0.325\0.325\0.324\0.324\0.324\0.324\0.324\0.323\0.323\0.323\0.323\0.323\0.322\0.322\0.322\0.322\0.322\0.321\0.321\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.319\0.319\0.318\0.318\0.318\0.318\0.318\0.317\0.317\0.317\0.317\0.317\0.317\0.316\0.316\0.316\0.316\0.316\0.316\0.315\0.315\0.315\0.315\0.315\0.315\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.281\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.27\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.269\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.268\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.267\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.266\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.265\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.264\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.263\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.262\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.26\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.259\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.253\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.252\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.251\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.25\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.249\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.248\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.247\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.246\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.245\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.244\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.243\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.242\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.241\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.24\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.239\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.238\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.237\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.236\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.235\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.234\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.233\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232\0.232</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.756\0.747\0.737\0.726\0.716\0.705\0.694\0.684\0.675\0.666\0.657\0.649\0.642\0.634\0.628\0.622\0.616\0.61\0.605\0.6\0.596\0.591\0.587\0.583\0.58\0.576\0.573\0.57\0.567\0.564\0.562\0.559\0.557\0.555\0.552\0.55\0.548\0.547\0.545\0.543\0.542\0.54\0.539\0.537\0.536\0.535\0.533\0.532\0.531\0.53\0.529\0.528\0.527\0.526\0.524\0.523\0.522\0.521\0.52\0.519\0.518\0.517\0.516\0.515\0.514\0.513\0.512\0.511\0.51\0.509\0.508\0.508\0.507\0.506\0.505\0.504\0.503\0.502\0.501\0.5\0.499\0.498\0.497\0.496\0.495\0.494\0.493\0.493\0.492\0.491\0.49\0.489\0.488\0.487\0.486\0.485\0.485\0.484\0.483\0.482\0.481\0.48\0.479\0.479\0.478\0.477\0.476\0.475\0.475\0.474\0.473\0.472\0.471\0.471\0.47\0.469\0.468\0.467\0.467\0.466\0.465\0.464\0.464\0.463\0.462\0.461\0.461\0.46\0.459\0.459\0.458\0.457\0.457\0.456\0.455\0.455\0.454\0.453\0.453\0.452\0.451\0.451\0.45\0.449\0.449\0.448\0.448\0.447\0.446\0.446\0.445\0.445\0.444\0.443\0.443\0.442\0.441\0.441\0.44\0.44\0.439\0.439\0.438\0.437\0.437\0.436\0.436\0.435\0.434\0.434\0.433\0.433\0.432\0.432\0.431\0.43\0.43\0.429\0.429\0.428\0.428\0.427\0.426\0.426\0.425\0.425\0.424\0.424\0.423\0.422\0.422\0.421\0.421\0.42\0.42\0.419\0.419\0.418\0.418\0.417\0.416\0.416\0.415\0.415\0.414\0.414\0.413\0.413\0.412\0.412\0.411\0.411\0.41\0.41\0.409\0.409\0.408\0.407\0.407\0.406\0.406\0.405\0.405\0.404\0.404\0.404\0.403\0.403\0.402\0.402\0.401\0.401\0.4\0.4\0.399\0.399\0.398\0.398\0.397\0.397\0.397\0.396\0.396\0.395\0.395\0.394\0.394\0.394\0.393\0.393\0.392\0.392\0.391\0.391\0.391\0.39\0.39\0.39\0.389\0.389\0.388\0.388\0.388\0.387\0.387\0.386\0.386\0.386\0.385\0.385\0.385\0.384\0.384\0.383\0.383\0.383\0.382\0.382\0.382\0.381\0.381\0.381\0.38\0.38\0.38\0.379\0.379\0.379\0.378\0.378\0.378\0.377\0.377\0.377\0.376\0.376\0.376\0.375\0.375\0.375\0.374\0.374\0.374\0.374\0.373\0.373\0.373\0.372\0.372\0.372\0.371\0.371\0.371\0.371\0.37\0.37\0.37\0.37\0.369\0.369\0.369\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.366\0.366\0.366\0.366\0.365\0.365\0.365\0.365\0.364\0.364\0.364\0.364\0.363\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.361\0.361\0.361\0.361\0.361\0.36\0.36\0.36\0.36\0.359\0.359\0.359\0.359\0.359\0.358\0.358\0.358\0.358\0.358\0.357\0.357\0.357\0.357\0.357\0.356\0.356\0.356\0.356\0.356\0.355\0.355\0.355\0.355\0.355\0.354\0.354\0.354\0.354\0.354\0.354\0.353\0.353\0.353\0.353\0.353\0.352\0.352\0.352\0.352\0.352\0.352\0.351\0.351\0.351\0.351\0.351\0.351\0.35\0.35\0.35\0.35\0.35\0.35\0.349\0.349\0.349\0.349\0.349\0.349\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.347\0.347\0.347\0.347\0.347\0.347\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.344\0.344\0.344\0.344\0.344\0.344\0.343\0.343\0.343\0.343\0.343\0.343\0.343\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.342\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.337\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.336\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.335\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.334\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.333\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.327\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.284\0.284\0.284\0.284\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.282\0.282\0.282\0.283\0.283\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.283\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.282\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.283\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.284\0.285\0.285\0.285\0.285\0.284\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.285\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.287\0.287\0.287\0.287\0.286\0.286\0.286\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.288\0.288\0.287\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.29\0.29\0.29\0.29\0.29\0.289\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.291\0.291\0.291\0.29\0.29\0.29\0.29\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.29\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.291\0.292\0.292\0.292\0.292\0.291\0.291\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.291\0.292\0.292\0.292\0.292\0.291\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.293\0.292\0.292\0.292\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.293\0.293\0.293\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.292\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.293\0.292\0.292\0.292\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.293\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.293\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.293\0.293\0.292\0.293\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.292\0.292\0.292\0.293\0.293\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.292\0.292\0.292\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.292\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.294\0.293\0.293\0.293\0.293\0.293\0.293\0.294\0.294\0.294\0.294\0.294\0.293\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.295\0.294\0.295\0.295\0.294\0.294\0.294\0.294\0.294\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.296\0.296\0.295\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.297\0.297\0.297\0.296\0.296\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.298\0.297\0.298\0.298\0.297\0.297\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.298\0.299\0.298\0.298\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.3\0.299\0.299\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.299\0.299\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.301\0.3\0.301\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.301\0.301\0.3\0.3\0.3\0.3\0.301\0.3\0.301\0.301\0.3\0.301\0.301\0.301\0.301\0.301\0.301\0.3\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.302\0.301\0.302\0.302\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.302\0.301\0.301\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.303\0.303\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.303\0.303\0.302\0.302\0.303\0.302\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.303\0.303\0.303\0.303\0.302\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.303\0.302\0.303\0.303\0.302\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.303\0.302\0.302\0.302\0.302\0.302\0.303\0.303\0.302\0.302\0.302\0.303\0.303\0.302\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.303\0.302\0.302\0.303\0.302\0.302\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>10001</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="GES3ns" Title="Adaptive moment estimation method results">
   <Caption Id="hqNC04">The following table shows the training results by the adaptive moment estimation method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.232
0.302
10000
00:00:03
Maximum number of epochs
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="zSMhPy" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="sQrvAg" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="M826Do" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="akoy99">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000869751\120.02\6.41985\8.42691
5.57533e-6\0.769358\0.0411529\0.0540187
0.000557533\76.9358\4.11529\5.40187</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="n1DrJV" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="rrin7Q">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00468445\302.251\7.5383\14.0779
1.37778e-5\0.888974\0.0221715\0.0414055
0.00137778\88.8974\2.21715\4.14055</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="oTn4UT" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="uc0Ami">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0155869\24.2374\4.68893\3.74452
6.28502e-5\0.0977316\0.018907\0.0150989
0.00628502\9.77316\1.8907\1.50989</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Sf2KW1" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="3OUnRe">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0432396\48.3228\6.35475\5.03373
0.00060055\0.671149\0.0882604\0.069913
0.060055\67.115\8.82604\6.9913</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="JU9YzN" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="BW9DjB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00605822\14.8417\0.82935\1.01376
0.000356366\0.87304\0.0487853\0.0596331
0.0356366\87.304\4.87853\5.96331</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="k6jIJR" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="LVRMGB" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="oEekoJ" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="tblGHm" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="vyh8Pr" Title="Optimization algorithm">The Levenberg-Marquardt algorithm is used here for training, which was designed to approach second-order training speed without having to compute the Hessian matrix.
The Levenberg-Marquardt algorithm can only be applied when the loss index has the form of a sum of squares (as the mean squared error or the normalized squared error). </Text>
  <DoubleLineChart Id="lixd7c" Title="Levenberg-Marquardt algorithm errors history">
   <Caption Id="mWrypt">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.08366, and the final value after 243 epochs is 0.311922.
The initial value of the selection error is 0.802151, and the final value after 243 epochs is 0.285983.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243</X2Data>
   <Y1Data>1.08\1.01\0.587\0.532\0.515\0.406\0.395\0.391\0.389\0.386\0.385\0.382\0.381\0.379\0.378\0.376\0.375\0.374\0.373\0.372\0.371\0.37\0.369\0.368\0.367\0.366\0.365\0.365\0.364\0.363\0.362\0.362\0.361\0.361\0.36\0.359\0.359\0.358\0.357\0.357\0.356\0.356\0.355\0.355\0.354\0.354\0.353\0.353\0.352\0.352\0.351\0.346\0.345\0.345\0.344\0.344\0.343\0.343\0.343\0.342\0.341\0.338\0.338\0.337\0.337\0.337\0.337\0.336\0.334\0.333\0.332\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.33\0.328\0.328\0.328\0.327\0.327\0.327\0.327\0.327\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.325\0.325\0.325\0.325\0.325\0.325\0.325\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.322\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.802\1.01\0.412\0.523\0.416\0.367\0.362\0.36\0.369\0.364\0.356\0.354\0.36\0.356\0.35\0.349\0.353\0.35\0.344\0.344\0.347\0.345\0.34\0.34\0.341\0.34\0.336\0.335\0.336\0.335\0.332\0.332\0.332\0.331\0.328\0.328\0.328\0.325\0.324\0.326\0.325\0.322\0.322\0.323\0.322\0.319\0.319\0.319\0.319\0.317\0.319\0.312\0.311\0.306\0.306\0.307\0.306\0.305\0.305\0.305\0.298\0.296\0.296\0.298\0.298\0.297\0.297\0.297\0.294\0.293\0.293\0.294\0.294\0.293\0.294\0.293\0.293\0.293\0.293\0.292\0.291\0.29\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.291\0.29\0.291\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.29\0.289\0.29\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.289\0.288\0.288\0.289\0.289\0.288\0.289\0.288\0.288\0.288\0.289\0.288\0.288\0.289\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.288\0.287\0.288\0.288\0.287\0.287\0.288\0.288\0.287\0.287\0.288\0.287\0.287\0.288\0.288\0.287\0.287\0.288\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.287\0.286\0.287\0.287\0.287\0.286\0.287\0.287\0.286\0.286\0.287\0.287\0.286\0.286\0.287\0.286\0.286\0.286\0.287\0.286\0.286\0.287\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286\0.286</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>244</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="cI8jP4" Title="Levenberg-Marquardt algorithm results">
   <Caption Id="GMnNtR">In the next  the training results by the Levenberg-Marquardt algorithm are listed.
They include final states from the neural network, the loss index and the optimization algorithm.
</Caption>
   <Data>0.312
0.286
243
00:00:07
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="E5daAT" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="ytEUCS" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="SvdEBF" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="T4N7yh">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00637436\108.183\6.55779\7.87463
4.08613e-5\0.693479\0.0420371\0.0504784
0.00408613\69.3479\4.20371\5.04784</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="URloDe" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="q28E6A">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00776863\309.338\6.78032\14.1764
2.28489e-5\0.909816\0.0199421\0.0416954
0.00228489\90.9816\1.99421\4.16954</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Wb7btF" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="uPne40">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0196629\32.3393\5.02155\4.26225
7.92857e-5\0.1304\0.0202482\0.0171865
0.00792857\13.04\2.02482\1.71865</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uJObt7" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QmXSq2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0355835\36.9497\6.08379\4.59267
0.000494215\0.513191\0.0844971\0.0637871
0.0494215\51.3191\8.44971\6.37871</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="nGEg1n" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Ath0e4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00479364\14.2091\0.825622\1.03939
0.000281979\0.835829\0.048566\0.0611407
0.0281979\83.5829\4.8566\6.11407</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="jod5Cu" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="6vurxZ" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="8TvTth" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="n4C8X1" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="YdTD60" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="UxoKhp" Title="Quasi-Newton method errors history">
   <Caption Id="Fyxux9">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.02865, and the final value after 207 epochs is 0.30321.
The initial value of the selection error is 0.77529, and the final value after 207 epochs is 0.274957.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207</X2Data>
   <Y1Data>1.03\0.688\0.603\0.573\0.511\0.466\0.44\0.426\0.415\0.398\0.389\0.38\0.37\0.363\0.356\0.351\0.347\0.344\0.341\0.338\0.335\0.333\0.331\0.33\0.329\0.327\0.326\0.324\0.323\0.322\0.322\0.321\0.32\0.319\0.318\0.318\0.317\0.317\0.316\0.315\0.315\0.314\0.314\0.313\0.312\0.312\0.312\0.311\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.775\0.617\0.575\0.53\0.449\0.416\0.401\0.387\0.375\0.359\0.362\0.355\0.34\0.318\0.309\0.303\0.301\0.296\0.28\0.271\0.271\0.272\0.272\0.272\0.271\0.27\0.272\0.27\0.273\0.274\0.272\0.273\0.272\0.27\0.272\0.273\0.273\0.271\0.273\0.273\0.273\0.276\0.274\0.273\0.272\0.272\0.27\0.271\0.269\0.268\0.268\0.268\0.269\0.269\0.269\0.269\0.27\0.271\0.27\0.27\0.271\0.271\0.273\0.271\0.272\0.272\0.272\0.273\0.273\0.272\0.27\0.271\0.271\0.271\0.271\0.27\0.271\0.27\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.274\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.275\0.275\0.276\0.276\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.275</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>208</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="DN68fG" Title="Quasi-Newton method results">
   <Caption Id="WJ8UIO">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.303
0.275
207
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="EffgcM" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="C9JKlS" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="gspzgr" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="XmXCW7">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00900269\113.673\6.08424\7.87182
5.77095e-5\0.728676\0.0390016\0.0504604
0.00577095\72.8676\3.90016\5.04604</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="nCosJD" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="1ByGZY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0457516\301.259\6.69987\13.9054
0.000134563\0.886057\0.0197055\0.0408982
0.0134563\88.6057\1.97055\4.08982</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="K7KgX6" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="sQYXUw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>8.7738e-5\38.4939\4.86101\4.32835
3.53782e-7\0.155217\0.0196008\0.017453
3.53782e-5\15.5217\1.96008\1.7453</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="worhEW" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="x66K22">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0523739\39.9525\5.85317\4.47089
0.000727415\0.554895\0.0812941\0.0620958
0.0727415\55.4895\8.12941\6.20958</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="yOao7r" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="DoYb0p">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00244188\14.3823\0.791396\1.01239
0.00014364\0.846017\0.0465527\0.0595521
0.014364\84.6017\4.65527\5.95521</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="y46Z7F" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="LiLaQA" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 97.</Text>
 </Task>
 <Task Id="e6z6GI" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="wmBQN7" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="OnD47H" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="HwjjaT" Title="Quasi-Newton method errors history">
   <Caption Id="Nb45kr">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.11433, and the final value after 204 epochs is 0.0935276.
The initial value of the selection error is 1.1717, and the final value after 204 epochs is 0.160358.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204</X2Data>
   <Y1Data>1.11\0.807\0.462\0.403\0.302\0.267\0.244\0.22\0.202\0.191\0.173\0.163\0.155\0.148\0.143\0.139\0.137\0.135\0.133\0.131\0.128\0.125\0.123\0.122\0.12\0.119\0.118\0.116\0.115\0.113\0.112\0.111\0.11\0.109\0.108\0.108\0.107\0.106\0.106\0.105\0.104\0.103\0.103\0.102\0.101\0.101\0.101\0.1\0.0997\0.0995\0.0994\0.099\0.0987\0.0986\0.0984\0.0981\0.0979\0.0975\0.0972\0.0969\0.0966\0.0963\0.0963\0.0961\0.0959\0.0958\0.0957\0.0956\0.0956\0.0956\0.0955\0.0955\0.0955\0.0954\0.0954\0.0953\0.0953\0.0953\0.0952\0.0951\0.0951\0.0951\0.095\0.0949\0.0948\0.0947\0.0946\0.0946\0.0945\0.0944\0.0943\0.0943\0.0942\0.0942\0.0941\0.0941\0.094\0.0939\0.0938\0.0938\0.0937\0.0937\0.0936\0.0936\0.0936\0.0936\0.0936\0.0936\0.0936\0.0936\0.0936\0.0936\0.0936\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935\0.0935</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.17\0.857\0.5\0.5\0.448\0.396\0.324\0.29\0.247\0.221\0.203\0.193\0.18\0.173\0.159\0.15\0.148\0.147\0.148\0.157\0.166\0.172\0.177\0.185\0.183\0.186\0.186\0.188\0.194\0.193\0.195\0.196\0.197\0.2\0.203\0.207\0.213\0.21\0.205\0.2\0.199\0.196\0.196\0.192\0.195\0.195\0.195\0.185\0.184\0.188\0.185\0.189\0.189\0.191\0.188\0.187\0.185\0.181\0.176\0.172\0.171\0.167\0.168\0.167\0.166\0.162\0.162\0.162\0.162\0.162\0.164\0.164\0.164\0.161\0.161\0.162\0.163\0.162\0.161\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.161\0.162\0.161\0.16\0.159\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.16\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.161\0.16\0.161\0.161\0.161\0.161\0.161\0.161\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16\0.16</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>205</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="DBCQjj" Title="Quasi-Newton method results">
   <Caption Id="xsUzxy">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0935
0.16
204
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="iScidc" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="8ZVmzp" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="wfdj9T" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="orrQlb">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0298843\121.659\6.15209\8.67392
0.000191566\0.779865\0.0394365\0.0556021
0.0191566\77.9865\3.94365\5.56021</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="yDphIE" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="I1Nl2n" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 127.</Text>
 </Task>
 <Task Id="iTmkGk" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="DVBEKp" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="Weryzz" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="qHkR6k" Title="Quasi-Newton method errors history">
   <Caption Id="pEv1RK">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.935453, and the final value after 207 epochs is 0.0803242.
The initial value of the selection error is 0.917902, and the final value after 207 epochs is 0.15191.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207</X2Data>
   <Y1Data>0.935\0.695\0.473\0.429\0.335\0.273\0.235\0.214\0.19\0.175\0.164\0.154\0.145\0.134\0.127\0.124\0.122\0.12\0.117\0.115\0.114\0.112\0.11\0.108\0.107\0.105\0.103\0.102\0.101\0.1\0.0996\0.0984\0.0968\0.0961\0.0956\0.0952\0.0949\0.0943\0.0937\0.0929\0.0922\0.0914\0.0905\0.0898\0.0891\0.0887\0.0882\0.0877\0.087\0.0867\0.0861\0.0857\0.0853\0.085\0.0848\0.0847\0.0847\0.0846\0.0845\0.0844\0.0842\0.0842\0.084\0.084\0.0839\0.0838\0.0837\0.0835\0.0835\0.0834\0.0833\0.0832\0.0831\0.083\0.0828\0.0828\0.0827\0.0826\0.0825\0.0825\0.0825\0.0824\0.0824\0.0824\0.0823\0.0823\0.0823\0.0823\0.0823\0.0823\0.0822\0.0822\0.0821\0.0821\0.082\0.082\0.082\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0819\0.0818\0.0818\0.0818\0.0818\0.0817\0.0817\0.0817\0.0817\0.0816\0.0816\0.0816\0.0815\0.0815\0.0815\0.0814\0.0814\0.0813\0.0813\0.0812\0.0812\0.0812\0.0811\0.0811\0.0811\0.0811\0.081\0.081\0.081\0.0809\0.0809\0.0809\0.0809\0.0808\0.0808\0.0808\0.0807\0.0807\0.0807\0.0806\0.0806\0.0805\0.0805\0.0805\0.0805\0.0805\0.0805\0.0805\0.0805\0.0805\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0803\0.0803\0.0803\0.0803\0.0803\0.0803\0.0803\0.0803\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0804\0.0803\0.0803\0.0803</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.918\0.765\0.643\0.558\0.527\0.362\0.315\0.285\0.234\0.231\0.236\0.227\0.219\0.192\0.177\0.172\0.166\0.157\0.155\0.151\0.155\0.161\0.167\0.169\0.168\0.166\0.163\0.16\0.163\0.159\0.16\0.162\0.155\0.153\0.152\0.153\0.154\0.156\0.16\0.159\0.158\0.159\0.157\0.156\0.159\0.155\0.155\0.153\0.152\0.152\0.151\0.152\0.155\0.154\0.152\0.151\0.151\0.151\0.151\0.15\0.152\0.149\0.147\0.147\0.147\0.148\0.149\0.147\0.147\0.147\0.146\0.145\0.145\0.144\0.144\0.144\0.145\0.146\0.144\0.144\0.143\0.143\0.144\0.144\0.144\0.144\0.144\0.144\0.144\0.143\0.144\0.145\0.146\0.145\0.145\0.145\0.146\0.146\0.146\0.146\0.145\0.145\0.145\0.145\0.145\0.145\0.145\0.145\0.144\0.144\0.144\0.144\0.144\0.144\0.144\0.144\0.144\0.144\0.145\0.145\0.145\0.145\0.146\0.146\0.146\0.146\0.146\0.145\0.146\0.146\0.146\0.146\0.146\0.146\0.146\0.146\0.147\0.147\0.147\0.147\0.148\0.148\0.148\0.148\0.148\0.148\0.148\0.149\0.15\0.151\0.151\0.151\0.151\0.152\0.153\0.153\0.153\0.153\0.153\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.152\0.151\0.151\0.151\0.151\0.151\0.151\0.15\0.15\0.15\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.15\0.15\0.15\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.151\0.152</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>208</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="pyVZgf" Title="Quasi-Newton method results">
   <Caption Id="32jbnr">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0803
0.152
207
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ogWSUP" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="64IuZF" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="9sDk3p" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="t6TJwV">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00695038\91.2673\6.06132\7.4472
4.45537e-5\0.585047\0.0388546\0.0477385
0.00445537\58.5047\3.88546\4.77385</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="HMvyT9" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="UAO3GA" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 85.</Text>
 </Task>
 <Task Id="feKAY3" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="a7apa7" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="WmfZnM" Title="Optimization algorithm">The conjugate gradient is used here for training.
In this algorithm search is performed along conjugate directions, which produces generally faster convergence than gradient descent directions. </Text>
  <DoubleLineChart Id="KFVmnk" Title="Conjugate gradient errors history">
   <Caption Id="F6vTJk">The following plot shows the training and selection errors in each iteration.
The initial value of the training error is 1.09133, and the final value after 202 epochs is 0.10434.
The initial value of the selection error is 1.08373, and the final value after 202 epochs is 0.172481.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202</X2Data>
   <Y1Data>1.09\0.878\0.524\0.48\0.433\0.332\0.289\0.268\0.252\0.239\0.223\0.21\0.203\0.194\0.185\0.18\0.175\0.169\0.162\0.158\0.154\0.151\0.147\0.145\0.143\0.141\0.139\0.137\0.136\0.134\0.133\0.132\0.13\0.129\0.128\0.127\0.126\0.125\0.124\0.123\0.123\0.122\0.122\0.121\0.121\0.12\0.12\0.119\0.119\0.118\0.118\0.117\0.117\0.116\0.116\0.116\0.115\0.115\0.115\0.115\0.115\0.114\0.114\0.114\0.114\0.114\0.113\0.113\0.113\0.113\0.112\0.112\0.112\0.112\0.112\0.112\0.112\0.112\0.111\0.111\0.111\0.111\0.111\0.111\0.111\0.11\0.11\0.109\0.108\0.108\0.108\0.108\0.108\0.108\0.108\0.108\0.108\0.108\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.107\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.106\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.105\0.104\0.104\0.104\0.104\0.104\0.104\0.104\0.104\0.104</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.08\0.872\0.455\0.462\0.435\0.357\0.371\0.348\0.337\0.31\0.254\0.258\0.242\0.257\0.237\0.246\0.227\0.233\0.21\0.217\0.203\0.211\0.199\0.208\0.196\0.203\0.191\0.196\0.184\0.188\0.179\0.184\0.176\0.18\0.171\0.175\0.167\0.171\0.163\0.167\0.162\0.165\0.16\0.164\0.159\0.162\0.158\0.162\0.158\0.161\0.158\0.162\0.158\0.161\0.159\0.162\0.159\0.162\0.159\0.162\0.16\0.162\0.16\0.163\0.161\0.163\0.161\0.164\0.162\0.164\0.161\0.164\0.162\0.165\0.163\0.165\0.163\0.165\0.163\0.165\0.163\0.165\0.163\0.166\0.164\0.166\0.165\0.167\0.169\0.167\0.169\0.17\0.172\0.171\0.171\0.17\0.171\0.17\0.17\0.169\0.171\0.169\0.17\0.169\0.17\0.169\0.17\0.169\0.17\0.169\0.17\0.169\0.17\0.169\0.17\0.169\0.17\0.169\0.17\0.169\0.171\0.17\0.17\0.169\0.17\0.17\0.17\0.169\0.17\0.17\0.171\0.17\0.171\0.17\0.171\0.17\0.171\0.17\0.171\0.17\0.171\0.17\0.171\0.17\0.171\0.17\0.171\0.17\0.171\0.17\0.171\0.17\0.171\0.17\0.171\0.171\0.171\0.171\0.172\0.171\0.171\0.171\0.171\0.171\0.172\0.171\0.172\0.171\0.172\0.171\0.172\0.171\0.172\0.171\0.172\0.172\0.173\0.172\0.172\0.172\0.172\0.172\0.172\0.172\0.172\0.171\0.172\0.172\0.172\0.172\0.172\0.172\0.172\0.172\0.172\0.172\0.173\0.172\0.172\0.172\0.172\0.172\0.172</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>203</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="ASv6Do" Title="Conjugate gradient results">
   <Caption Id="JsnYO2">The following table shows the training results by the conjugate gradient method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.104
0.172
202
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="lr5NHj" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="zhhVoX" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="A11MrD" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ObEecV">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00384521\104.372\6.35717\7.89231
2.46488e-5\0.669052\0.0407511\0.0505918
0.00246488\66.9052\4.07511\5.05918</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="p0R6Np" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="45dPxo" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 41.</Text>
 </Task>
 <Task Id="i2dnYW" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="MKa7Yp" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="AdpTf4" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="sa0VJW" Title="Quasi-Newton method errors history">
   <Caption Id="hjHt0h">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.57509, and the final value after 33 epochs is 0.115111.
The initial value of the selection error is 1.66538, and the final value after 33 epochs is 0.164184.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33</X2Data>
   <Y1Data>1.58\0.677\0.54\0.265\0.182\0.152\0.136\0.129\0.124\0.121\0.118\0.117\0.117\0.116\0.116\0.116\0.116\0.116\0.115\0.115\0.115\0.115\0.115\0.115\0.115\0.115\0.115\0.115\0.115\0.115\0.115\0.115\0.115\0.115</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.67\0.753\0.647\0.366\0.291\0.269\0.231\0.213\0.2\0.181\0.173\0.171\0.17\0.169\0.164\0.161\0.16\0.161\0.164\0.165\0.165\0.165\0.165\0.165\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164\0.164</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>34</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="nFjFBk" Title="Quasi-Newton method results">
   <Caption Id="QXYAEP">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.115
0.164
33
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="pFNWlB" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="lPhmmF" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="orgFjv" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="wvjM8n">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0\116.081\6.07279\8.11428
0\0.744108\0.0389281\0.0520146
0\74.4109\3.89281\5.20146</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="xrD1L3" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="ujzKnE" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 87.</Text>
 </Task>
 <Task Id="Y1RVc9" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="ptL23g" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="X0HIs5" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="Jj3xAk" Title="Quasi-Newton method errors history">
   <Caption Id="ZOQuWZ">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.00223, and the final value after 225 epochs is 0.0925864.
The initial value of the selection error is 0.919643, and the final value after 225 epochs is 0.167668.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225</X2Data>
   <Y1Data>1\0.943\0.544\0.535\0.497\0.423\0.37\0.323\0.299\0.268\0.244\0.232\0.217\0.209\0.205\0.192\0.18\0.169\0.16\0.151\0.146\0.142\0.139\0.136\0.132\0.128\0.124\0.123\0.12\0.117\0.114\0.112\0.111\0.109\0.108\0.107\0.105\0.104\0.104\0.103\0.102\0.102\0.102\0.101\0.101\0.1\0.0998\0.0993\0.0989\0.0987\0.0984\0.0981\0.0978\0.0975\0.0973\0.0971\0.0968\0.0967\0.0964\0.0961\0.0959\0.0955\0.0952\0.0947\0.0945\0.0941\0.0938\0.0936\0.0934\0.0933\0.0931\0.0931\0.093\0.0929\0.0929\0.0928\0.0928\0.0928\0.0928\0.0927\0.0927\0.0927\0.0926\0.0926\0.0926\0.0926\0.0925\0.0926\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0924\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0925\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0927\0.0927\0.0927\0.0927\0.0927\0.0927\0.0927\0.0927\0.0927\0.0927\0.0927\0.0928\0.0928\0.0928\0.0928\0.0928\0.0928\0.0928\0.0928\0.0928\0.0928\0.0928\0.0928\0.0928\0.0928\0.0928\0.0927\0.0927\0.0927\0.0927\0.0927\0.0927\0.0927\0.0927\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926\0.0926</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.92\0.943\0.504\0.483\0.481\0.423\0.409\0.404\0.347\0.317\0.318\0.307\0.278\0.274\0.263\0.257\0.243\0.222\0.214\0.201\0.192\0.184\0.18\0.183\0.184\0.187\0.18\0.175\0.176\0.174\0.183\0.195\0.195\0.194\0.191\0.191\0.189\0.189\0.189\0.185\0.186\0.184\0.187\0.19\0.192\0.188\0.183\0.18\0.181\0.179\0.176\0.172\0.173\0.172\0.173\0.173\0.175\0.173\0.171\0.17\0.169\0.171\0.169\0.174\0.174\0.172\0.172\0.172\0.17\0.171\0.17\0.169\0.169\0.169\0.168\0.169\0.168\0.166\0.165\0.165\0.165\0.165\0.165\0.165\0.165\0.166\0.165\0.165\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.166\0.167\0.167\0.167\0.166\0.166\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.168\0.168\0.168\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.168\0.168\0.167\0.167\0.168\0.168\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.167\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.167\0.167\0.167\0.167\0.168\0.168\0.168\0.167\0.167\0.167\0.167\0.168\0.168\0.167\0.167\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168\0.168</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>226</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="KPHAHb" Title="Quasi-Newton method results">
   <Caption Id="t4uuau">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0926
0.168
225
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="fDBEpp" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="ETTiH7" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="BFd4Sv" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="N4mpzW">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.00291252\117.227\5.85561\8.12655
1.867e-5\0.751455\0.0375359\0.0520932
0.001867\75.1455\3.75359\5.20932</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="F6nzRr" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="sc0GN5" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 87.</Text>
 </Task>
 <Task Id="SbDMsv" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="IBnGql" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="lGmto2" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="nGudrB" Title="Quasi-Newton method errors history">
   <Caption Id="uJ6yz1">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.16586, and the final value after 143 epochs is 0.391585.
The initial value of the selection error is 0.936402, and the final value after 143 epochs is 0.552447.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143</X2Data>
   <Y1Data>1.17\0.93\0.924\0.809\0.806\0.717\0.652\0.593\0.557\0.519\0.487\0.472\0.462\0.456\0.451\0.447\0.442\0.436\0.432\0.427\0.423\0.419\0.413\0.41\0.408\0.406\0.404\0.402\0.4\0.399\0.398\0.397\0.397\0.396\0.396\0.395\0.395\0.394\0.394\0.394\0.394\0.394\0.393\0.393\0.393\0.393\0.393\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392\0.392</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.936\1.48\1.46\1.54\1.51\1.14\0.792\0.858\0.858\0.766\0.669\0.632\0.642\0.627\0.611\0.602\0.606\0.591\0.605\0.584\0.6\0.614\0.595\0.62\0.6\0.588\0.593\0.583\0.552\0.549\0.547\0.544\0.545\0.552\0.553\0.553\0.552\0.548\0.548\0.548\0.551\0.554\0.556\0.554\0.556\0.559\0.56\0.561\0.558\0.56\0.559\0.56\0.558\0.558\0.558\0.555\0.557\0.557\0.556\0.555\0.554\0.554\0.553\0.553\0.553\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.551\0.55\0.55\0.55\0.55\0.549\0.548\0.548\0.548\0.547\0.546\0.546\0.546\0.547\0.546\0.546\0.547\0.548\0.548\0.548\0.548\0.548\0.549\0.55\0.55\0.55\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.553\0.553\0.553\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.552\0.553\0.552\0.552</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>144</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="sg4J6H" Title="Quasi-Newton method results">
   <Caption Id="Tko1vO">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.392
0.552
143
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Yox45b" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="hGuw4B" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="PzcfOt" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="q5dVhS">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.082324 and its percentage error 8.2324</Caption>
   <Data>0.0252762\35.9242\6.48846\4.6638
0.000351058\0.498947\0.0901175\0.064775
0.0351058\49.8947\9.01175\6.47751</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="6lxwBN" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="9Zj7vk" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 134.</Text>
 </Task>
 <Task Id="57W80k" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="y5KKtm" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="MA1gus" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="7jys3s" Title="Quasi-Newton method errors history">
   <Caption Id="B1X5Jk">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.947387, and the final value after 226 epochs is 0.080179.
The initial value of the selection error is 0.948315, and the final value after 226 epochs is 0.157392.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226</X2Data>
   <Y1Data>0.947\0.832\0.74\0.66\0.521\0.378\0.355\0.323\0.284\0.255\0.237\0.214\0.195\0.181\0.168\0.16\0.152\0.147\0.144\0.141\0.137\0.134\0.131\0.128\0.125\0.122\0.119\0.116\0.115\0.113\0.111\0.109\0.108\0.107\0.106\0.105\0.104\0.104\0.103\0.103\0.102\0.102\0.101\0.1\0.0997\0.0994\0.0991\0.0985\0.0982\0.098\0.0979\0.0978\0.0975\0.097\0.0963\0.0955\0.0948\0.0943\0.0941\0.0936\0.093\0.0924\0.0918\0.091\0.0905\0.09\0.0895\0.0891\0.0886\0.0884\0.0882\0.088\0.0879\0.0877\0.0875\0.0874\0.0871\0.087\0.0868\0.0867\0.0866\0.0865\0.0865\0.0864\0.0862\0.0862\0.0861\0.0859\0.0859\0.0858\0.0857\0.0857\0.0856\0.0855\0.0854\0.0853\0.0853\0.0851\0.085\0.0849\0.0848\0.0848\0.0847\0.0844\0.0842\0.0839\0.0836\0.0834\0.0832\0.083\0.0827\0.0826\0.0824\0.0823\0.0821\0.0821\0.082\0.0818\0.0817\0.0815\0.0813\0.0813\0.0812\0.0812\0.081\0.081\0.0809\0.0807\0.0805\0.0804\0.0803\0.0802\0.0801\0.0801\0.0801\0.0801\0.0799\0.0797\0.0796\0.0795\0.0794\0.0795\0.0795\0.0795\0.0795\0.0796\0.0796\0.0795\0.0794\0.0793\0.0793\0.0794\0.0793\0.0794\0.0794\0.0794\0.0794\0.0795\0.0795\0.0795\0.0795\0.0795\0.0795\0.0795\0.0794\0.0794\0.0794\0.0794\0.0794\0.0794\0.0795\0.0795\0.0795\0.0795\0.0795\0.0795\0.0795\0.0795\0.0795\0.0795\0.0795\0.0796\0.0796\0.0796\0.0796\0.0796\0.0796\0.0796\0.0797\0.0797\0.0797\0.0797\0.0797\0.0797\0.0797\0.0797\0.0798\0.0798\0.0798\0.0798\0.0798\0.0798\0.0798\0.0798\0.0799\0.0799\0.0799\0.0799\0.0799\0.0799\0.08\0.08\0.08\0.08\0.0801\0.0801\0.0801\0.0801\0.0801\0.0801\0.0801\0.0801\0.0801\0.0802\0.0802\0.0802\0.0802</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.948\0.764\0.64\0.565\0.455\0.444\0.384\0.367\0.321\0.314\0.294\0.277\0.241\0.211\0.192\0.18\0.172\0.168\0.165\0.162\0.159\0.157\0.163\0.168\0.182\0.187\0.191\0.197\0.189\0.188\0.186\0.182\0.177\0.174\0.175\0.176\0.176\0.177\0.177\0.175\0.174\0.172\0.175\0.178\0.178\0.175\0.179\0.179\0.179\0.178\0.178\0.174\0.174\0.179\0.175\0.175\0.179\0.181\0.179\0.175\0.177\0.176\0.176\0.169\0.165\0.161\0.161\0.158\0.158\0.157\0.156\0.155\0.155\0.157\0.159\0.158\0.157\0.156\0.158\0.158\0.159\0.159\0.157\0.157\0.156\0.156\0.157\0.158\0.159\0.156\0.156\0.156\0.157\0.157\0.157\0.158\0.158\0.157\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.157\0.156\0.157\0.158\0.158\0.158\0.157\0.158\0.157\0.158\0.158\0.159\0.16\0.159\0.159\0.159\0.159\0.16\0.16\0.159\0.159\0.158\0.158\0.159\0.16\0.161\0.16\0.16\0.161\0.162\0.163\0.162\0.162\0.161\0.161\0.16\0.159\0.159\0.16\0.16\0.16\0.16\0.158\0.158\0.158\0.16\0.16\0.16\0.16\0.159\0.159\0.159\0.159\0.159\0.16\0.16\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.158\0.158\0.159\0.159\0.159\0.158\0.158\0.158\0.158\0.159\0.158\0.158\0.158\0.158\0.158\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.159\0.158\0.158\0.158\0.158\0.158\0.158\0.159\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.158\0.157\0.157\0.157\0.157\0.158\0.157\0.157\0.157\0.157</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>227</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="ojsxMa" Title="Quasi-Newton method results">
   <Caption Id="JeFPFT">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.0802
0.157
226
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="CqGrDP" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="NfR07D" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="0PrSyj" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="B4hkt9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0788105 and its percentage error 7.88103</Caption>
   <Data>0.0276184\114.659\6.08361\8.40081
0.000177041\0.734995\0.0389975\0.0538514
0.0177041\73.4995\3.89975\5.38514</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="maR1oQ" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="jP5QJp" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 143.</Text>
 </Task>
 <Task Id="v6KM4N" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="w96kOe" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="FXTV7t" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="Ys1Lu7" Title="Quasi-Newton method errors history">
   <Caption Id="3hCap6">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.06158, and the final value after 229 epochs is 0.368292.
The initial value of the selection error is 0.762037, and the final value after 229 epochs is 0.3619.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229</X2Data>
   <Y1Data>1.06\0.994\0.873\0.755\0.702\0.674\0.656\0.646\0.617\0.588\0.566\0.547\0.538\0.523\0.507\0.494\0.484\0.476\0.457\0.444\0.437\0.429\0.419\0.412\0.408\0.404\0.403\0.399\0.395\0.393\0.39\0.387\0.386\0.385\0.383\0.381\0.379\0.378\0.377\0.376\0.376\0.375\0.374\0.374\0.373\0.373\0.372\0.372\0.372\0.371\0.371\0.371\0.371\0.371\0.37\0.37\0.37\0.37\0.37\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.368\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.762\0.875\0.712\0.572\0.621\0.552\0.545\0.538\0.523\0.485\0.467\0.468\0.498\0.502\0.47\0.443\0.437\0.443\0.464\0.439\0.414\0.394\0.394\0.408\0.404\0.398\0.388\0.382\0.384\0.393\0.386\0.38\0.382\0.389\0.384\0.376\0.373\0.377\0.378\0.371\0.366\0.365\0.365\0.367\0.367\0.365\0.365\0.366\0.366\0.366\0.366\0.366\0.367\0.368\0.368\0.368\0.367\0.367\0.368\0.369\0.367\0.367\0.368\0.368\0.369\0.369\0.368\0.368\0.369\0.371\0.371\0.37\0.369\0.369\0.37\0.37\0.371\0.371\0.369\0.37\0.37\0.37\0.37\0.371\0.371\0.371\0.37\0.367\0.368\0.368\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.369\0.369\0.368\0.368\0.367\0.367\0.367\0.367\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.366\0.366\0.367\0.367\0.366\0.366\0.367\0.367\0.367\0.366\0.367\0.368\0.368\0.367\0.366\0.366\0.366\0.365\0.365\0.364\0.364\0.364\0.364\0.364\0.363\0.363\0.362\0.363\0.363\0.362\0.362\0.362\0.361\0.361\0.362\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>230</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="LtQsTS" Title="Quasi-Newton method results">
   <Caption Id="mgb9bx">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.368
0.362
229
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="2J4ZHX" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="LEDwc4" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="NmpHhj" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="cDkDjs">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00949097\105.81\7.6815\8.96798
6.08395e-5\0.678271\0.0492404\0.0574871
0.00608395\67.8271\4.92404\5.74871</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="bQXXAA" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9CoUtj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00260067\307.593\6.7338\14.198
7.64903e-6\0.904687\0.0198053\0.0417589
0.000764903\90.4687\1.98053\4.17589</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="6UnLqf" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="mBIgRb">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0121765\28.4704\5.37753\4.63192
4.90988e-5\0.1148\0.0216836\0.0186771
0.00490988\11.48\2.16836\1.86771</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Gd9E1l" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="UoZ8iA">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0156403\39.5228\7.74549\5.07202
0.000217226\0.548928\0.107576\0.0704448
0.0217226\54.8928\10.7576\7.04448</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="p1O9NL" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9lMzbX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00152814\14.8609\0.772878\0.985311
8.98908e-5\0.87417\0.0454634\0.0579594
0.00898908\87.417\4.54634\5.79594</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="KdTT4Y" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="wumsIk" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 143.</Text>
 </Task>
 <Task Id="ckblNp" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="7oJdFM" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="ColYP9" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="vEuHQV" Title="Quasi-Newton method errors history">
   <Caption Id="IfAj6S">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.08399, and the final value after 185 epochs is 0.371112.
The initial value of the selection error is 0.764162, and the final value after 185 epochs is 0.362203.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185</X2Data>
   <Y1Data>1.08\0.851\0.745\0.637\0.585\0.558\0.533\0.518\0.499\0.485\0.464\0.453\0.444\0.433\0.426\0.42\0.413\0.406\0.399\0.395\0.39\0.387\0.386\0.384\0.382\0.381\0.38\0.379\0.378\0.378\0.377\0.377\0.376\0.376\0.376\0.376\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.375\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.374\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.373\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.372\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371\0.371</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.764\0.794\0.836\0.687\0.586\0.557\0.543\0.52\0.502\0.482\0.467\0.458\0.462\0.458\0.44\0.427\0.424\0.407\0.399\0.387\0.387\0.386\0.381\0.385\0.382\0.378\0.373\0.373\0.372\0.369\0.367\0.367\0.366\0.366\0.368\0.368\0.368\0.367\0.368\0.368\0.368\0.367\0.367\0.368\0.367\0.367\0.368\0.367\0.368\0.367\0.367\0.366\0.366\0.365\0.366\0.365\0.364\0.364\0.365\0.364\0.365\0.364\0.364\0.364\0.364\0.364\0.364\0.363\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.363\0.363\0.363\0.362\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.362\0.363\0.363\0.363\0.363\0.363\0.364\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.363\0.362\0.362\0.362\0.362\0.362\0.362\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.363\0.362\0.362\0.362\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.363\0.363\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>186</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="HXzGx3" Title="Quasi-Newton method results">
   <Caption Id="FEbNRX">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.371
0.362
185
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="jTLUaU" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="0NQITn" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="p3bKz0" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="z4d9mm">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\107.836\7.46172\8.69551
0\0.691258\0.0478315\0.0557404
0\69.1258\4.78315\5.57404</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="qsVhw0" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="5qrsNK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00445557\309.532\6.63274\14.1752
1.31046e-5\0.910389\0.0195081\0.0416918
0.00131046\91.0389\1.95081\4.16918</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Z2nZgE" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="3wKzeH">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00661469\25.6501\5.52483\4.76685
2.66721e-5\0.103428\0.0222775\0.0192212
0.00266721\10.3428\2.22775\1.92212</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="6FQaQD" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="470XNc">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0262985\40.9143\7.51537\5.05195
0.000365257\0.568254\0.10438\0.0701659
0.0365257\56.8254\10.438\7.01659</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="g6a8pz" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="i33JFC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\12.3138\0.722762\1.05881
0\0.724339\0.0425154\0.0622829
0\72.4339\4.25154\6.22829</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="glPLq2" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="ul4bpD" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 143.</Text>
 </Task>
 <Task Id="n5rr5t" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="H0JCOT" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="UoPSBx" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="AEq3m0" Title="Quasi-Newton method errors history">
   <Caption Id="rmcTrp">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.05429, and the final value after 169 epochs is 0.366631.
The initial value of the selection error is 0.794441, and the final value after 169 epochs is 0.362172.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169</X2Data>
   <Y1Data>1.05\0.911\0.836\0.769\0.69\0.637\0.603\0.577\0.56\0.536\0.514\0.494\0.474\0.462\0.45\0.438\0.429\0.423\0.416\0.408\0.401\0.396\0.392\0.389\0.386\0.382\0.38\0.379\0.377\0.376\0.375\0.374\0.373\0.372\0.371\0.371\0.37\0.37\0.37\0.369\0.369\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.794\0.853\0.853\0.741\0.625\0.592\0.586\0.552\0.519\0.478\0.481\0.472\0.464\0.45\0.431\0.43\0.432\0.42\0.411\0.4\0.391\0.387\0.391\0.383\0.38\0.379\0.37\0.366\0.365\0.361\0.362\0.361\0.36\0.358\0.357\0.359\0.359\0.359\0.358\0.36\0.361\0.362\0.362\0.363\0.363\0.364\0.364\0.363\0.364\0.364\0.363\0.364\0.363\0.362\0.362\0.362\0.362\0.361\0.36\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.362\0.362\0.362\0.361\0.361\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.363\0.363\0.362\0.362\0.363\0.362\0.362\0.362\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>170</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="LJSvln" Title="Quasi-Newton method results">
   <Caption Id="iDusX7">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.367
0.362
169
00:00:01
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="qwTSSD" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="UndY0B" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="3hpaDc" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="h9xkR9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\108.23\7.45762\8.66073
0\0.693784\0.0478053\0.0555175
0\69.3784\4.78053\5.55175</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="O9ng26" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="kE9EEQ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00244141\309.437\6.68659\14.1561
7.18061e-6\0.91011\0.0196665\0.0416355
0.000718061\91.011\1.96665\4.16356</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="sslTAi" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ATXKPH">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0130844\31.3282\5.51773\4.85634
5.27597e-5\0.126323\0.0222489\0.019582
0.00527597\12.6323\2.22489\1.9582</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="aRxGpV" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="VW1xso">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.024683\40.2263\7.74091\5.09946
0.000342819\0.558699\0.107513\0.0708258
0.0342819\55.8699\10.7513\7.08258</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="CeDU7L" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ai9MBi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\13.4626\0.733276\1.0731
0\0.791918\0.0431339\0.0631233
0\79.1918\4.31339\6.31233</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="9WQ66R" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="PsLsCn" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 143.</Text>
 </Task>
 <Task Id="9q47Q2" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="TzpYMw" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="FtV2bv" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="N7Lr72" Title="Quasi-Newton method errors history">
   <Caption Id="1DhuKd">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.07148, and the final value after 219 epochs is 0.364762.
The initial value of the selection error is 0.779333, and the final value after 219 epochs is 0.362325.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219</X2Data>
   <Y1Data>1.07\0.888\0.791\0.653\0.625\0.579\0.557\0.53\0.5\0.482\0.464\0.453\0.442\0.434\0.427\0.422\0.417\0.411\0.406\0.402\0.399\0.395\0.393\0.39\0.387\0.385\0.384\0.382\0.381\0.38\0.379\0.377\0.376\0.374\0.373\0.373\0.372\0.371\0.371\0.37\0.37\0.37\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.368\0.368\0.368\0.368\0.368\0.368\0.368\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.367\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.366\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.779\0.776\0.779\0.531\0.508\0.515\0.512\0.476\0.456\0.444\0.432\0.414\0.394\0.389\0.391\0.393\0.392\0.389\0.382\0.376\0.375\0.373\0.373\0.371\0.369\0.37\0.367\0.368\0.365\0.362\0.361\0.361\0.362\0.363\0.361\0.361\0.36\0.36\0.36\0.359\0.359\0.359\0.359\0.36\0.361\0.362\0.362\0.362\0.362\0.363\0.363\0.363\0.362\0.362\0.361\0.362\0.362\0.361\0.362\0.361\0.361\0.362\0.362\0.362\0.362\0.362\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.361\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.359\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.36\0.359\0.36\0.36\0.359\0.36\0.36\0.36\0.36\0.359\0.359\0.359\0.358\0.358\0.359\0.36\0.361\0.362\0.361\0.361\0.36\0.36\0.361\0.362\0.363\0.363\0.363\0.363\0.362\0.361\0.36\0.36\0.361\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.362</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>220</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="lrp1ku" Title="Quasi-Newton method results">
   <Caption Id="mo5nGh">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.365
0.362
219
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="G3oKdB" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="sZwkaM" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="t7sofl" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Qx4jxn">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00672913\106.857\7.43781\8.72645
4.31354e-5\0.684982\0.0476782\0.0559388
0.00431354\68.4982\4.76782\5.59388</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ZQYsgh" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="3eheAB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.05196\308.153\6.70715\14.1965
0.000152824\0.906331\0.0197269\0.0417544
0.0152824\90.6331\1.97269\4.17544</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="5cnzWP" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="XDh58k">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00303459\34.346\5.49472\4.87021
1.22363e-5\0.138492\0.0221561\0.019638
0.00122363\13.8492\2.21561\1.9638</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="AWqMez" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="nxWEJ3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0257435\39.7009\7.71773\5.07745
0.000357548\0.551401\0.107191\0.0705201
0.0357548\55.1401\10.7191\7.05202</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="3rw7kw" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="oHsOh2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0034976\14.5871\0.707388\1.00353
0.000205741\0.858063\0.041611\0.0590312
0.0205741\85.8063\4.1611\5.90312</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Igo2eh" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="mK1jX9" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="N1O3BA" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="46tOUH" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="JMpiPN" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="TFz7Se" Title="Quasi-Newton method errors history">
   <Caption Id="ZXjfmb">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.06373, and the final value after 222 epochs is 0.315557.
The initial value of the selection error is 0.794318, and the final value after 222 epochs is 0.272291.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222</X2Data>
   <Y1Data>1.06\0.972\0.768\0.732\0.614\0.573\0.551\0.524\0.5\0.468\0.441\0.427\0.416\0.4\0.392\0.385\0.378\0.376\0.368\0.362\0.358\0.356\0.353\0.351\0.348\0.345\0.345\0.343\0.342\0.34\0.338\0.337\0.337\0.337\0.335\0.335\0.334\0.334\0.333\0.332\0.332\0.331\0.331\0.33\0.33\0.33\0.329\0.329\0.329\0.328\0.328\0.327\0.327\0.327\0.326\0.326\0.326\0.326\0.326\0.325\0.325\0.325\0.325\0.324\0.324\0.324\0.323\0.323\0.323\0.322\0.322\0.322\0.322\0.322\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.319\0.318\0.319\0.319\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.794\0.873\0.762\0.678\0.504\0.525\0.503\0.459\0.431\0.423\0.414\0.393\0.376\0.361\0.364\0.361\0.349\0.337\0.327\0.335\0.333\0.317\0.307\0.308\0.308\0.309\0.3\0.294\0.291\0.289\0.286\0.282\0.282\0.281\0.277\0.275\0.274\0.277\0.274\0.273\0.273\0.272\0.271\0.272\0.273\0.274\0.272\0.273\0.273\0.273\0.273\0.272\0.273\0.274\0.273\0.274\0.274\0.273\0.274\0.273\0.275\0.276\0.276\0.275\0.275\0.276\0.276\0.274\0.273\0.276\0.275\0.273\0.273\0.273\0.273\0.273\0.272\0.271\0.272\0.272\0.271\0.272\0.271\0.271\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.27\0.269\0.269\0.27\0.271\0.271\0.271\0.27\0.271\0.271\0.271\0.271\0.27\0.27\0.271\0.271\0.27\0.271\0.271\0.271\0.272\0.272\0.272\0.271\0.271\0.271\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.271\0.271\0.27\0.27\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.27\0.27\0.27\0.27\0.27\0.27\0.271\0.271\0.27\0.27\0.27\0.27\0.27\0.27\0.271\0.271\0.271\0.271\0.271\0.271\0.27\0.27\0.27\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.27\0.27\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>223</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="zAdhJ6" Title="Quasi-Newton method results">
   <Caption Id="sSCR8z">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.316
0.272
222
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="QOWe2V" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="GtDE6z" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="sjlqRY" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="0A7Idm">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.016243\112.274\6.16506\7.93302
0.000104122\0.719706\0.0395196\0.0508527
0.0104122\71.9706\3.95196\5.08527</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="6tuPxs" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="sYK0Hk">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.003582\306.583\6.58904\14.088
1.05353e-5\0.901715\0.0193795\0.0414354
0.00105353\90.1715\1.93795\4.14354</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="2b32k6" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="RiDgcl">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00770187\27.6177\4.75165\4.10808
3.10559e-5\0.111362\0.0191599\0.0165648
0.00310559\11.1362\1.91599\1.65648</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="0VENDS" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="RolajZ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0368633\40.7623\5.8354\4.45096
0.000511991\0.566143\0.0810472\0.0618188
0.0511991\56.6143\8.10472\6.18189</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="dKFbER" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="5c0Djj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0119712\14.3549\0.735055\0.981451
0.00070419\0.844407\0.0432385\0.0577324
0.070419\84.4407\4.32385\5.77324</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="2XSLgw" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="8hvIfV" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="xTuFmS" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="Kgz575" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="8OlycJ" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="IG9Bfl" Title="Quasi-Newton method errors history">
   <Caption Id="nlqOht">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.10224, and the final value after 246 epochs is 0.305423.
The initial value of the selection error is 0.770318, and the final value after 246 epochs is 0.303354.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246</X2Data>
   <Y1Data>1.1\0.937\0.757\0.69\0.607\0.559\0.507\0.467\0.445\0.426\0.409\0.394\0.386\0.379\0.373\0.364\0.358\0.352\0.347\0.343\0.34\0.338\0.335\0.334\0.332\0.331\0.329\0.328\0.327\0.326\0.325\0.324\0.323\0.322\0.322\0.321\0.32\0.319\0.319\0.318\0.316\0.315\0.314\0.314\0.313\0.313\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.77\0.863\0.857\0.821\0.619\0.547\0.511\0.461\0.444\0.432\0.415\0.406\0.405\0.384\0.367\0.363\0.349\0.34\0.333\0.329\0.329\0.324\0.318\0.319\0.316\0.31\0.306\0.309\0.306\0.306\0.308\0.309\0.308\0.308\0.31\0.309\0.307\0.306\0.308\0.306\0.306\0.308\0.307\0.307\0.305\0.303\0.305\0.306\0.305\0.304\0.304\0.302\0.303\0.303\0.303\0.301\0.301\0.301\0.301\0.3\0.3\0.301\0.301\0.3\0.3\0.3\0.301\0.302\0.303\0.303\0.303\0.303\0.302\0.302\0.303\0.303\0.304\0.304\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303\0.303</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>247</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="7UJxNQ" Title="Quasi-Newton method results">
   <Caption Id="2BUTDN">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.305
0.303
246
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="0p2r8n" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="WSnvjn" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="hkh7ey" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="pAHLKu">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\116.512\5.9003\7.85382
0\0.746871\0.0378225\0.050345
0\74.6871\3.78225\5.0345</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="o8XbsA" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="WPVGp4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00102329\311.866\6.69358\14.295
3.00968e-6\0.917253\0.019687\0.0420441
0.000300968\91.7253\1.9687\4.20441</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="BqL4pU" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="EUsVKF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00580978\35.5929\4.97706\4.411
2.34265e-5\0.14352\0.0200688\0.0177863
0.00234265\14.352\2.00688\1.77863</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="RZ017Q" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="r5yE6q">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0776062\39.7545\6.53433\4.59752
0.00107786\0.552145\0.0907547\0.0638545
0.107786\55.2145\9.07547\6.38545</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="YVGiK2" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="azM4IF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>7.58171e-5\12.9392\0.743839\1.0032
4.45983e-6\0.761131\0.0437552\0.059012
0.000445983\76.1131\4.37552\5.9012</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="XtfEtg" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="HO9Y0B" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="6LIcou" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="HPw1zi" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="hUPGEc" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="usalhN" Title="Quasi-Newton method errors history">
   <Caption Id="lUt71l">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.09797, and the final value after 231 epochs is 0.328209.
The initial value of the selection error is 0.791111, and the final value after 231 epochs is 0.271443.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231</X2Data>
   <Y1Data>1.1\0.652\0.563\0.505\0.45\0.419\0.4\0.381\0.371\0.365\0.359\0.355\0.353\0.349\0.347\0.345\0.343\0.342\0.341\0.34\0.339\0.339\0.338\0.337\0.337\0.336\0.336\0.336\0.335\0.335\0.335\0.335\0.334\0.334\0.333\0.333\0.333\0.333\0.333\0.332\0.333\0.332\0.332\0.332\0.332\0.332\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.331\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.33\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.329\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.791\0.59\0.47\0.474\0.449\0.401\0.368\0.345\0.323\0.315\0.312\0.305\0.302\0.289\0.28\0.282\0.281\0.281\0.284\0.281\0.285\0.285\0.282\0.279\0.274\0.274\0.275\0.274\0.273\0.27\0.274\0.276\0.275\0.274\0.276\0.274\0.274\0.272\0.274\0.275\0.275\0.273\0.273\0.273\0.274\0.275\0.276\0.275\0.275\0.273\0.273\0.275\0.276\0.276\0.274\0.274\0.274\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.275\0.276\0.276\0.275\0.275\0.275\0.275\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.275\0.275\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.273\0.273\0.273\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.273\0.273\0.274\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.272\0.272\0.273\0.272\0.272\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.271</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>232</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="gHX9Um" Title="Quasi-Newton method results">
   <Caption Id="O9f39q">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.328
0.271
231
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="DY3pXZ" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="gU4DD8" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="KiF2IT" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="xhJ9We">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\115.863\6.08251\8.07118
0\0.742711\0.0389904\0.0517383
0\74.2711\3.89904\5.17383</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Jhk4Np" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="yaQlu3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0279198\306.639\6.81373\14.5585
8.2117e-5\0.90188\0.0200404\0.042819
0.0082117\90.188\2.00404\4.2819</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="iyrjOY" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="wEEd2L">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00430107\40.0523\5.13749\4.67452
1.7343e-5\0.161501\0.0207157\0.0188489
0.0017343\16.1501\2.07157\1.88489</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Fuh12s" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="BZPwJC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0471992\40.5943\5.54907\4.32129
0.000655545\0.56381\0.0770704\0.0600179
0.0655545\56.381\7.70704\6.00179</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="TcHUbP" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="zEDKvg">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0050509\14.8463\0.696141\1.02699
0.000297112\0.873315\0.0409495\0.0604114
0.0297112\87.3315\4.09495\6.04114</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="DLYs01" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="Xsc7Ed" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="gh6PnH" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="U3sg3M" Title="Quasi-Newton method errors history">
   <Caption Id="jql6As">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.32821, and the final value after 138 epochs is 0.32816.
The initial value of the selection error is 0.271443, and the final value after 138 epochs is 0.272193.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138</X2Data>
   <Y1Data>0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328\0.328</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.271\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>139</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="Kfa29T" Title="Quasi-Newton method results">
   <Caption Id="czT2yj">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.328
0.272
138
00:00:00
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="NvvCaV" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="9EX94l" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="tItKtm" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="4Jjj4G">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0\115.742\6.06858\8.07906
0\0.741933\0.0389011\0.0517889
0\74.1933\3.89011\5.17889</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="IQnHBD" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="XNAxXu">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0367737\306.578\6.81341\14.5686
0.000108158\0.9017\0.0200395\0.042849
0.0108158\90.17\2.00395\4.2849</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="5w2Jq1" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="fFe1UK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00392914\39.5581\5.1352\4.66497
1.58433e-5\0.159509\0.0207064\0.0188104
0.00158433\15.9509\2.07064\1.88104</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="6jQPDK" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="gXUmdj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00299072\40.673\5.57496\4.33573
4.15378e-5\0.564903\0.07743\0.0602184
0.00415378\56.4903\7.743\6.02185</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="wICqwk" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="8XdqFn">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000471354\14.8689\0.693547\1.02747
2.77267e-5\0.874641\0.0407969\0.0604394
0.00277267\87.4641\4.07969\6.04394</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="TjAKyz" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="ihnkNl" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="ukUmUe" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="QZT1tM" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="MTlKPm" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="Lx5aQB" Title="Quasi-Newton method errors history">
   <Caption Id="0vtQfs">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.13214, and the final value after 262 epochs is 0.294876.
The initial value of the selection error is 0.812508, and the final value after 262 epochs is 0.256039.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262</X2Data>
   <Y1Data>1.13\0.802\0.66\0.599\0.539\0.481\0.446\0.43\0.408\0.395\0.384\0.374\0.366\0.358\0.353\0.349\0.345\0.342\0.338\0.335\0.332\0.33\0.328\0.326\0.325\0.324\0.323\0.322\0.321\0.32\0.319\0.319\0.318\0.318\0.317\0.317\0.316\0.316\0.316\0.315\0.315\0.315\0.314\0.314\0.314\0.313\0.313\0.313\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.303\0.303\0.302\0.302\0.301\0.301\0.301\0.3\0.3\0.299\0.299\0.299\0.299\0.298\0.298\0.298\0.298\0.298\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.297\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.296\0.296\0.296\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.813\0.673\0.613\0.549\0.456\0.419\0.401\0.393\0.372\0.356\0.35\0.345\0.334\0.316\0.31\0.305\0.297\0.293\0.291\0.288\0.284\0.277\0.278\0.273\0.276\0.277\0.278\0.278\0.279\0.277\0.277\0.276\0.276\0.277\0.276\0.276\0.276\0.277\0.277\0.277\0.276\0.276\0.275\0.274\0.273\0.272\0.274\0.274\0.274\0.273\0.273\0.274\0.275\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.275\0.275\0.276\0.276\0.276\0.276\0.276\0.276\0.275\0.275\0.275\0.275\0.276\0.276\0.275\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.277\0.277\0.276\0.277\0.276\0.277\0.277\0.277\0.276\0.276\0.277\0.277\0.276\0.276\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.276\0.275\0.275\0.274\0.274\0.274\0.275\0.275\0.275\0.274\0.273\0.273\0.274\0.273\0.273\0.272\0.271\0.271\0.27\0.269\0.268\0.267\0.266\0.265\0.263\0.263\0.263\0.263\0.262\0.262\0.263\0.262\0.263\0.262\0.263\0.262\0.263\0.262\0.262\0.262\0.262\0.262\0.262\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.26\0.261\0.261\0.261\0.261\0.261\0.261\0.261\0.26\0.26\0.26\0.26\0.259\0.259\0.259\0.259\0.259\0.259\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.258\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.257\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>263</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="edZVNw" Title="Quasi-Newton method results">
   <Caption Id="c1lVSP">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.295
0.256
262
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="x7cNLz" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="cMHrV4" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="vqCSxR" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="sOCH5K">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0123062\113.159\6.06932\7.95785
7.8886e-5\0.725378\0.0389059\0.0510119
0.0078886\72.5378\3.89059\5.10119</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="XI3eq9" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="31Hppa">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0291843\305.401\6.67615\14.063
8.58363e-5\0.898237\0.0196357\0.0413618
0.00858363\89.8237\1.96357\4.13618</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="lhc65W" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="2j7TSZ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.020546\37.4743\4.91869\4.40668
8.28466e-5\0.151106\0.0198334\0.0177689
0.00828466\15.1106\1.98334\1.77689</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="2smroJ" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="QV2gIV">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0212831\40.844\5.32737\4.2009
0.000295599\0.567278\0.0739912\0.0583458
0.0295599\56.7278\7.39913\5.83458</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="AsAGsQ" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="hQa69b">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00105441\14.6311\0.704797\1.02062
6.20239e-5\0.860652\0.0414587\0.0600367
0.00620239\86.0652\4.14587\6.00367</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="NTGGrH" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="HZw2ml" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="YHLoZL" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="78VmHX" Title="Quasi-Newton method errors history">
   <Caption Id="AR8fka">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.294876, and the final value after 187 epochs is 0.29168.
The initial value of the selection error is 0.256039, and the final value after 187 epochs is 0.254753.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187</X2Data>
   <Y1Data>0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.295\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.294\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292\0.292</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.256\0.255\0.255\0.255\0.255\0.254\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.254\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.256\0.257\0.256\0.256\0.256\0.256\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.254\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255\0.255</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>188</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="Ues91z" Title="Quasi-Newton method results">
   <Caption Id="SSLrH6">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.292
0.255
187
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="UC0y4w" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="g93i0B" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="fVhbdC" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="AyEe6I">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0203094\115.929\6.06557\8.1207
0.000130189\0.743133\0.0388819\0.0520558
0.0130189\74.3133\3.88819\5.20558</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="if0dae" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="IxGmvG">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0114136\306.194\6.66948\14.0185
3.35693e-5\0.90057\0.0196161\0.041231
0.00335693\90.057\1.96161\4.1231</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="lH3YXn" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="pkq5Qx">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000202179\37.4636\4.87045\4.40991
8.15238e-7\0.151063\0.0196389\0.0177819
8.15238e-5\15.1063\1.96389\1.77819</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="WRYtwH" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="XHPRFQ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.043829\44.0771\5.25286\4.23326
0.000608736\0.612182\0.0729563\0.0587952
0.0608736\61.2182\7.29563\5.87952</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="7bcS2U" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="usWDvH">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00157189\14.6874\0.69559\1.02446
9.24643e-5\0.863962\0.0409171\0.0602626
0.00924643\86.3962\4.09171\6.02626</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="dmtIew" Title="Goodness-of-fit analysis" Component="Testing analysis" Name="Goodness-of-fit analysis">
  <Text Id="HVyjqJ" Title="Task description">The goodness-of-fit of a statistical model describes how well it fits a set of observations.
Measures of goodness-of-fit typically summarize the discrepancy between observed values and the values expected under the model in question.
One of the most used goodness-of-fit measures is the coefficient of determination, R2.
R2 coefficient quantifies the proportion of variation of the predicted variable from the actual values.
If we had a perfect fit (outputs equal to targets), R2 would be equal to 1.
</Text>
  <Table Id="s6UVFh" Title="PM2.5(microg/m3)_ahead_1 goodness-of-fit parameters">
   <Caption Id="EE5D7i">The next table lists the goodness-of-fit parameters for the output PM2.5(microg/m3)_ahead_1.
</Caption>
   <Data>0.7784639</Data>
   <RowsName>Determination</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>9</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <LinearRegressionChart Id="nF1K70" Title="PM2.5(microg/m3)_ahead_1 goodness-of-fit chart">
   <Caption Id="zfJDJB">The following chart illustrates the predicted values versus the actual ones for the output PM2.5(microg/m3)_ahead_1.
The grey line indicates the best prediciton result (outputs equal to targets).
Note that some scaled outputs fall outside the range defined by the targets, so they are not plotted. </Caption>
   <XData>55\49\53\31\40\29\49\56\60\50\27\26\36\30\45\51\59\62\53\23\23\45\31\28\34\48\60\53\72\50\52\46\51\41\29\37\54\65\76\68\89\90\63\29\42\77\49\48\65\59\63\71\64\78\58\67\80\75\71\36\51\62\72\77\81\60\47\51\60\53\36\22\39\33\24\26\31\24\30\34\39\31\56\58\40\49\63\76\61\54\67\68\63\61\37\44\38\15\17\22\35\25\23\39\22\50\54\30\27\147\136\113\84\77\31\28\27\25\31\42\63\57\53\35\32\31\36\25\40\58\46\28\26\32\29\52\46\45\53\52\68\70\91\87\73\82\34\53\63\65\80\81\66\65\67\87\97\87\73\61\60\50\61\62\42\47\38\43\32\26\33\31\27\38\51\51\57\57\58\59\58\48\50\55\56\102\84\64\62\57\51\53\58\78\75\40\72\55\50\49\43\53\58\60\44\52\40\44\48\68\69\64\44\45\40\43\45\46\58\57\55\53\70\37\31\40\41\40\71\98\41\48\39\46\59\35\38\37\46\55\54\65\48\58\67\82\57\63\64\64\73\64\62\67\66\72\56\50\62\60\85\88\47\35\35\31\22\55\58\61\73\51\46\48\65\74\80\55\55\29\66\59\49\75\87\103\67\53\53\65\84\79\83\75\83\97\98\102\67\59\94\91\85\113\68\55\53\58\84\79\65\66\51\34\69\100\128\99\116\111\94\90\52\76\82\88\72\89\79\74\107\108\88\66\63\60\79\119\100\85\52\79\86\64\65\62\60\87\87\89\145\85\78\48\40\30\24\26\39\41\99\74\34\54\53\41\47\63\63\49\36\48\45\61\55\42\43\51\56\40\53\60\63\63\63\59\58\41\23\35\46\55\48\61\59\60\90\102\18\13\10\28\28\30\58\47\50\54\77\54\58\52\28\41\31\38\38\37\50\49\42\45\44\45\24\26\34\49\46\12\45\28\24\27\43\59\63\61\69\66\62\58\43\63\83\67\59\48\32\37\33\36\56\68\65\76\69\79\63\34\45\39\56\38\47\38\40\40\47\60\81\83\71\70\48\45\47\53\46\44\42\52\49\45\53\64\34\48\66\80\49\27\50\39\58\75\78\62\53\35\38\39\63\68\30\42\48\41\44\56\49\34\47\54\60\39\51\23\28\26\34\27\46\59\45\25\32\47\166\80\35\31\48\39\31\22\32\34\39\44\92\136\80\55\45\34\27\27\42\46\48\46\39\39\45\42\31\40\30\52\49\43\44\42\23\33\31\20\37\43\60\50\47\51\74</XData>
   <YData>49.576935\42.981632\40.723152\34.631073\37.797859\29.535469\46.166443\60.5923\61.963089\47.690723\27.090172\22.490158\35.648766\31.880201\50.889824\57.208061\61.682953\63.289913\48.132679\30.793358\25.988262\49.555283\32.103874\35.719315\37.229832\50.534393\64.866425\65.913986\59.646545\43.211212\50.602905\42.440685\49.850391\39.826847\29.882025\38.004372\53.380852\62.454853\74.217941\64.929962\77.799423\78.175896\64.792877\38.856468\36.249596\91.830719\36.922443\50.178665\57.262383\62.695152\60.183453\67.430412\64.424896\71.239021\57.364235\61.760735\73.340408\75.789665\75.092422\46.576237\41.271172\59.057526\72.708504\77.31041\78.201263\61.771061\51.565422\47.7939\55.2589\58.443115\45.877289\27.476454\35.400928\30.257696\29.253014\30.218361\28.43285\28.005217\36.318459\31.428432\29.952011\34.711647\52.654812\60.016392\43.005005\48.484035\63.848946\77.511177\54.098366\55.915977\72.043999\69.459694\62.914196\52.747978\39.223598\43.535858\44.372375\18.81319\26.197199\24.02729\35.181389\40.584442\27.385624\38.257374\25.733664\42.198215\48.329361\33.885063\29.408279\31.071266\115.5219\102.81664\88.907761\70.135719\44.514721\25.212\26.259993\28.456192\28.813303\42.242577\55.072014\54.812538\56.139923\34.156609\27.641352\38.777473\35.397644\31.693632\33.996021\67.320297\45.078896\33.519005\30.055052\31.057949\32.616676\44.939415\47.780224\43.12331\49.090492\50.728622\63.196823\72.434105\107.57549\105.13663\66.998978\96.055992\35.863293\50.628231\70.801254\82.412369\85.33683\72.985992\69.5597\81.231995\76.134933\106.23734\100.26416\93.696259\77.374451\55.833099\56.911957\45.259483\56.499969\57.645092\43.43169\44.114414\39.271416\38.658287\37.239285\28.439026\77.999863\17.586266\25.431967\35.234451\47.071945\56.346817\55.243809\59.110794\66.941292\57.438927\63.381386\54.846088\56.162712\55.320427\58.509323\62.030941\80.278343\66.975449\57.441261\54.834232\43.756226\48.650822\51.111855\63.781731\55.743732\53.303173\59.012863\52.905224\51.335495\44.336761\36.294468\43.812912\57.061558\53.246189\46.428391\43.076214\42.822395\36.508762\43.172173\60.006592\61.091118\56.646397\45.44175\43.048828\37.794571\39.920277\44.711658\45.161446\51.305042\53.611797\50.21022\52.701153\51.100609\45.912064\29.186806\34.790375\37.194595\37.746803\55.688938\68.381454\54.440285\50.426567\35.247337\45.899158\60.535645\37.692135\34.041031\39.805138\42.459332\54.254841\50.721188\54.676517\50.410198\50.539551\64.531891\66.495743\59.262829\59.540833\55.023075\55.457397\65.732506\60.511219\57.466789\57.294357\60.991253\64.925446\50.850063\48.884987\57.283417\70.974899\65.785484\62.445877\62.278912\31.226702\31.690336\32.125515\28.208378\40.373695\51.49831\62.605915\61.224163\51.336163\40.014179\46.23838\60.170506\67.767731\68.13401\59.619373\48.499565\34.919891\50.209255\59.230434\44.789513\70.933205\78.815651\102.6796\66.182205\45.446133\53.51759\62.915161\89.399979\63.455238\68.324173\72.282074\80.053505\84.197083\90.253189\91.514069\75.022758\64.429985\80.410492\93.784431\64.226021\96.258598\65.756393\51.645657\46.519554\54.817909\63.204018\62.315479\64.946274\70.349831\38.431744\39.706383\57.504951\77.787262\113.47337\94.864662\102.41367\106.69402\89.163002\78.42981\56.343853\64.112228\74.368225\82.081772\72.823631\76.962158\78.127731\66.556664\81.180237\95.59726\89.923218\66.110992\56.43235\55.589531\64.709015\84.222809\86.761017\80.999657\57.735332\69.949715\79.01535\70.256058\67.344246\58.487919\58.105743\68.707069\80.036819\86.004913\88.692711\88.453865\78.596619\53.227425\38.193192\35.568638\32.771229\23.358032\32.290623\39.47768\69.987282\70.969307\44.100136\51.195595\49.246338\43.948093\50.110603\56.244167\58.388504\38.945637\43.649666\47.165688\43.161293\56.507065\58.268326\45.777592\41.917213\45.649338\49.618446\50.396584\54.22966\61.160934\65.452316\52.654266\62.631775\60.967213\55.890759\43.491432\33.197266\35.341148\49.547535\58.710236\57.239635\63.96117\66.601578\59.073795\59.40955\81.639938\42.317047\28.682259\14.445518\27.977486\28.185114\28.305765\52.511627\53.977612\50.669674\54.813587\77.566681\61.009216\55.975616\49.902409\33.46822\43.47443\34.500534\36.156754\44.389473\36.814564\51.481163\53.454662\43.705894\47.853939\46.818306\54.262192\28.137196\30.423918\28.0026\51.855988\45.115849\28.234814\21.15836\31.933069\28.574194\28.76268\37.840057\56.098904\62.52319\63.438976\73.836998\66.726563\64.134773\57.267067\47.609566\63.967995\77.955582\68.168015\60.61097\47.055923\37.857849\42.6371\33.020309\39.146111\53.046669\68.551926\60.75779\73.863419\66.311844\72.472794\63.513893\35.481789\41.676655\44.700874\48.741962\40.775665\40.805641\46.34996\38.854698\40.810844\45.769905\52.36639\68.989426\79.43306\78.430931\68.069542\51.490906\43.070621\44.421959\50.573204\47.525379\68.305527\38.263542\52.292519\58.366978\42.929863\54.130959\63.68848\38.726238\56.338608\60.420635\75.047028\52.417255\31.443884\50.715698\40.057495\55.248928\73.548386\87.982193\69.987511\53.439171\33.753448\39.214539\45.746536\62.153126\62.727188\34.352692\40.710636\46.867214\39.453621\43.714874\58.575832\42.50742\42.75235\44.174828\51.849888\59.92215\41.61787\45.991982\36.634171\27.141205\27.230925\31.2458\32.50565\47.175472\93.441826\29.519775\35.057388\28.655134\74.347847\131.35043\44.879539\35.485096\29.508135\42.360317\47.266773\31.168081\28.627527\29.826195\33.74823\49.363712\36.959526\63.922882\95.027481\111.15491\54.927284\46.060577\32.479179\30.564388\29.282228\37.851837\44.126862\44.949959\47.428566\41.604889\38.711708\40.170776\45.965656\31.196417\36.195213\32.51749\48.869057\48.211578\41.065269\44.592117\42.989918\22.202675\29.326456\29.231346\26.59243\32.680054\42.708267\53.28186\54.825539\49.407429\50.059902\78.330734</YData>
   <Metadata/>
   <MetadataName>Sample</MetadataName>
   <XLabel>Real PM2.5(microg/m3)_ahead_1</XLabel>
   <YLabel>Predicted PM2.5(microg/m3)_ahead_1</YLabel>
   <Minimum>10</Minimum>
   <Maximum>166</Maximum>
  </LinearRegressionChart>
  <Table Id="EEcLP5" Title="PM10(microg/m3)_ahead_1 goodness-of-fit parameters">
   <Caption Id="IUdmfj">The next table lists the goodness-of-fit parameters for the output PM10(microg/m3)_ahead_1.
</Caption>
   <Data>0.28367168</Data>
   <RowsName>Determination</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>9</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <LinearRegressionChart Id="rVXjCl" Title="PM10(microg/m3)_ahead_1 goodness-of-fit chart">
   <Caption Id="0px4wx">The following chart illustrates the predicted values versus the actual ones for the output PM10(microg/m3)_ahead_1.
The grey line indicates the best prediciton result (outputs equal to targets).
Note that some scaled outputs fall outside the range defined by the targets, so they are not plotted. </Caption>
   <XData>16\17\11\13\10\20\27\30\20\11\9\13\12\22\28\31\32\22\9\10\24\14\14\17\25\34\35\35\18\18\14\17\12\11\14\23\26\33\26\36\34\22\10\13\57\18\16\23\24\25\29\25\31\19\26\33\33\33\14\16\23\32\37\38\24\17\17\20\21\14\9\16\12\8\9\10\8\13\13\13\13\22\25\16\19\28\36\20\18\33\32\26\19\9\14\13\6\8\9\12\8\9\13\6\17\19\8\7\7\52\42\31\26\10\10\9\11\10\16\23\21\22\12\12\15\14\10\14\37\22\10\11\12\16\21\18\18\17\19\29\34\69\72\41\64\15\20\35\47\51\38\32\46\44\73\68\57\42\25\24\17\24\24\15\17\13\15\14\11\52\11\8\12\20\25\26\28\35\32\31\27\32\32\33\35\30\27\25\23\20\20\20\28\21\15\28\21\21\18\14\17\23\23\17\19\13\14\14\27\26\24\18\19\13\14\17\18\24\24\23\24\28\17\12\15\16\16\29\36\17\23\17\21\32\19\15\15\20\28\25\26\19\24\34\36\27\29\26\25\29\27\27\29\30\33\22\22\30\40\43\30\21\13\13\13\9\20\23\28\31\24\19\20\32\36\35\28\26\13\32\28\21\38\43\66\40\24\25\34\52\34\33\35\43\48\49\50\39\34\53\54\36\55\28\22\18\26\33\30\27\38\20\14\32\46\78\59\63\65\51\46\24\35\39\43\35\41\40\33\47\50\44\31\27\26\33\47\36\32\20\36\42\35\40\33\29\36\38\46\53\35\34\18\14\13\12\9\14\15\38\27\11\22\20\17\23\26\26\15\14\21\19\26\26\21\18\20\22\22\29\31\33\23\27\27\26\18\12\17\22\30\29\34\34\27\30\33\5\7\4\11\10\9\22\20\24\25\39\26\25\19\11\18\11\14\16\13\21\23\17\19\19\24\8\9\12\19\15\5\5\9\10\11\17\20\27\26\34\30\27\23\15\30\41\30\24\15\10\14\13\17\24\30\24\29\26\34\27\11\14\14\20\11\18\17\15\15\17\21\30\33\33\30\17\15\16\18\15\34\17\22\26\17\23\31\16\27\31\37\20\8\22\16\26\36\46\36\25\16\15\18\30\29\9\14\18\15\18\28\16\12\17\21\27\16\21\11\10\7\9\10\22\58\16\10\10\46\344\103\30\14\19\19\11\7\12\12\21\17\30\50\59\30\20\12\11\9\18\18\19\21\19\19\16\23\13\14\13\25\22\17\21\21\7\13\11\8\12\16\24\23\21\24\42\42</XData>
   <YData>23.491358\14.870202\17.736462\13.329005\18.701523\14.948748\26.593601\30.480684\30.088486\20.207731\10.544116\16.174974\20.183601\18.565857\29.778643\28.696054\31.488178\29.475353\22.852446\15.71243\21.622593\28.032049\16.33667\19.213696\17.668655\26.509821\37.340862\28.403847\24.001614\19.094175\17.580465\13.185795\20.454092\12.250415\16.16291\22.13806\22.212387\27.850231\32.343082\27.485991\37.575356\36.389923\15.542169\11.97446\13.853019\32.792419\17.123638\13.657262\26.616398\27.424728\27.350143\28.179663\22.571989\23.493311\22.065157\28.733837\32.892529\30.587309\37.376678\17.956982\20.603874\28.223309\35.144615\34.146843\26.940153\19.246552\11.536728\15.841651\21.574915\27.59346\20.609352\13.191898\16.885881\12.270531\13.130555\17.304279\16.763494\13.406307\18.957699\16.477228\13.962579\15.520993\25.568983\21.745037\15.895209\20.851768\28.168177\27.283689\13.730783\22.019457\34.592358\31.682949\23.380402\21.755169\12.423312\22.765356\15.257729\12.721115\15.324177\15.708683\15.596446\8.6921415\13.497022\18.343956\12.799896\18.543133\19.366627\9.9335136\8.1526508\19.02618\35.235428\38.543396\29.844048\17.663162\9.5090275\10.898164\10.782681\14.563122\16.140114\15.698142\24.819843\16.549786\13.161241\10.133619\16.975025\18.553846\14.208502\12.889132\15.126731\25.309576\16.953823\12.298738\15.999328\12.518682\19.797379\19.933554\13.546622\19.283037\21.769802\22.750502\28.662613\31.351173\50.62265\52.968189\34.869949\34.825806\15.094898\26.812889\36.152199\37.878304\43.750416\36.009193\28.866407\36.476437\39.802891\55.56295\49.654263\40.656715\35.697956\26.335945\18.32477\17.935251\25.602617\26.671148\12.343956\19.05192\18.994812\22.303982\18.746674\16.78022\35.022171\11.390713\11.188066\22.20233\22.566895\25.805357\27.80962\27.653357\29.264629\29.398186\34.958481\31.905556\30.683508\32.658447\23.800858\24.868322\24.678759\27.01214\29.336067\23.691593\20.569328\18.992714\13.366478\20.285622\23.074219\18.943729\25.623182\21.173708\19.624779\17.073185\17.053612\23.510653\26.282139\20.275257\15.154532\19.037151\12.757771\12.190138\13.531155\25.466171\18.211359\22.392048\17.19087\16.833826\13.206705\20.057213\22.537563\21.967838\25.798353\25.067198\22.629597\24.064878\23.580128\17.740709\16.549009\20.682692\17.413664\16.095459\29.921513\32.57238\18.402229\27.278645\18.593388\26.571514\28.218567\17.394745\19.44488\20.648216\25.646217\29.711975\22.092064\24.137072\22.492836\30.091679\24.558758\28.258369\25.416332\31.178616\24.01893\22.153536\33.890114\31.682495\30.94681\30.526934\30.324984\28.621462\22.839291\29.135918\32.853134\36.345348\30.297836\24.684673\20.304728\15.720088\18.609831\20.528198\16.903656\24.465363\26.261814\29.736204\29.317684\25.114754\22.841579\23.583513\32.348915\33.144062\32.004681\25.629959\26.677435\21.232391\24.993523\27.520761\25.797453\37.426781\38.777126\50.797802\30.101723\26.424631\27.490517\33.801273\44.729164\34.601837\34.037674\35.791458\38.528851\44.499191\45.519531\39.146339\31.731533\28.210016\38.566788\46.184795\34.298588\44.258499\23.789759\19.983358\21.711809\30.808693\33.771427\29.388935\30.199923\30.539951\20.105009\20.328966\34.145153\37.614899\56.13147\48.709061\51.956516\49.656647\43.764587\39.085014\27.45628\36.091854\35.41666\36.896732\34.941616\42.443855\39.348728\30.083561\33.877792\43.976776\39.629112\29.949734\29.391445\26.911922\23.165905\33.196579\31.222572\31.034952\22.859278\39.730129\41.825188\33.382774\33.553566\30.409897\26.814095\32.490494\38.436874\38.107582\30.661287\33.959591\32.62632\19.58638\16.293467\20.382179\21.074524\16.566435\16.894287\15.675733\26.611229\21.911448\18.196766\29.627998\21.962135\21.191593\27.415777\24.174438\25.772919\15.415582\21.38608\27.306749\21.439548\30.04888\24.905523\21.410473\24.131039\24.251602\25.62447\25.887527\31.011414\28.646532\22.830191\26.18829\30.769144\30.387203\27.067822\19.26828\14.737478\21.275827\27.706142\33.335976\30.059805\35.42067\31.449274\17.397484\29.28809\24.724628\11.785767\14.522705\14.49111\17.437031\13.526375\15.586271\27.507984\24.923603\25.455637\26.337286\36.571301\24.167707\24.537786\23.897814\16.658405\21.45998\15.991493\19.803143\20.084505\14.073311\22.727211\18.430027\17.023893\20.659477\19.284935\20.933718\9.7203264\16.171154\21.759211\22.500326\15.611187\10.854044\18.049934\13.488455\19.436832\14.894224\20.360432\23.804976\29.184511\30.926247\36.021362\29.081924\27.371197\23.884275\21.821659\28.743917\37.212704\29.264782\18.182907\11.475824\9.6371098\16.372305\21.014633\22.393629\26.325043\31.82579\25.497211\28.982571\28.610229\33.030617\14.371468\11.995279\16.237967\14.341341\20.19017\17.137711\21.362986\19.103745\15.201049\17.35248\17.837111\21.4629\30.381325\35.464397\32.144291\32.557247\19.895601\13.552065\18.801979\22.055895\16.990164\33.543774\19.626642\25.713997\27.774906\23.802671\28.234097\32.028641\19.015575\31.407455\26.28178\27.189936\18.206059\15.99741\29.84877\19.716984\27.092636\33.531944\37.139847\30.148201\23.571829\17.035713\19.482517\22.212275\31.757917\23.264751\8.6430998\23.653795\24.421682\21.516266\20.14226\23.962057\9.8799992\10.383035\20.257895\25.242418\27.260096\18.381695\21.104507\7.2765694\18.395638\11.221561\15.562405\11.590452\23.998644\43.733421\11.412709\7.9565182\12.642807\37.806259\60.842953\60.17078\23.970869\14.22545\19.088692\21.595139\12.140357\8.405159\15.358855\13.505931\14.725245\14.691589\29.765511\43.030128\46.101822\23.861631\17.898468\12.430106\14.466576\14.595753\23.266481\20.016254\19.935715\25.310688\18.079712\18.300732\20.634562\24.768175\13.147949\16.703697\18.180649\25.623009\24.670475\21.806641\23.719442\21.555542\10.068855\18.51251\10.862805\9.2090254\23.223476\17.662708\25.146414\19.473156\18.103964\26.2939\39.995758</YData>
   <Metadata/>
   <MetadataName>Sample</MetadataName>
   <XLabel>Real PM10(microg/m3)_ahead_1</XLabel>
   <YLabel>Predicted PM10(microg/m3)_ahead_1</YLabel>
   <Minimum>4</Minimum>
   <Maximum>344</Maximum>
  </LinearRegressionChart>
  <Table Id="VxLFKY" Title="O3(microg/m3)_ahead_1 goodness-of-fit parameters">
   <Caption Id="GCHxom">The next table lists the goodness-of-fit parameters for the output O3(microg/m3)_ahead_1.
</Caption>
   <Data>0.58711481</Data>
   <RowsName>Determination</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>9</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <LinearRegressionChart Id="LOeT7d" Title="O3(microg/m3)_ahead_1 goodness-of-fit chart">
   <Caption Id="kldKJY">The following chart illustrates the predicted values versus the actual ones for the output O3(microg/m3)_ahead_1.
The grey line indicates the best prediciton result (outputs equal to targets).
Note that some scaled outputs fall outside the range defined by the targets, so they are not plotted. </Caption>
   <XData>29\33\32\31\33\32\37\46\34\30\31\28\30\24\27\26\37\33\35\37\34\30\30\28\32\38\29\23\27\22\27\28\26\28\27\12\15\16\24\26\19\23\23\19\20\28\24\19\18\14\13\19\9\14\16\7\9\17\22\23\20\10\9\14\22\16\20\18\13\17\27\25\27\26\27\30\29\27\24\30\31\18\15\27\14\11\3\18\18\9\3\21\20\24\21\21\26\17\29\24\29\26\24\27\15\20\19\24\25\2\11\10\6\24\23\27\31\22\17\18\11\12\33\33\27\22\18\23\21\24\27\29\27\28\21\31\30\28\26\24\24\28\26\31\27\30\26\24\20\25\29\30\28\27\21\10\22\31\29\23\29\32\28\34\35\33\40\28\34\33\30\36\34\30\35\38\37\34\29\39\35\35\38\37\27\32\36\38\41\34\28\24\32\25\32\25\31\30\38\39\41\38\32\34\28\34\35\28\33\30\34\34\38\35\39\36\40\41\42\39\42\34\36\35\37\33\40\27\18\40\33\45\46\38\43\38\33\48\47\44\48\54\45\40\37\43\42\40\33\43\44\46\46\48\48\46\43\64\40\45\28\31\39\28\28\21\33\47\44\45\36\35\48\38\41\39\31\36\33\23\33\39\45\48\45\33\40\40\54\63\61\59\49\49\61\56\43\43\44\47\44\44\46\48\32\38\45\38\37\40\44\33\42\40\40\40\47\45\46\43\63\48\47\51\45\46\50\47\42\47\55\53\46\41\39\38\25\30\42\50\42\25\38\33\30\30\30\35\23\14\29\30\37\32\38\32\28\29\33\22\36\37\32\38\35\35\30\37\26\29\21\27\29\23\35\33\32\37\30\30\33\24\32\19\27\31\27\27\27\29\19\30\27\28\27\17\13\27\23\28\19\26\26\23\28\25\23\13\23\20\23\26\20\25\23\22\21\22\13\13\14\18\19\28\28\28\14\26\26\21\26\25\30\28\11\11\13\14\14\14\17\17\7\7\15\13\13\18\21\29\27\13\9\10\17\10\10\31\30\20\26\27\31\25\18\27\26\29\24\21\14\20\14\29\27\30\27\25\27\28\29\28\29\29\27\33\22\22\18\32\32\30\30\27\19\19\26\30\31\29\29\17\25\27\28\30\28\25\21\24\22\29\24\29\33\25\32\31\34\25\32\29\30\28\30\36\25\22\28\29\24\28\31\30\31\28\30\22\28\29\33\32\26\39\36\39\37\37\33\37\32\30\28\31\35\36\33\37\41\41\43\43\32\37\39\31\33\38\54\36\33\30\43\32\37</XData>
   <YData>36.334312\32.886414\36.321846\32.974823\32.865524\34.389439\33.686718\37.288448\43.17234\33.465885\29.206261\30.460629\28.198008\32.509689\26.657665\30.266399\29.507519\35.577118\31.447618\31.976194\34.385319\33.141312\28.918467\27.898575\27.699108\28.540874\31.812649\29.668737\27.186672\29.338873\25.805975\28.710115\26.217247\25.79521\25.825855\23.256794\18.987091\17.874813\18.613956\24.399809\23.643059\18.669651\24.764534\21.212543\21.204416\25.052036\28.524103\25.71216\18.555771\16.140951\15.185469\15.00095\19.040485\16.175146\15.543478\17.058228\10.865698\13.448355\16.242893\22.921156\21.162468\18.380878\13.451159\12.177492\17.251493\22.349337\21.729446\21.305843\19.861706\14.429653\17.093201\22.420872\20.689812\22.840687\22.460979\23.523403\25.809166\24.944901\23.980726\24.059824\29.538544\26.988319\20.891113\18.749714\22.718269\16.539923\14.538136\13.508984\20.03146\16.664251\11.441553\10.206381\17.490341\18.227787\20.493597\20.379894\20.624557\21.99868\18.94614\24.795616\23.049313\57.082626\22.098793\20.264013\23.382471\18.638481\20.21104\20.979881\26.51198\21.823704\11.083885\4.2499084\20.827335\14.261963\24.981476\25.289047\26.568802\31.106916\25.738029\23.464283\25.075581\17.653242\20.089794\32.394386\28.610439\29.870083\26.437033\22.068375\26.277676\24.070309\26.510128\26.685553\29.620913\27.203339\27.612024\24.405649\34.750553\28.788227\26.814495\26.432055\24.096031\23.983595\25.763746\24.078503\31.192078\28.103775\33.709637\27.086033\24.058453\22.148792\25.911465\28.767923\29.241333\28.162815\24.942871\19.632751\13.823393\23.397961\30.215225\27.947622\25.228947\26.812542\31.089584\29.092577\35.890045\33.045887\30.412809\38.811287\29.834591\36.381958\30.85445\30.618998\34.403728\29.705858\32.879803\35.056606\38.112804\37.180355\35.016666\30.06312\38.472874\38.630104\37.529308\39.52446\41.864044\33.587315\36.015526\36.483871\39.469379\41.926682\34.366951\33.160488\28.813066\34.335323\26.990307\33.00412\27.757729\31.726456\31.83247\37.239964\35.202011\38.994907\40.429405\34.141411\36.572914\31.446712\36.244217\36.125729\29.70537\34.445412\32.82011\37.342361\36.767059\39.544254\35.348228\36.995499\35.613876\39.991165\41.304222\42.524166\42.165154\45.479797\34.908073\38.96941\36.06535\38.446491\35.601273\43.75528\32.665588\26.890709\44.322636\40.726734\47.434658\47.210678\44.121811\44.431183\37.716713\37.863335\47.41814\47.956787\48.570057\50.186462\53.648853\48.128159\47.135517\44.202728\45.175755\45.371941\44.235199\35.767311\44.360023\44.93248\47.222347\48.310165\50.612984\51.01062\50.437141\46.985905\52.176003\44.340984\51.239098\37.524597\40.315403\44.675198\31.629154\33.863499\28.409763\39.774227\46.723709\48.588326\48.658619\39.907955\40.899536\50.98774\45.519081\45.694347\45.015324\39.176083\43.659473\41.058529\33.922588\39.469803\43.97747\48.116436\50.333412\46.019268\41.260147\44.116852\45.891731\52.382725\57.112534\57.47802\57.625626\50.958145\51.781952\54.214943\53.457863\49.496025\47.389088\51.032349\50.780975\47.739761\49.536919\51.222519\53.758339\40.723076\43.440598\45.674934\42.898018\42.423149\44.968857\49.558834\40.404442\45.738617\45.908089\45.000374\44.215034\48.583565\50.819954\54.162861\49.175224\54.091507\50.03083\49.882042\52.204258\51.002762\50.636047\50.677345\50.476776\46.821266\51.857216\53.675362\54.409973\49.853111\45.98481\45.65934\45.841782\34.981766\37.460678\45.440178\48.86467\42.263126\30.545956\42.038025\38.7169\35.437523\36.219124\35.256031\37.149303\29.709923\25.422672\36.740059\35.952316\40.080494\35.765316\36.637745\35.519257\31.098829\32.300964\39.039177\30.549654\37.97987\36.233276\34.757622\37.50071\35.436737\35.61483\31.774706\35.689987\27.02919\30.573185\23.530193\30.810034\29.170532\26.473232\34.570133\33.212944\33.731426\35.77145\31.221914\30.869852\34.240887\26.189171\29.861727\22.171455\28.530602\32.243256\28.170919\28.971405\27.242764\27.250307\21.093136\28.64094\26.976131\27.117012\26.580765\24.497568\25.320301\32.142765\24.560757\27.508234\19.321205\24.020844\22.355019\20.33548\24.581535\22.380554\21.316864\16.984186\22.733284\22.094055\21.805801\24.725897\19.634388\23.982468\21.371918\23.372898\21.416706\20.645866\18.1036\15.137096\15.731764\19.000202\20.238916\24.586044\23.755039\25.042698\18.112476\23.308514\22.586243\19.154282\22.783842\21.714153\29.00106\25.630747\16.998375\13.051115\14.000481\14.447409\14.376556\17.313736\18.474205\18.275085\12.099697\12.725431\16.110018\18.421265\20.042475\21.524889\23.649826\25.069466\23.727737\17.52302\15.633961\17.33083\47.463631\13.177588\13.947044\31.416273\30.019384\20.44515\25.901279\22.971851\25.609432\27.089933\20.510458\28.05467\23.181717\26.408689\20.764477\17.330807\14.023214\18.295876\14.777346\27.334404\27.655714\25.737068\24.945675\24.193918\22.98085\25.9683\26.753422\25.824791\27.289045\26.279125\25.656616\32.354893\25.15242\23.226732\21.957027\29.542053\28.640228\30.281631\27.903492\28.032595\19.4422\20.717587\25.71863\28.233486\29.981876\27.531258\26.950451\24.233387\27.710903\27.827322\26.584558\31.556522\27.695721\27.881216\24.088631\27.943462\24.463966\27.036289\23.78516\27.446018\32.358093\26.04722\32.674633\27.573147\30.819813\26.321877\30.225847\26.740993\26.581364\28.48053\30.663012\33.987076\24.110767\18.205238\30.70013\35.66259\27.055021\27.850914\28.111572\30.443439\32.095436\28.409752\28.148771\25.810303\28.045082\26.466707\29.509066\31.618767\32.781681\38.78326\32.782272\33.327927\31.50769\32.598392\31.420736\35.71291\30.960627\33.84227\31.840298\31.948862\36.867237\42.36372\37.284264\37.942699\43.379055\42.338108\43.125443\44.448074\37.171539\41.014954\38.959641\34.364017\33.799847\34.992359\49.595581\36.003799\36.638565\32.120659\40.722771\33.355461</YData>
   <Metadata/>
   <MetadataName>Sample</MetadataName>
   <XLabel>Real O3(microg/m3)_ahead_1</XLabel>
   <YLabel>Predicted O3(microg/m3)_ahead_1</YLabel>
   <Minimum>1</Minimum>
   <Maximum>249</Maximum>
  </LinearRegressionChart>
  <Table Id="0dQ9fK" Title="NO2(microg/m3)_ahead_1 goodness-of-fit parameters">
   <Caption Id="doZmS6">The next table lists the goodness-of-fit parameters for the output NO2(microg/m3)_ahead_1.
</Caption>
   <Data>0.35961556</Data>
   <RowsName>Determination</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>9</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <LinearRegressionChart Id="nc2amB" Title="NO2(microg/m3)_ahead_1 goodness-of-fit chart">
   <Caption Id="e9wig3">The following chart illustrates the predicted values versus the actual ones for the output NO2(microg/m3)_ahead_1.
The grey line indicates the best prediciton result (outputs equal to targets).
Note that some scaled outputs fall outside the range defined by the targets, so they are not plotted. </Caption>
   <XData>18\14\11\13\8\28\36\35\23\10\9\8\16\25\31\34\34\15\6\8\28\16\19\21\26\27\32\24\16\17\20\19\9\16\17\26\30\35\29\28\25\21\16\22\18\14\21\29\30\31\34\32\30\15\27\31\30\31\27\27\29\40\40\34\27\29\22\22\30\28\25\27\19\14\13\11\12\21\17\13\14\23\26\20\31\32\33\19\20\26\27\25\16\10\22\20\14\16\17\20\10\14\21\5\18\16\12\8\10\65\46\42\38\16\14\10\7\13\17\21\18\17\9\8\16\15\13\14\17\19\10\14\17\17\21\20\21\25\21\27\34\33\35\21\9\19\25\25\21\26\23\10\23\21\27\30\23\14\12\27\16\27\25\18\21\16\18\14\12\15\11\10\12\25\29\29\29\23\13\10\17\18\18\11\10\16\14\20\29\15\14\18\11\10\15\24\15\16\16\10\10\23\21\17\18\15\10\13\19\21\13\13\18\11\10\9\18\25\25\19\19\10\14\13\14\15\15\9\7\14\19\11\19\18\8\7\16\14\16\16\17\12\12\14\14\16\16\21\8\15\20\22\20\19\14\7\8\17\12\14\12\6\2\2\3\5\8\15\23\18\9\11\15\19\24\27\13\8\2\11\14\12\20\22\18\12\9\11\15\24\15\10\18\27\28\27\30\10\11\13\22\20\18\19\9\6\11\18\12\20\14\8\9\11\12\21\16\25\21\13\14\12\12\14\15\21\19\16\12\13\22\23\14\10\14\14\12\15\21\17\21\20\19\13\14\12\16\19\22\13\18\23\20\14\13\14\10\12\11\17\16\13\31\25\22\22\26\21\9\19\18\23\32\32\20\13\19\17\23\30\31\32\16\26\26\22\20\11\18\23\32\26\34\31\28\12\8\4\8\6\18\19\15\28\32\31\27\44\32\29\23\17\27\21\24\23\20\19\22\21\20\28\31\17\13\18\32\22\11\6\8\15\16\24\29\30\39\48\38\31\25\20\31\29\25\29\19\13\15\11\16\21\33\29\33\32\29\22\18\19\27\28\15\22\28\28\27\26\29\30\40\46\43\31\22\20\22\26\25\27\33\34\33\35\37\15\40\38\34\16\10\34\25\32\41\38\24\18\16\23\26\36\35\13\18\28\26\31\33\20\17\14\27\30\20\29\16\16\7\13\13\29\26\22\14\12\21\25\22\14\16\13\9\17\11\15\18\16\12\12\17\21\18\23\15\10\10\13\18\19\19\13\11\10\5\14\18\7\12\15\16\15\14\12\19\16\10\13\21\26\22\22\15\24\17</XData>
   <YData>25.895849\19.229265\17.967318\14.119949\17.224787\18.80002\29.659605\35.429695\33.856674\24.329113\15.13598\14.969031\18.645367\24.345514\30.064842\32.828419\32.437588\29.229982\20.771412\20.363691\24.268944\28.587313\22.358652\24.029943\20.791374\28.174158\34.596512\28.493404\20.939177\21.699389\20.531296\18.358484\24.270739\18.687153\24.623461\26.511038\25.498335\32.574154\32.400497\30.855686\34.825535\31.758469\21.464806\22.597507\25.545719\12.967082\17.998915\23.792612\32.255592\36.329407\34.801453\34.727455\28.334641\24.900341\26.659367\31.604668\33.680359\33.302116\38.49247\27.697721\30.104124\36.37616\41.975639\40.228981\31.108776\26.292229\21.830942\21.916718\27.990469\36.200352\34.723499\29.816616\30.04454\21.771393\19.770882\23.597332\25.128458\21.440643\26.893703\24.939978\17.099543\19.230686\28.932156\26.606693\25.90407\31.305914\33.889473\25.67853\19.834324\29.801439\31.672245\31.128387\29.153503\24.567739\16.811378\29.276854\27.748915\26.817249\22.883818\26.587938\24.721914\17.918781\19.083427\26.503119\22.131086\19.213322\22.161087\18.247334\13.227086\20.922913\37.887283\50.189674\34.826637\31.790873\20.779034\15.692923\14.424045\17.217278\21.101871\18.958076\23.965357\17.612703\10.818426\13.698366\19.511723\21.157629\18.329216\19.463501\17.978924\11.586245\16.749454\18.171852\22.391115\21.177902\22.953569\21.649418\16.301363\23.929684\30.8839\28.07336\28.730757\34.052422\27.728996\26.972841\22.250385\13.63292\19.531445\29.991386\31.314312\21.105877\23.854412\25.519508\19.481289\20.841345\24.273472\31.836708\26.741823\19.07605\17.895412\21.903372\23.314203\22.319887\28.955362\28.755791\17.002686\23.07868\26.59407\26.859558\21.586555\20.537607\13.435822\12.314791\15.59784\24.662809\26.976721\29.42729\30.765594\28.872509\19.162596\18.06385\24.107588\23.749781\20.732546\23.349201\10.310179\6.6814613\15.553909\22.45285\24.817877\28.140541\20.215611\16.186811\12.752663\12.685037\19.728495\22.721373\21.846706\21.870043\17.22534\15.114046\16.498619\22.007956\24.938984\20.906261\18.68255\19.012728\13.707614\11.973033\18.577917\20.031212\17.919605\16.315205\15.7665\14.28603\13.364483\20.599859\19.540089\22.984787\26.524605\25.926867\17.840803\19.632627\16.859322\18.438103\20.398159\20.851421\18.600531\14.560035\14.851748\17.896135\18.128456\21.479092\17.264612\22.286957\14.512372\11.114175\15.774656\23.049585\21.840076\20.080582\14.696096\14.093623\15.967006\21.586363\12.074186\12.336686\19.838223\19.825661\17.132664\11.216206\25.355606\25.105057\25.56188\22.993557\20.931543\11.32801\11.49419\19.894768\22.027557\15.584397\13.824124\12.067801\8.6671429\8.2311668\12.877518\17.837811\15.99875\16.047544\19.589054\19.511246\17.699228\17.06712\17.452326\18.424116\20.359749\22.148716\21.043373\14.695981\17.287693\15.240807\13.179106\18.619699\19.474701\18.949783\20.550369\24.172207\12.483781\17.009909\16.368059\18.757805\20.181606\17.397415\17.445635\20.265385\23.258583\30.090458\27.747654\18.175009\11.781128\14.279321\15.449558\22.107882\23.044153\18.93664\13.561244\8.5205126\16.403872\19.947449\21.433178\17.975039\22.663534\11.407434\10.557906\17.571711\20.33082\15.499001\29.491718\24.162661\21.59313\18.421217\19.323048\18.768576\18.683125\18.289471\17.028736\12.535514\19.129126\25.909761\20.181568\14.386686\14.039933\22.918285\18.841185\14.597196\18.128351\16.904491\12.489695\13.184912\20.430834\20.098839\19.034342\28.116102\24.811945\22.318321\16.583164\17.117777\11.717977\17.580387\25.544304\20.798794\11.317765\27.039373\25.511335\19.291628\16.123009\22.789503\23.659527\20.774239\18.471979\15.490386\10.585516\15.196241\24.040216\32.934269\28.158852\26.19854\26.077765\22.204662\23.283377\19.043829\25.811668\27.18215\27.207121\34.129658\27.606039\23.057962\24.242062\25.54175\24.418898\27.254725\30.929905\27.011669\23.679523\26.132225\30.283815\31.106007\26.730183\24.22983\13.755348\22.074095\31.163147\34.85405\30.460545\35.477165\31.002449\22.090597\16.126295\14.973975\16.370802\20.490816\21.606161\24.548695\21.892265\22.97555\34.372917\36.509483\34.170033\30.512157\39.646511\30.04089\29.329952\32.254402\27.126102\29.304844\27.436167\30.186142\25.55018\22.647152\26.377514\23.367378\24.088602\25.607378\29.069036\25.875904\20.826273\23.363167\30.308531\32.036518\27.201187\22.13084\25.087095\21.150766\26.242092\19.798111\26.673351\33.205921\36.169968\43.270821\47.243771\39.12735\31.645357\29.426743\31.752748\32.106476\31.371998\31.498653\26.548128\17.01597\16.100674\21.337234\25.842619\25.674156\26.29397\34.316925\29.915983\34.227013\37.268822\32.067734\19.137423\22.020866\21.928755\24.667475\28.81567\27.942036\27.263956\28.64945\28.203953\28.869299\25.8356\29.945765\35.259556\41.573906\43.458714\43.820755\33.029419\20.157604\24.587162\30.298775\27.863514\27.94323\29.992743\34.408104\31.644329\34.699623\38.764313\37.070187\24.446762\39.349693\34.287277\24.291679\20.414227\23.403275\36.348362\30.726727\32.678905\37.655167\29.342232\24.451744\24.314129\22.781704\27.065248\30.847992\32.566246\27.186644\16.558245\28.579014\32.42923\31.340721\30.533648\28.857758\15.547845\17.109348\23.316177\30.342491\31.114994\25.372095\28.101725\16.67749\21.217915\16.762396\22.08783\18.673935\29.600286\23.682789\18.333536\15.55738\19.103298\21.867725\24.400316\34.32954\16.565783\15.358016\17.021517\20.301188\20.837011\17.357498\20.953648\21.162739\11.701401\14.698265\21.237995\21.488914\21.228985\18.228622\20.668083\15.789797\16.186043\19.780294\22.989611\21.258789\22.945341\24.810265\14.393605\14.816806\19.767689\16.243679\15.908729\19.991777\16.999125\12.803732\18.798197\23.537664\19.669416\17.083557\19.501493\21.199924\14.351929\14.717433\22.317739\23.389353\24.944298\21.183386\20.699883\18.531773\24.312643</YData>
   <Metadata/>
   <MetadataName>Sample</MetadataName>
   <XLabel>Real NO2(microg/m3)_ahead_1</XLabel>
   <YLabel>Predicted NO2(microg/m3)_ahead_1</YLabel>
   <Minimum>2</Minimum>
   <Maximum>74</Maximum>
  </LinearRegressionChart>
  <Table Id="vTXKuI" Title="SO2(microg/m3)_ahead_1 goodness-of-fit parameters">
   <Caption Id="tLMLnV">The next table lists the goodness-of-fit parameters for the output SO2(microg/m3)_ahead_1.
</Caption>
   <Data>0.044132173</Data>
   <RowsName>Determination</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>9</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <LinearRegressionChart Id="Y9AsXP" Title="SO2(microg/m3)_ahead_1 goodness-of-fit chart">
   <Caption Id="WHGFWJ">The following chart illustrates the predicted values versus the actual ones for the output SO2(microg/m3)_ahead_1.
The grey line indicates the best prediciton result (outputs equal to targets).
Note that some scaled outputs fall outside the range defined by the targets, so they are not plotted. </Caption>
   <XData>3\3\3\3\3\3\3\4\3\6\3\3\1\1\5\0\0\0\0\0\0\0\0\0\1\1\1\1\0\0\0\1\1\1\2\1\1\1\1\1\1\0\1\1\1\0\1\1\1\1\1\1\1\1\1\2\1\2\1\1\1\1\2\2\1\1\1\0\1\1\1\0\1\1\1\1\1\1\1\1\1\1\1\1\1\2\2\0\1\2\1\1\0\1\1\1\1\0\1\1\1\1\1\2\0\0\0\0\0\1\17\0\0\0\1\0\0\0\1\1\1\0\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\0\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\2\1\1\1\1\1\1\1\1\1\1\1\0\1\1\1\1\1\1\1\2\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\2\2\1\2\2\2\0\2\2\2\2\2\1\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\3\2\2\2\2\3\4\3\3\2\3\5\3\2\2\2\3\3\3\3\2\2\2\3\3\3\3\2\2\3\3\4\7\7\0\0\0\4\6\7\6\4\4\3\4\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\2\2\2\2\2\1\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\1\0\1\1\1\1\1\1\1\0\0\0\1\0\1\1\1\1\1\1\1\2\1\1\1\1\1\1\1\1\1\0\0\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\1\2\2\2\1\1\1\1\1\1\1\1\1\0\1\1\2\1\1\2\2\1\1\0\1\2\1\1\1\1\2\1\1\1\2\2\2\2\2\1\2\2\2\2\2\2\2\2\2\2\1\2\2\1\1\1\2\2\1\2\2\2\2\1\2\10\2\1\1\1\1\1\1\2\1\1\0\1\1\1\1\1\1\1\1\1\1\1\1\1\2\1\1\1\1\1\1\1\1\0\1\1\1\1\1\1\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\2\1\2\2\2\2\1\2\2\2\2</XData>
   <YData>3.3169403\2.8417008\3.1799984\3.5163391\3.5500944\3.8097439\3.2750189\3.0537617\3.3957677\2.764642\5.2097049\4.3415074\3.775517\2.3454916\1.7636173\4.0979567\1.6117215\0.5367074\0.42888045\1.0116305\1.3046949\1.0212114\0.75643659\0.85148001\0.6304245\1.242548\1.5693369\1.039475\0.652143\1.0989082\0.80600929\0.7556181\1.5445025\1.7434546\1.8919057\2.3374276\1.9228231\1.762967\1.5548266\1.405218\1.5672691\1.7357621\0.6666925\1.5573744\1.679754\0.80378747\0.73235965\1.302048\1.8965731\1.8783202\1.7641309\1.6287619\1.2112151\1.0657544\1.7693597\1.8810791\2.3441143\2.0053673\2.5613256\1.461256\1.6111079\1.9134436\1.8498807\1.9964821\1.7718419\1.2554947\0.93322635\1.1889848\1.093641\1.7764418\1.6792001\1.4283304\0.99783707\1.1735541\1.3387208\1.8668398\1.99288\1.9200858\1.879868\1.7365971\1.4533659\1.6565758\1.9844044\1.5886003\1.5190964\1.6905627\2.2612858\1.9264885\0.85738444\1.7831076\2.3875484\2.1091633\1.6835024\1.2244852\1.3773121\2.0177519\1.7787024\1.7339756\0.8536973\1.6547195\1.6591414\3.5623124\5.5819044\1.7940847\2.1795924\1.1366334\0.90580463\0.68584633\0.54623961\1.1468835\2.3126469\9.8629923\3.8803005\0.56054258\0.38896942\1.0098245\0.95457172\1.2143244\1.0236442\1.4660943\1.7380555\1.5494897\0.5985837\1.1940074\1.9571042\1.5931302\1.5141138\1.906038\1.6117787\1.0804143\1.0155282\1.4386046\1.630306\1.6118573\1.8959129\1.3950068\1.3570288\1.6085118\1.7938969\1.752365\1.6817249\1.6324605\1.1534525\1.1006758\0.92116761\0.87674928\1.3429376\1.6544436\2.0470924\1.4137735\1.2971265\1.3547742\1.3576442\1.2930455\1.4313661\1.7550004\1.3885366\0.93502045\1.0007946\1.0273333\1.1853325\1.3707331\1.6640373\1.4794161\1.0696614\1.425889\1.7245084\1.6239045\1.4782814\1.6129854\1.3394746\1.1470553\1.5488547\1.8767455\1.5364921\1.2712045\1.3508672\1.2737128\0.9050982\1.2643944\1.8913009\1.6381166\1.3303447\1.55226\0.73033428\0.85786939\1.6030684\1.5238661\1.4045136\1.2763741\1.2350234\1.2407473\1.2261286\1.1379561\1.715827\2.2750502\1.5307047\1.6931902\1.2732594\1.2460686\1.4662344\1.7929512\1.4890935\1.1783317\1.2648298\1.312331\1.1730622\0.71560359\1.4099596\1.5117342\0.95194936\1.2108277\1.3950884\1.1429664\1.1290938\2.3555217\1.8686877\1.7244269\1.4320384\1.1995137\0.94193363\1.1414144\1.3366843\1.6194327\1.626142\1.7323318\1.6685557\1.2109373\1.3060508\1.7710658\1.5496917\1.3578961\1.411624\1.5676566\1.5825707\1.9464747\1.922609\2.4379148\2.2305298\2.2564707\0.76888108\1.3595339\2.1548853\2.4711027\1.724934\1.8344618\1.7595085\1.8772857\2.0272975\2.1732922\3.084409\2.2528458\2.0619478\2.0306809\2.0119271\1.8099066\2.1938677\2.7056744\2.305779\2.1690669\1.7700518\2.1536803\2.3749764\2.2170081\2.857187\2.8549378\2.528142\2.50894\2.5564265\1.9390379\1.8261269\2.3041706\2.3419309\2.1630764\2.1865385\1.8172386\1.7087164\1.8215135\2.434792\2.422298\2.3568845\2.477931\2.3758233\2.0526295\1.966025\2.8614683\1.9918085\2.4570701\2.428128\2.2413664\2.4549191\3.3733165\3.3727274\3.1299174\2.0842299\2.5654781\4.2247939\2.930934\2.0269115\1.9074216\2.0430202\2.7548687\2.7290084\2.6516395\2.6611378\2.1703224\2.6185412\2.4050097\2.8099651\2.9813101\3.2009525\2.5519013\2.2716553\2.5243456\2.9126816\3.0763836\3.8302011\5.8577337\6.4407034\1.6753654\0.6858139\0.68820739\3.1813357\5.4976206\6.3869319\5.72966\4.4442501\4.1843786\3.1688237\3.6172378\2.985256\3.0572424\2.715203\2.7133925\3.258863\3.0253711\2.5967293\2.8240035\3.5039937\3.1480396\2.9583519\3.4267433\3.1617069\3.0366426\2.9144185\3.2625897\3.0322182\3.2936895\3.6339197\2.8008344\2.5906274\2.8900652\2.2343087\2.0321167\2.0420046\2.6405034\1.7942977\1.1605494\0.93913126\0.72824073\0.23304057\0.65202093\1.0561285\1.1739662\0.75096798\0.97733879\0.90185738\0.52654147\0.51128602\0.61831522\1.0718656\1.0641923\1.0249219\1.0213084\0.4680562\0.29785204\0.73600793\0.99586415\0.87774467\1.0296788\0.73000884\0.4767468\0.17426467\0.63342237\1.1565582\0.91975713\1.0078876\1.3415077\0.64137983\1.4766389\1.8186605\1.6592116\1.2681419\1.6088583\1.3242042\0.55815482\0.67184949\0.65349555\1.0649517\1.6030406\1.5480361\1.6467392\1.5732933\1.8794204\2.0851221\1.6730462\1.3517903\1.4302331\2.1075189\1.1726955\1.1191714\1.580433\1.5181283\1.670188\1.605256\1.6896173\1.4481905\1.4249688\1.1287801\0.72720289\1.2781831\1.6624554\1.5784682\1.0297365\1.1211116\1.7867891\1.8877954\1.7238824\1.6065561\1.7391742\1.6846442\2.2314816\1.9716105\1.6198721\1.8071027\1.8614677\2.0018568\2.3545411\2.3700781\2.0595372\1.5679244\1.3859186\1.7598358\1.6688827\1.4826707\1.6138186\1.2394001\0.99150205\1.3542215\0.93052387\1.6624935\1.9918365\2.571152\2.2574961\1.4336058\4.340807\6.6082287\1.7756865\0.87974906\0.92282343\1.3032485\2.0756617\1.7188358\1.8706967\1.5215727\1.4560055\2.0267406\1.8106611\1.4832048\1.5420213\2.3754504\2.5004699\2.2163658\2.2448828\1.8544909\1.1664546\2.0453997\2.6814752\2.2352278\2.2632008\2.3751686\2.3826575\2.0015719\2.095305\2.394932\2.0749171\1.2387004\2.4141531\1.9319115\1.0054247\1.1344769\1.8333249\2.4435253\2.1845913\1.7509991\2.0974247\1.7234875\1.7594341\2.1528769\1.7700071\2.2039709\7.6842518\4.2998552\1.1233349\0.87003779\1.8962661\1.772576\1.4956254\1.3970498\1.6698819\0.83827996\1.3956169\1.2267001\1.5639489\1.3814902\1.3926973\1.4391379\1.0712173\1.6630365\1.5776892\1.6376318\1.5406966\1.769906\1.2971648\0.93314981\1.1534916\2.200562\1.715646\0.48240089\0.6588335\0.97480631\1.3063234\1.633188\1.8214948\1.5532973\0.84382892\1.6126562\1.7260635\1.1104498\1.3899\1.7593725\1.9017378\2.0076208\1.5921853\1.8753005\1.9453174\2.5037184\2.969136\2.4691262\2.1799142\2.338871\2.3400955\2.0246367\2.3734469\2.7219977\2.369277\2.0183573\2.2789431\2.392633\2.1352623\2.2987566\2.3938346\1.9677333\2.07019\2.4049866\2.3776293\1.447206\2.083601\2.8238027\2.2636037\2.0814328\1.4259511\1.904084\2.2981014\2.3165896</YData>
   <Metadata/>
   <MetadataName>Sample</MetadataName>
   <XLabel>Real SO2(microg/m3)_ahead_1</XLabel>
   <YLabel>Predicted SO2(microg/m3)_ahead_1</YLabel>
   <Minimum>0</Minimum>
   <Maximum>17</Maximum>
  </LinearRegressionChart>
 </Task>
 <Task Id="RPjRWN" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="8h8kbS" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="lckRG6" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="AMtWeR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0203094\115.929\6.06557\8.1207
0.000130189\0.743133\0.0388819\0.0520558
0.0130189\74.3133\3.88819\5.20558</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ZKBHUG" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="rpo8xh">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0114136\306.194\6.66948\14.0185
3.35693e-5\0.90057\0.0196161\0.041231
0.00335693\90.057\1.96161\4.1231</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="yb11Ll" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="zUGxcK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000202179\37.4636\4.87045\4.40991
8.15238e-7\0.151063\0.0196389\0.0177819
8.15238e-5\15.1063\1.96389\1.77819</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="bah0ui" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="PTGyd5">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.043829\44.0771\5.25286\4.23326
0.000608736\0.612182\0.0729563\0.0587952
0.0608736\61.2182\7.29563\5.87952</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="w4GGqg" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="pNBNiI">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00157189\14.6874\0.69559\1.02446
9.24643e-5\0.863962\0.0409171\0.0602626
0.00924643\86.3962\4.09171\6.02626</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="y8qAor" Title="Inputs-targets correlations" Component="Data set" Name="Calculate inputs-targets correlations">
  <Text Id="UACt02" Title="Task description">It might be interesting to look for dependencies between single inputs and single targets in the data set.
This task calculates the correlation coefficient values between all inputs and all targets.
Correlations close to 1 mean a strong relationship between input and target variables.
Correlations close to 0 mean no relationship between that variables.
Note that, in general, the targets depend on many inputs simultaneously.
</Text>
  <BarsChart Id="ojg4HU" Title="PM2.5(microg/m3)_ahead_1 correlations chart">
   <Caption Id="2ay06V">The following chart illustrates the target 'PM2.5(microg/m3)_ahead_1' dependency with the 21 input columns with greatest correlation in the data set.
 </Caption>
   <Data>-0.594\-0.444\-0.295\-0.291\-0.208\0.161\0.18\0.198\0.226\0.244\0.247\0.257\0.28\0.314\0.329\0.391\0.473\0.482\0.638\0.701\0.884</Data>
   <XTitle>Correlation</XTitle>
   <Names>WINDSPEED(km/h)_ahead_1\WINDSPEED(km/h)_lag_0\WINDSPEED(km/h)_lag_1\O3(microg/m3)_lag_0\O3(microg/m3)_lag_1\TAVG(C)_lag_0\TAVG(C)_ahead_1\TMAX(C)_lag_1\SO2(microg/m3)_lag_1\SO2(microg/m3)_lag_0\TMAX(C)_lag_0\TMAX(C)_ahead_1\PRESSURE(hPa)_lag_1\PRESSURE(hPa)_ahead_1\PRESSURE(hPa)_lag_0\NO2(microg/m3)_lag_1\PM2.5(microg/m3)_lag_1\NO2(microg/m3)_lag_0\PM10(microg/m3)_lag_1\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_0</Names>
   <Minimum>-1</Minimum>
   <Maximum>1</Maximum>
  </BarsChart>
  <Table Id="0iK2N1" Title="PM2.5(microg/m3)_ahead_1 correlations table">
   <Caption Id="2Ikx1X">The following table shows the value of all the correlations between input and target variables.
</Caption>
   <Data>power\0.884050
power\0.701394
power\0.638466
exponential\0.482375
power\0.473253
linear\0.391011
linear\0.328659
exponential\0.314347
linear\0.280244
exponential\0.257349
exponential\0.247166
linear\0.244286
linear\0.225909
exponential\0.198383
exponential\0.179579
exponential\0.161389
linear\0.151239
linear\0.148567
exponential\0.143016
exponential\0.134852
exponential\0.111849
exponential\0.079461
exponential\0.078373
exponential\0.064704
power\0.061330
power\0.047790
linear\-0.023706
power\-0.042847
linear\-0.063356
exponential\-0.063708
power\-0.087214
power\-0.103464
power\-0.110062
exponential\-0.129826
exponential\-0.146498
logarithmic\-0.207574
logarithmic\-0.290735
exponential\-0.295388
exponential\-0.444198
power\-0.594266</Data>
   <RowsName>PM10(microg/m3)_lag_0\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_1\NO2(microg/m3)_lag_0\PM2.5(microg/m3)_lag_1\NO2(microg/m3)_lag_1\PRESSURE(hPa)_lag_0\PRESSURE(hPa)_ahead_1\PRESSURE(hPa)_lag_1\TMAX(C)_ahead_1\TMAX(C)_lag_0\SO2(microg/m3)_lag_0\SO2(microg/m3)_lag_1\TMAX(C)_lag_1\TAVG(C)_ahead_1\TAVG(C)_lag_0\MONTH_lag_1\MONTH_lag_0\MONTH_ahead_1\TAVG(C)_lag_1\TMIN(C)_ahead_1\TMIN(C)_lag_0\TMIN(C)_lag_1\DAY_ahead_1\DAY_lag_0\DAY_lag_1\WEEKDAY_ahead_1\WEEKDAY_lag_1\WEEKDAY_lag_0\PRECIPITATIONS(mm)_ahead_1\HUMIDITY(percentage)_ahead_1\HUMIDITY(percentage)_lag_1\HUMIDITY(percentage)_lag_0\PRECIPITATIONS(mm)_lag_1\PRECIPITATIONS(mm)_lag_0\O3(microg/m3)_lag_1\O3(microg/m3)_lag_0\WINDSPEED(km/h)_lag_1\WINDSPEED(km/h)_lag_0\WINDSPEED(km/h)_ahead_1</RowsName>
   <ColumnsName>type\PM2.5(microg/m3)_ahead_1</ColumnsName>
   <RowHeadingsWidth>42</RowHeadingsWidth>
   <ColumnHeadingsWidth>18</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <BarsChart Id="Eha5fC" Title="PM10(microg/m3)_ahead_1 correlations chart">
   <Caption Id="aVAkR8">The following chart illustrates the target 'PM10(microg/m3)_ahead_1' dependency with the 21 input columns with greatest correlation in the data set.
 </Caption>
   <Data>-0.406\-0.279\-0.275\-0.266\-0.255\-0.22\0.212\0.213\0.215\0.267\0.268\0.286\0.316\0.321\0.326\0.358\0.368\0.408\0.44\0.496\0.726</Data>
   <XTitle>Correlation</XTitle>
   <Names>WINDSPEED(km/h)_ahead_1\HUMIDITY(percentage)_ahead_1\WINDSPEED(km/h)_lag_0\HUMIDITY(percentage)_lag_0\HUMIDITY(percentage)_lag_1\PRECIPITATIONS(mm)_ahead_1\NO2(microg/m3)_lag_1\TMIN(C)_ahead_1\PRESSURE(hPa)_lag_0\PRESSURE(hPa)_ahead_1\TAVG(C)_lag_1\TAVG(C)_lag_0\TAVG(C)_ahead_1\TMAX(C)_lag_1\PM2.5(microg/m3)_lag_1\TMAX(C)_lag_0\NO2(microg/m3)_lag_0\TMAX(C)_ahead_1\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_1\PM10(microg/m3)_lag_0</Names>
   <Minimum>-1</Minimum>
   <Maximum>1</Maximum>
  </BarsChart>
  <Table Id="8xkyV5" Title="PM10(microg/m3)_ahead_1 correlations table">
   <Caption Id="PBY4fV">The following table shows the value of all the correlations between input and target variables.
</Caption>
   <Data>power\0.725863
power\0.495633
power\0.439650
exponential\0.407742
exponential\0.368115
exponential\0.357771
power\0.326074
exponential\0.320507
exponential\0.316418
exponential\0.286361
exponential\0.268006
power\0.267183
power\0.215265
exponential\0.212670
exponential\0.212056
exponential\0.204962
exponential\0.204678
exponential\0.178802
exponential\0.168642
power\0.162249
power\0.159308
power\0.158858
power\0.155218
exponential\0.138883
exponential\0.125031
power\0.048117
power\0.035742
power\0.035305
power\-0.022642
power\-0.065342
exponential\-0.110706
exponential\-0.161729
exponential\-0.180959
exponential\-0.207226
exponential\-0.219803
power\-0.254813
power\-0.265621
exponential\-0.275025
power\-0.278981
exponential\-0.406021</Data>
   <RowsName>PM10(microg/m3)_lag_0\PM10(microg/m3)_lag_1\PM2.5(microg/m3)_lag_0\TMAX(C)_ahead_1\NO2(microg/m3)_lag_0\TMAX(C)_lag_0\PM2.5(microg/m3)_lag_1\TMAX(C)_lag_1\TAVG(C)_ahead_1\TAVG(C)_lag_0\TAVG(C)_lag_1\PRESSURE(hPa)_ahead_1\PRESSURE(hPa)_lag_0\TMIN(C)_ahead_1\NO2(microg/m3)_lag_1\TMIN(C)_lag_1\TMIN(C)_lag_0\SO2(microg/m3)_lag_0\O3(microg/m3)_lag_1\MONTH_lag_1\MONTH_lag_0\MONTH_ahead_1\PRESSURE(hPa)_lag_1\SO2(microg/m3)_lag_1\O3(microg/m3)_lag_0\DAY_ahead_1\DAY_lag_1\DAY_lag_0\WEEKDAY_lag_1\WEEKDAY_lag_0\WEEKDAY_ahead_1\PRECIPITATIONS(mm)_lag_1\PRECIPITATIONS(mm)_lag_0\WINDSPEED(km/h)_lag_1\PRECIPITATIONS(mm)_ahead_1\HUMIDITY(percentage)_lag_1\HUMIDITY(percentage)_lag_0\WINDSPEED(km/h)_lag_0\HUMIDITY(percentage)_ahead_1\WINDSPEED(km/h)_ahead_1</RowsName>
   <ColumnsName>type\PM10(microg/m3)_ahead_1</ColumnsName>
   <RowHeadingsWidth>42</RowHeadingsWidth>
   <ColumnHeadingsWidth>17</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <BarsChart Id="T6BSye" Title="O3(microg/m3)_ahead_1 correlations chart">
   <Caption Id="RBGGeg">The following chart illustrates the target 'O3(microg/m3)_ahead_1' dependency with the 21 input columns with greatest correlation in the data set.
 </Caption>
   <Data>-0.619\-0.606\-0.603\-0.474\-0.443\-0.418\-0.401\-0.356\-0.254\0.273\0.59\0.597\0.612\0.632\0.642\0.643\0.654\0.654\0.655\0.67\0.764</Data>
   <XTitle>Correlation</XTitle>
   <Names>HUMIDITY(percentage)_ahead_1\HUMIDITY(percentage)_lag_0\HUMIDITY(percentage)_lag_1\PRESSURE(hPa)_ahead_1\PRESSURE(hPa)_lag_0\PRESSURE(hPa)_lag_1\NO2(microg/m3)_lag_0\NO2(microg/m3)_lag_1\SO2(microg/m3)_lag_0\WINDSPEED(km/h)_ahead_1\TMIN(C)_lag_1\TMIN(C)_lag_0\TMIN(C)_ahead_1\TMAX(C)_lag_1\TMAX(C)_lag_0\TAVG(C)_lag_1\TAVG(C)_lag_0\O3(microg/m3)_lag_1\TMAX(C)_ahead_1\TAVG(C)_ahead_1\O3(microg/m3)_lag_0</Names>
   <Minimum>-1</Minimum>
   <Maximum>1</Maximum>
  </BarsChart>
  <Table Id="XUqlqS" Title="O3(microg/m3)_ahead_1 correlations table">
   <Caption Id="AAdmZc">The following table shows the value of all the correlations between input and target variables.
</Caption>
   <Data>power\0.763971
linear\0.670267
linear\0.655462
power\0.653745
linear\0.653510
linear\0.642726
linear\0.642177
linear\0.632111
linear\0.611553
linear\0.597147
linear\0.589647
power\0.272807
power\0.233245
power\0.220838
logarithmic\0.156707
logarithmic\0.152664
logarithmic\0.145069
logarithmic\0.113985
logarithmic\0.087407
exponential\0.058956
linear\0.043088
linear\0.042684
power\0.029812
exponential\0.021551
exponential\-0.030638
linear\-0.048444
linear\-0.051349
linear\-0.070297
exponential\-0.109033
exponential\-0.143218
exponential\-0.221753
exponential\-0.253815
exponential\-0.355592
exponential\-0.400988
exponential\-0.418248
exponential\-0.442719
exponential\-0.473887
logarithmic\-0.602630
logarithmic\-0.605722
logarithmic\-0.618606</Data>
   <RowsName>O3(microg/m3)_lag_0\TAVG(C)_ahead_1\TMAX(C)_ahead_1\O3(microg/m3)_lag_1\TAVG(C)_lag_0\TAVG(C)_lag_1\TMAX(C)_lag_0\TMAX(C)_lag_1\TMIN(C)_ahead_1\TMIN(C)_lag_0\TMIN(C)_lag_1\WINDSPEED(km/h)_ahead_1\WINDSPEED(km/h)_lag_0\WINDSPEED(km/h)_lag_1\MONTH_lag_0\MONTH_ahead_1\MONTH_lag_1\PM10(microg/m3)_lag_1\PM10(microg/m3)_lag_0\WEEKDAY_ahead_1\DAY_lag_0\DAY_lag_1\WEEKDAY_lag_0\DAY_ahead_1\WEEKDAY_lag_1\PRECIPITATIONS(mm)_lag_1\PRECIPITATIONS(mm)_lag_0\PRECIPITATIONS(mm)_ahead_1\PM2.5(microg/m3)_lag_1\PM2.5(microg/m3)_lag_0\SO2(microg/m3)_lag_1\SO2(microg/m3)_lag_0\NO2(microg/m3)_lag_1\NO2(microg/m3)_lag_0\PRESSURE(hPa)_lag_1\PRESSURE(hPa)_lag_0\PRESSURE(hPa)_ahead_1\HUMIDITY(percentage)_lag_1\HUMIDITY(percentage)_lag_0\HUMIDITY(percentage)_ahead_1</RowsName>
   <ColumnsName>type\O3(microg/m3)_ahead_1</ColumnsName>
   <RowHeadingsWidth>42</RowHeadingsWidth>
   <ColumnHeadingsWidth>15</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <BarsChart Id="4g2S3t" Title="NO2(microg/m3)_ahead_1 correlations chart">
   <Caption Id="aaB6UY">The following chart illustrates the target 'NO2(microg/m3)_ahead_1' dependency with the 21 input columns with greatest correlation in the data set.
 </Caption>
   <Data>-0.374\-0.371\-0.352\-0.335\-0.33\-0.318\-0.308\-0.305\-0.289\-0.255\-0.227\-0.227\0.218\0.259\0.337\0.348\0.401\0.409\0.472\0.528\0.734</Data>
   <XTitle>Correlation</XTitle>
   <Names>O3(microg/m3)_lag_0\TMIN(C)_ahead_1\TMIN(C)_lag_0\WINDSPEED(km/h)_ahead_1\TMIN(C)_lag_1\O3(microg/m3)_lag_1\TAVG(C)_lag_1\TAVG(C)_lag_0\TAVG(C)_ahead_1\TMAX(C)_lag_1\WINDSPEED(km/h)_lag_0\TMAX(C)_lag_0\HUMIDITY(percentage)_lag_1\PM10(microg/m3)_lag_0\SO2(microg/m3)_lag_1\PRESSURE(hPa)_lag_1\SO2(microg/m3)_lag_0\PRESSURE(hPa)_lag_0\PRESSURE(hPa)_ahead_1\NO2(microg/m3)_lag_1\NO2(microg/m3)_lag_0</Names>
   <Minimum>-1</Minimum>
   <Maximum>1</Maximum>
  </BarsChart>
  <Table Id="nxy84I" Title="NO2(microg/m3)_ahead_1 correlations table">
   <Caption Id="cBEBMe">The following table shows the value of all the correlations between input and target variables.
</Caption>
   <Data>linear\0.733535
linear\0.527994
linear\0.471671
linear\0.408950
linear\0.401349
linear\0.347654
linear\0.337204
logarithmic\0.259462
logarithmic\0.217933
logarithmic\0.206780
logarithmic\0.183804
linear\0.165709
logarithmic\0.114882
linear\0.110479
linear\0.102207
linear\0.096830
linear\0.093842
power\0.028353
power\0.012529
power\0.012408
logarithmic\-0.039118
linear\-0.051428
linear\-0.092692
power\-0.119813
linear\-0.138045
logarithmic\-0.167940
linear\-0.180324
exponential\-0.185480
logarithmic\-0.226863
linear\-0.227149
logarithmic\-0.255439
linear\-0.288982
linear\-0.305379
linear\-0.307715
logarithmic\-0.318479
linear\-0.329514
logarithmic\-0.335077
linear\-0.351708
linear\-0.370644
logarithmic\-0.374116</Data>
   <RowsName>NO2(microg/m3)_lag_0\NO2(microg/m3)_lag_1\PRESSURE(hPa)_ahead_1\PRESSURE(hPa)_lag_0\SO2(microg/m3)_lag_0\PRESSURE(hPa)_lag_1\SO2(microg/m3)_lag_1\PM10(microg/m3)_lag_0\HUMIDITY(percentage)_lag_1\HUMIDITY(percentage)_lag_0\HUMIDITY(percentage)_ahead_1\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_1\MONTH_lag_1\MONTH_lag_0\PM2.5(microg/m3)_lag_1\MONTH_ahead_1\DAY_ahead_1\DAY_lag_0\DAY_lag_1\WEEKDAY_lag_1\PRECIPITATIONS(mm)_lag_1\PRECIPITATIONS(mm)_lag_0\WEEKDAY_lag_0\PRECIPITATIONS(mm)_ahead_1\WINDSPEED(km/h)_lag_1\TMAX(C)_ahead_1\WEEKDAY_ahead_1\WINDSPEED(km/h)_lag_0\TMAX(C)_lag_0\TMAX(C)_lag_1\TAVG(C)_ahead_1\TAVG(C)_lag_0\TAVG(C)_lag_1\O3(microg/m3)_lag_1\TMIN(C)_lag_1\WINDSPEED(km/h)_ahead_1\TMIN(C)_lag_0\TMIN(C)_ahead_1\O3(microg/m3)_lag_0</RowsName>
   <ColumnsName>type\NO2(microg/m3)_ahead_1</ColumnsName>
   <RowHeadingsWidth>42</RowHeadingsWidth>
   <ColumnHeadingsWidth>16</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <BarsChart Id="oVZ6cV" Title="SO2(microg/m3)_ahead_1 correlations chart">
   <Caption Id="uO9Vda">The following chart illustrates the target 'SO2(microg/m3)_ahead_1' dependency with the 21 input columns with greatest correlation in the data set.
 </Caption>
   <Data>-0.252\-0.209\-0.197\-0.181\-0.167\-0.158\-0.156\-0.152\-0.131\-0.111\-0.109\-0.0966\0.126\0.139\0.198\0.241\0.274\0.302\0.383\0.764\0.838</Data>
   <XTitle>Correlation</XTitle>
   <Names>O3(microg/m3)_lag_0\O3(microg/m3)_lag_1\TMIN(C)_ahead_1\TMIN(C)_lag_0\TMIN(C)_lag_1\TAVG(C)_lag_0\TAVG(C)_ahead_1\TAVG(C)_lag_1\TMAX(C)_lag_1\TMAX(C)_lag_0\MONTH_ahead_1\MONTH_lag_0\PM10(microg/m3)_lag_0\PM2.5(microg/m3)_lag_0\PRESSURE(hPa)_lag_1\PRESSURE(hPa)_lag_0\PRESSURE(hPa)_ahead_1\NO2(microg/m3)_lag_1\NO2(microg/m3)_lag_0\SO2(microg/m3)_lag_1\SO2(microg/m3)_lag_0</Names>
   <Minimum>-1</Minimum>
   <Maximum>1</Maximum>
  </BarsChart>
  <Table Id="O9TIao" Title="SO2(microg/m3)_ahead_1 correlations table">
   <Caption Id="5HGG56">The following table shows the value of all the correlations between input and target variables.
</Caption>
   <Data>linear\0.837991
linear\0.763773
linear\0.382837
linear\0.302185
linear\0.273613
linear\0.240839
linear\0.198319
linear\0.138790
logarithmic\0.125745
linear\0.094585
linear\0.072930
logarithmic\0.066331
linear\0.063662
linear\0.052884
linear\0.010630
linear\-0.009855
linear\-0.015696
linear\-0.017661
logarithmic\-0.021660
logarithmic\-0.021933
linear\-0.031239
linear\-0.033140
logarithmic\-0.048085
linear\-0.062272
logarithmic\-0.080146
linear\-0.085062
logarithmic\-0.088027
linear\-0.092820
logarithmic\-0.096587
logarithmic\-0.109100
linear\-0.111239
logarithmic\-0.130658
linear\-0.152422
linear\-0.155828
linear\-0.157577
linear\-0.167114
linear\-0.181436
linear\-0.197378
logarithmic\-0.208831
logarithmic\-0.251860</Data>
   <RowsName>SO2(microg/m3)_lag_0\SO2(microg/m3)_lag_1\NO2(microg/m3)_lag_0\NO2(microg/m3)_lag_1\PRESSURE(hPa)_ahead_1\PRESSURE(hPa)_lag_0\PRESSURE(hPa)_lag_1\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_0\PM2.5(microg/m3)_lag_1\HUMIDITY(percentage)_lag_1\PM10(microg/m3)_lag_1\HUMIDITY(percentage)_lag_0\HUMIDITY(percentage)_ahead_1\WEEKDAY_lag_1\DAY_lag_1\DAY_lag_0\DAY_ahead_1\WINDSPEED(km/h)_lag_1\WEEKDAY_lag_0\PRECIPITATIONS(mm)_lag_1\WEEKDAY_ahead_1\WINDSPEED(km/h)_lag_0\PRECIPITATIONS(mm)_lag_0\MONTH_lag_1\PRECIPITATIONS(mm)_ahead_1\WINDSPEED(km/h)_ahead_1\TMAX(C)_ahead_1\MONTH_lag_0\MONTH_ahead_1\TMAX(C)_lag_0\TMAX(C)_lag_1\TAVG(C)_lag_1\TAVG(C)_ahead_1\TAVG(C)_lag_0\TMIN(C)_lag_1\TMIN(C)_lag_0\TMIN(C)_ahead_1\O3(microg/m3)_lag_1\O3(microg/m3)_lag_0</RowsName>
   <ColumnsName>type\SO2(microg/m3)_ahead_1</ColumnsName>
   <RowHeadingsWidth>42</RowHeadingsWidth>
   <ColumnHeadingsWidth>16</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Ww2xSU" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="PScmFV" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="rRUrmA" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="sfO1Z8" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="a4EzpG" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="GMOniu" Title="Quasi-Newton method errors history">
   <Caption Id="HRfPBw">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.08795, and the final value after 201 epochs is 0.299558.
The initial value of the selection error is 0.729431, and the final value after 201 epochs is 0.272176.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201</X2Data>
   <Y1Data>1.09\0.777\0.638\0.571\0.515\0.48\0.442\0.421\0.404\0.391\0.379\0.369\0.364\0.356\0.35\0.347\0.343\0.339\0.337\0.334\0.332\0.33\0.329\0.327\0.326\0.325\0.323\0.322\0.321\0.321\0.32\0.318\0.318\0.317\0.316\0.316\0.315\0.314\0.314\0.313\0.312\0.312\0.311\0.311\0.311\0.31\0.31\0.31\0.309\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.729\0.677\0.633\0.531\0.434\0.389\0.388\0.38\0.367\0.359\0.346\0.332\0.33\0.312\0.3\0.299\0.299\0.295\0.286\0.282\0.277\0.272\0.278\0.276\0.277\0.275\0.276\0.273\0.275\0.274\0.275\0.269\0.267\0.268\0.267\0.268\0.269\0.269\0.27\0.271\0.271\0.269\0.268\0.267\0.267\0.268\0.269\0.27\0.27\0.269\0.271\0.27\0.272\0.27\0.27\0.27\0.271\0.27\0.269\0.27\0.27\0.272\0.272\0.271\0.272\0.272\0.272\0.273\0.273\0.273\0.274\0.274\0.275\0.276\0.276\0.275\0.275\0.274\0.274\0.275\0.275\0.275\0.274\0.274\0.273\0.274\0.275\0.276\0.276\0.275\0.274\0.274\0.274\0.274\0.275\0.275\0.275\0.274\0.274\0.274\0.275\0.275\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.273\0.273\0.273\0.274\0.273\0.273\0.273\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.273\0.273\0.273\0.273\0.273\0.273\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272\0.272</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>202</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="wx4MvL" Title="Quasi-Newton method results">
   <Caption Id="qTe38q">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.3
0.272
201
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="L95pVe" Title="Order selection" Component="Model selection" Name="Perform neurons selection">
  <Text Id="AGAb6W" Title="Task description">The best selection is achieved by using a model whose complexity is the most appropriate to produce an adequate data fit.
The neurons selection algorithm is responsible for finding the optimal number of neurons in the network. </Text>
  <Text Id="O47MlI" Title="Neurons selection algorithm">The growing neurons algorithm is in charge of selecting the optimal number of neurons in the network.
</Text>
  <DoubleLineChart Id="1ZN4Th" Title="Growing neurons training/selection errors plot">
   <Caption Id="nPe56s">The following chart shows the error history for the different subsets during the growing neurons selection process.
The blue line represents the training error, and the yellow line symbolizes the selection error. </Caption>
   <X1Data>1\2\3\4\5\6\7\8\9\10</X1Data>
   <X2Data>1\2\3\4\5\6\7\8\9\10</X2Data>
   <Y1Data>0.657478\0.451985\0.364599\0.327304\0.31466\0.310582\0.296362\0.305656\0.292578\0.294554</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.568265\0.448852\0.36144\0.281022\0.286742\0.274582\0.255357\0.273106\0.250609\0.26636</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Neurons number</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>1</XMinimum>
   <XMaximum>10</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="FZEsnt" Title="Growing neurons results">
   <Caption Id="4b67FK">The following table shows the neurons selection results by the growing neurons algorithm.
They include some final states from the neural network, the loss index, and the neurons selection algorithm. </Caption>
   <Data>9
0.292578
0.250609
10
MaximumNeurons
00:00:16
</Data>
   <RowsName>Optimal order\Optimum training error\Optimum selection error\Epochs number\Stopping criterion\Elapsed time</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>17</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <NeuralNetworkGraph Id="VJFqqm" Title="Network architecture">
   <Caption Id="NmMmdp">The next figure depicts a graphical representation of the network architecture.
It contains the following layers:
-Scaling layer with 40 neurons (yellow).
-Perceptron layer with 9 neurons (blue).
-Perceptron layer with 5 neurons (blue).
-Unscaling layer with 5 neurons (red).
-Bounding layer with 5 neurons (purple).
</Caption>
   <ProjectType>forecasting</ProjectType>
   <InputsName>DAY_lag_1\MONTH_lag_1\WEEKDAY_lag_1\PM2.5(microg/m3)_lag_1\PM10(microg/m3)_lag_1\O3(microg/m3)_lag_1\NO2(microg/m3)_lag_1\SO2(microg/m3)_lag_1\PRECIPITATIONS(mm)_lag_1\TAVG(C)_lag_1\TMAX(C)_lag_1\TMIN(C)_lag_1\PRESSURE(hPa)_lag_1\WINDSPEED(km/h)_lag_1\HUMIDITY(percentage)_lag_1\DAY_lag_0\MONTH_lag_0\WEEKDAY_lag_0\PM2.5(microg/m3)_lag_0\PM10(microg/m3)_lag_0\O3(microg/m3)_lag_0\NO2(microg/m3)_lag_0\SO2(microg/m3)_lag_0\PRECIPITATIONS(mm)_lag_0\TAVG(C)_lag_0\TMAX(C)_lag_0\TMIN(C)_lag_0\PRESSURE(hPa)_lag_0\WINDSPEED(km/h)_lag_0\HUMIDITY(percentage)_lag_0\DAY_ahead_1\MONTH_ahead_1\WEEKDAY_ahead_1\PRECIPITATIONS(mm)_ahead_1\TAVG(C)_ahead_1\TMAX(C)_ahead_1\TMIN(C)_ahead_1\PRESSURE(hPa)_ahead_1\WINDSPEED(km/h)_ahead_1\HUMIDITY(percentage)_ahead_1</InputsName>
   <OutputsName>PM2.5(microg/m3)_ahead_1\PM10(microg/m3)_ahead_1\O3(microg/m3)_ahead_1\NO2(microg/m3)_ahead_1\SO2(microg/m3)_ahead_1</OutputsName>
   <LayersName>scaling_layer\perceptron_layer_1\perceptron_layer_2\unscaling_layer\bounding_layer</LayersName>
   <Architecture>40\9\5\5\5</Architecture>
  </NeuralNetworkGraph>
 </Task>
 <Task Id="oADKha" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="KDT4eQ" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="9AGMTA" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="mk3gG3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00862503\114.817\6.03421\8.15069
5.52887e-5\0.736007\0.0386809\0.052248
0.00552887\73.6007\3.86809\5.2248</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="86q4ZY" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="VJcjRm">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0145378\305.993\6.67531\14.0279
4.27583e-5\0.899981\0.0196333\0.0412585
0.00427583\89.9981\1.96333\4.12585</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4Kc0wo" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="hfdMfY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.0198059\39.2601\4.86425\4.45408
7.98625e-5\0.158307\0.0196139\0.01796
0.00798625\15.8307\1.96139\1.796</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="f7fKnh" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="NZ7eFL">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.00765228\40.8176\5.2827\4.27154
0.000106282\0.566911\0.0733709\0.059327
0.0106282\56.6911\7.33709\5.93269</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="JvJmbW" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="8TtgZD">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391694 and its percentage error 3.91697</Caption>
   <Data>0.000455379\14.2763\0.738322\1.01889
2.6787e-5\0.839784\0.0434307\0.0599347
0.0026787\83.9784\4.34307\5.99347</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="hkGKDK" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="ay2QNS" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="LSQw6k" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="lYHyk4" Title="Quasi-Newton method errors history">
   <Caption Id="K07gkR">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 3.80408, and the final value after 661 epochs is 0.745805.
The initial value of the selection error is 3.64798, and the final value after 661 epochs is 0.608229.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661</X2Data>
   <Y1Data>3.8\1.65\1.06\1.04\1.03\0.926\0.925\0.928\0.896\0.825\0.818\0.814\0.806\0.821\0.829\0.825\0.824\0.822\0.826\0.83\0.837\0.842\0.846\0.848\0.848\0.85\0.851\0.852\0.853\0.855\0.856\0.857\0.858\0.859\0.861\0.862\0.863\0.864\0.865\0.865\0.866\0.867\0.867\0.867\0.867\0.867\0.867\0.867\0.867\0.867\0.867\0.866\0.866\0.866\0.866\0.866\0.866\0.866\0.866\0.866\0.866\0.866\0.866\0.866\0.867\0.867\0.867\0.867\0.867\0.868\0.868\0.868\0.868\0.869\0.869\0.869\0.869\0.869\0.87\0.87\0.87\0.87\0.87\0.87\0.87\0.87\0.87\0.87\0.87\0.87\0.87\0.87\0.87\0.869\0.869\0.869\0.869\0.868\0.868\0.868\0.868\0.868\0.867\0.867\0.867\0.867\0.866\0.866\0.866\0.866\0.865\0.865\0.865\0.864\0.864\0.864\0.864\0.863\0.863\0.863\0.863\0.862\0.862\0.862\0.862\0.862\0.861\0.861\0.861\0.861\0.86\0.86\0.86\0.859\0.859\0.859\0.858\0.858\0.857\0.857\0.856\0.856\0.856\0.855\0.855\0.854\0.854\0.853\0.853\0.852\0.852\0.851\0.851\0.85\0.849\0.849\0.848\0.848\0.847\0.847\0.846\0.846\0.846\0.845\0.845\0.844\0.844\0.843\0.843\0.842\0.842\0.841\0.841\0.84\0.84\0.839\0.839\0.838\0.838\0.838\0.837\0.837\0.837\0.836\0.836\0.836\0.835\0.835\0.835\0.835\0.835\0.834\0.834\0.834\0.833\0.833\0.833\0.832\0.832\0.832\0.832\0.831\0.831\0.831\0.831\0.83\0.83\0.83\0.829\0.829\0.828\0.828\0.827\0.826\0.825\0.824\0.823\0.822\0.82\0.819\0.818\0.816\0.815\0.814\0.812\0.811\0.81\0.809\0.807\0.805\0.804\0.802\0.801\0.8\0.799\0.798\0.798\0.797\0.797\0.796\0.796\0.795\0.795\0.795\0.795\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.793\0.793\0.792\0.792\0.792\0.791\0.79\0.79\0.789\0.788\0.788\0.787\0.787\0.786\0.786\0.786\0.785\0.785\0.784\0.784\0.784\0.783\0.783\0.782\0.782\0.782\0.781\0.781\0.781\0.78\0.78\0.78\0.78\0.78\0.779\0.779\0.779\0.779\0.779\0.778\0.778\0.778\0.777\0.777\0.777\0.776\0.776\0.776\0.775\0.775\0.775\0.774\0.774\0.774\0.773\0.773\0.772\0.772\0.772\0.771\0.771\0.771\0.77\0.77\0.77\0.769\0.769\0.769\0.769\0.769\0.769\0.769\0.769\0.769\0.769\0.768\0.768\0.768\0.768\0.769\0.769\0.769\0.769\0.769\0.769\0.769\0.769\0.769\0.769\0.769\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.768\0.767\0.767\0.767\0.767\0.767\0.767\0.767\0.766\0.766\0.766\0.766\0.765\0.765\0.765\0.765\0.764\0.764\0.764\0.764\0.763\0.763\0.763\0.763\0.763\0.762\0.762\0.762\0.762\0.761\0.761\0.76\0.759\0.759\0.758\0.757\0.756\0.756\0.755\0.755\0.754\0.754\0.753\0.753\0.753\0.753\0.753\0.752\0.752\0.752\0.752\0.752\0.752\0.751\0.751\0.751\0.751\0.751\0.751\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.752\0.751\0.751\0.751\0.751\0.75\0.75\0.75\0.75\0.75\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.749\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>3.65\1.58\0.957\0.922\0.903\0.827\0.767\0.751\0.742\0.684\0.681\0.658\0.639\0.65\0.652\0.643\0.642\0.643\0.646\0.651\0.656\0.658\0.66\0.66\0.66\0.661\0.661\0.662\0.663\0.664\0.665\0.666\0.667\0.668\0.669\0.67\0.671\0.671\0.672\0.673\0.673\0.673\0.673\0.673\0.673\0.673\0.674\0.674\0.674\0.673\0.673\0.673\0.673\0.673\0.673\0.673\0.673\0.673\0.673\0.674\0.674\0.674\0.674\0.674\0.674\0.675\0.675\0.675\0.675\0.676\0.676\0.676\0.676\0.676\0.676\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.678\0.678\0.678\0.678\0.678\0.678\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.676\0.676\0.676\0.676\0.676\0.676\0.676\0.676\0.676\0.676\0.676\0.676\0.676\0.675\0.675\0.675\0.675\0.675\0.675\0.675\0.675\0.675\0.675\0.675\0.675\0.675\0.675\0.674\0.674\0.674\0.674\0.674\0.674\0.673\0.673\0.673\0.673\0.673\0.673\0.672\0.672\0.672\0.672\0.672\0.671\0.671\0.671\0.671\0.67\0.67\0.67\0.67\0.67\0.67\0.669\0.669\0.669\0.669\0.669\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.667\0.667\0.667\0.667\0.667\0.667\0.667\0.667\0.667\0.667\0.667\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.664\0.664\0.664\0.664\0.664\0.663\0.663\0.662\0.662\0.661\0.661\0.66\0.66\0.659\0.659\0.658\0.657\0.657\0.657\0.656\0.655\0.655\0.654\0.653\0.653\0.652\0.652\0.651\0.651\0.651\0.651\0.65\0.65\0.65\0.65\0.65\0.65\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.648\0.648\0.648\0.647\0.647\0.647\0.646\0.646\0.646\0.646\0.645\0.645\0.645\0.645\0.644\0.644\0.644\0.644\0.644\0.643\0.643\0.643\0.643\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.641\0.641\0.641\0.641\0.641\0.641\0.64\0.64\0.64\0.64\0.64\0.639\0.639\0.639\0.639\0.639\0.638\0.638\0.638\0.638\0.637\0.637\0.637\0.637\0.636\0.636\0.636\0.636\0.636\0.636\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.632\0.632\0.632\0.632\0.632\0.632\0.631\0.631\0.631\0.631\0.631\0.63\0.63\0.63\0.63\0.63\0.629\0.629\0.629\0.629\0.629\0.629\0.628\0.628\0.628\0.627\0.627\0.626\0.625\0.624\0.623\0.623\0.622\0.621\0.62\0.62\0.619\0.619\0.619\0.618\0.618\0.617\0.617\0.617\0.617\0.617\0.616\0.616\0.616\0.615\0.615\0.615\0.615\0.615\0.615\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.613\0.613\0.613\0.613\0.613\0.612\0.612\0.611\0.611\0.611\0.611\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>662</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>4</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="w61x9y" Title="Quasi-Newton method results">
   <Caption Id="SFNbxb">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.746
0.608
661
00:00:26
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="evok0d" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="w7v26j" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="L4EYuc" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="G3YJR4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0548706\101.206\10.3338\10.7438
0.000351735\0.648757\0.0662423\0.0688703
0.0351735\64.8757\6.62423\6.88703</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="EP7fcq" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="layUgd">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0163727\313.047\7.83895\14.6142
4.81549e-5\0.920726\0.0230557\0.0429829
0.00481549\92.0726\2.30557\4.29829</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="FZQXXm" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="MNNhhn">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00255585\26.355\5.30168\4.60952
1.03058e-5\0.10627\0.0213778\0.0185868
0.00103058\10.627\2.13778\1.85868</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="SFGjPE" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="xxoNWz">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0181446\41.3247\8.10036\5.00257
0.000252008\0.573954\0.112505\0.0694801
0.0252008\57.3954\11.2505\6.94801</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="DBJrbB" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="GcZ2rf">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.002285\15.3739\0.853524\0.942475
0.000134412\0.904347\0.0502073\0.0554397
0.0134412\90.4347\5.02073\5.54397</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="WGakKa" Title="PM2.5(microg/m3)_ahead_2 errors statistics">
   <Caption Id="ru9pUi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0474091\100.586\12.3665\12.3204
0.000303904\0.644782\0.0792724\0.0789771
0.0303904\64.4782\7.92724\7.89771</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Kcf710" Title="PM10(microg/m3)_ahead_2 errors statistics">
   <Caption Id="dZJqx9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00247574\321.29\9.50839\15.2374
7.28158e-6\0.944972\0.0279659\0.0448157
0.000728158\94.4972\2.79659\4.48157</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="VQuEUh" Title="O3(microg/m3)_ahead_2 errors statistics">
   <Caption Id="KLKhh9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00662994\25.6083\5.26606\4.54322
2.67336e-5\0.103259\0.0212341\0.0183195
0.00267336\10.3259\2.12341\1.83195</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="XHEzA2" Title="NO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="8afU55">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0312214\38.9895\8.47305\5.26558
0.00043363\0.541521\0.117681\0.0731331
0.043363\54.1521\11.7681\7.31331</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="rz0xtz" Title="SO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="LVX6c6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00322056\15.3329\0.970917\0.984972
0.000189445\0.901938\0.0571128\0.0579396
0.0189445\90.1938\5.71128\5.79396</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="GQHovj" Title="PM2.5(microg/m3)_ahead_3 errors statistics">
   <Caption Id="ps92f3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0192261\115.348\15.202\13.5735
0.000123244\0.739408\0.0974486\0.0870098
0.0123244\73.9408\9.74486\8.70098</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="BTW0Za" Title="PM10(microg/m3)_ahead_3 errors statistics">
   <Caption Id="dHKynF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0543804\319.396\10.067\15.2923
0.000159942\0.9394\0.0296089\0.0449772
0.0159942\93.94\2.96089\4.49772</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="iT5LA2" Title="O3(microg/m3)_ahead_3 errors statistics">
   <Caption Id="WSNpb6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0122509\25.3681\5.17973\4.35349
4.93988e-5\0.102291\0.020886\0.0175544
0.00493988\10.2291\2.0886\1.75544</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="K2hN1q" Title="NO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="QQNhci">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00984192\37.2808\8.34228\5.19716
0.000136693\0.517789\0.115865\0.0721828
0.0136693\51.7789\11.5865\7.21828</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="QL8rta" Title="SO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="o9Iper">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00287724\15.2384\1.01766\0.994265
0.000169249\0.896375\0.0598623\0.0584862
0.0169249\89.6376\5.98623\5.84862</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="K0v4cB" Title="PM2.5(microg/m3)_ahead_4 errors statistics">
   <Caption Id="Wb3fBc">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0941925\110.337\15.8567\13.6152
0.000603798\0.707285\0.101645\0.0872772
0.0603798\70.7285\10.1645\8.72772</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="MICDxg" Title="PM10(microg/m3)_ahead_4 errors statistics">
   <Caption Id="12tOFO">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0414066\320.092\9.81415\15.2361
0.000121784\0.941448\0.0288651\0.0448122
0.0121784\94.1448\2.88651\4.48122</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="dMVwwp" Title="O3(microg/m3)_ahead_4 errors statistics">
   <Caption Id="M8xkDI">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00865555\24.9851\5.2643\4.36318
3.49014e-5\0.100747\0.021227\0.0175935
0.00349014\10.0747\2.1227\1.75935</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="1VjydB" Title="NO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="peXm2Q">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0232697\36.3114\8.08516\5.04071
0.00032319\0.504326\0.112294\0.0700098
0.032319\50.4325\11.2294\7.00098</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="8RRMPb" Title="SO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="6E2oy0">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0103774\15.1403\1.05622\0.984732
0.000610436\0.890607\0.0621308\0.0579254
0.0610436\89.0607\6.21308\5.79254</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="hPVYaS" Title="WINDSPEED(km/h)_ahead_4 errors statistics">
   <Caption Id="TaPxgm">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0134583\15.8921\3.30339\2.6539
0.000392369\0.463326\0.0963088\0.0773731
0.0392369\46.3326\9.63088\7.73731</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="a3leHN" Title="PM2.5(microg/m3)_ahead_5 errors statistics">
   <Caption Id="qJfrnB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0748253\113.879\15.2219\13.3919
0.000479649\0.729993\0.0975763\0.0858458
0.0479649\72.9993\9.75763\8.58458</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="91eYAv" Title="PM10(microg/m3)_ahead_5 errors statistics">
   <Caption Id="vjsGhI">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00639343\319.379\9.90012\15.2677
1.88042e-5\0.93935\0.029118\0.0449051
0.00188042\93.935\2.9118\4.49051</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="BlQvqy" Title="O3(microg/m3)_ahead_5 errors statistics">
   <Caption Id="MLWEnK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0137329\24.2368\5.33666\4.47207
5.53746e-5\0.0977292\0.0215188\0.0180326
0.00553746\9.77292\2.15188\1.80326</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="2oz1HJ" Title="NO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="c2aslO">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0437622\37.8309\8.41102\5.23922
0.000607808\0.525429\0.11682\0.072767
0.0607808\52.5429\11.682\7.2767</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="66keaF" Title="SO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="SwFlMQ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0046401\15.0643\1.0736\0.976866
0.000272947\0.886132\0.0631529\0.0574627
0.0272947\88.6132\6.31529\5.74627</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="B9aNS6" Title="PM2.5(microg/m3)_ahead_6 errors statistics">
   <Caption Id="OUV3iW">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0290642\111.497\15.6176\13.6994
0.000186309\0.714724\0.100113\0.0878166
0.0186309\71.4724\10.0113\8.78166</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="H525vf" Title="PM10(microg/m3)_ahead_6 errors statistics">
   <Caption Id="DawDRq">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.398573\319.405\10.0693\15.3142
0.00117227\0.939426\0.0296155\0.0450417
0.117227\93.9426\2.96155\4.50417</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="evEoL0" Title="O3(microg/m3)_ahead_6 errors statistics">
   <Caption Id="JH1mXO">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0306282\23.26\5.36904\4.4843
0.000123501\0.0937903\0.0216494\0.0180818
0.0123501\9.37903\2.16494\1.80818</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="XVa0AJ" Title="NO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="aXvVtW">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.237835\39.6054\8.6102\5.33477
0.00330326\0.550074\0.119586\0.074094
0.330326\55.0074\11.9586\7.4094</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Bz8pla" Title="SO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="B1DZJd">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.000660777\14.8634\1.08888\0.971793
3.88692e-5\0.874316\0.0640515\0.0571643
0.00388692\87.4316\6.40515\5.71643</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="SyriDO" Title="PM2.5(microg/m3)_ahead_7 errors statistics">
   <Caption Id="mBrG8L">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0478325\112.793\16.2568\13.9127
0.000306619\0.723029\0.10421\0.0891837
0.0306619\72.3029\10.421\8.91837</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="IR8GWH" Title="PM10(microg/m3)_ahead_7 errors statistics">
   <Caption Id="Zd6G55">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.394499\319.399\10.0933\15.3152
0.00116029\0.93941\0.0296862\0.0450446
0.116029\93.941\2.96862\4.50446</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="fVkte9" Title="O3(microg/m3)_ahead_7 errors statistics">
   <Caption Id="tvBAKY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00916481\23.068\5.48358\4.50835
3.69549e-5\0.093016\0.0221112\0.0181788
0.00369549\9.3016\2.21112\1.81788</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="wgrU0c" Title="NO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="P2PzG3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0495644\39.8547\8.51949\5.29557
0.000688394\0.553537\0.118326\0.0735495
0.0688394\55.3537\11.8326\7.35495</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="M3EnB1" Title="SO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="yEqg3H">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00113988\14.4095\1.10812\0.951308
6.70517e-5\0.847616\0.0651835\0.0559593
0.00670517\84.7616\6.51835\5.59593</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="6La25S" Title="PM2.5(microg/m3)_ahead_8 errors statistics">
   <Caption Id="ZE3Ydj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.400032\111.599\16.3987\13.9799
0.00256431\0.71538\0.10512\0.0896149
0.256431\71.538\10.512\8.96149</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="flNDmm" Title="PM10(microg/m3)_ahead_8 errors statistics">
   <Caption Id="FDo13x">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.395552\319.396\10.109\15.3178
0.00116339\0.939401\0.0297322\0.0450524
0.116339\93.9401\2.97322\4.50524</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="FW8uOp" Title="O3(microg/m3)_ahead_8 errors statistics">
   <Caption Id="h7zYt7">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0227776\24.7648\5.63152\4.64249
9.1845e-5\0.0998581\0.0227077\0.0187197
0.0091845\9.98581\2.27077\1.87197</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="lg9jlf" Title="NO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="qHUOi2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.136423\39.4832\8.61917\5.34031
0.00189477\0.548378\0.119711\0.0741709
0.189477\54.8378\11.9711\7.41709</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="SlHqCi" Title="SO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="QdnmIa">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00226974\14.7627\1.13878\0.961774
0.000133514\0.868393\0.0669873\0.0565749
0.0133514\86.8393\6.69873\5.6575</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="69KYaj" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="XgovXy" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 3816.</Text>
 </Task>
 <Task Id="2OHyAw" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="xt25A8" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="RSiEeq" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="F3fWw4" Title="Quasi-Newton method errors history">
   <Caption Id="SlYdnB">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.22969, and the final value after 1000 epochs is 0.853483.
The initial value of the selection error is 0.917712, and the final value after 1000 epochs is 0.696769.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661\662\663\664\665\666\667\668\669\670\671\672\673\674\675\676\677\678\679\680\681\682\683\684\685\686\687\688\689\690\691\692\693\694\695\696\697\698\699\700\701\702\703\704\705\706\707\708\709\710\711\712\713\714\715\716\717\718\719\720\721\722\723\724\725\726\727\728\729\730\731\732\733\734\735\736\737\738\739\740\741\742\743\744\745\746\747\748\749\750\751\752\753\754\755\756\757\758\759\760\761\762\763\764\765\766\767\768\769\770\771\772\773\774\775\776\777\778\779\780\781\782\783\784\785\786\787\788\789\790\791\792\793\794\795\796\797\798\799\800\801\802\803\804\805\806\807\808\809\810\811\812\813\814\815\816\817\818\819\820\821\822\823\824\825\826\827\828\829\830\831\832\833\834\835\836\837\838\839\840\841\842\843\844\845\846\847\848\849\850\851\852\853\854\855\856\857\858\859\860\861\862\863\864\865\866\867\868\869\870\871\872\873\874\875\876\877\878\879\880\881\882\883\884\885\886\887\888\889\890\891\892\893\894\895\896\897\898\899\900\901\902\903\904\905\906\907\908\909\910\911\912\913\914\915\916\917\918\919\920\921\922\923\924\925\926\927\928\929\930\931\932\933\934\935\936\937\938\939\940\941\942\943\944\945\946\947\948\949\950\951\952\953\954\955\956\957\958\959\960\961\962\963\964\965\966\967\968\969\970\971\972\973\974\975\976\977\978\979\980\981\982\983\984\985\986\987\988\989\990\991\992\993\994\995\996\997\998\999\1000</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661\662\663\664\665\666\667\668\669\670\671\672\673\674\675\676\677\678\679\680\681\682\683\684\685\686\687\688\689\690\691\692\693\694\695\696\697\698\699\700\701\702\703\704\705\706\707\708\709\710\711\712\713\714\715\716\717\718\719\720\721\722\723\724\725\726\727\728\729\730\731\732\733\734\735\736\737\738\739\740\741\742\743\744\745\746\747\748\749\750\751\752\753\754\755\756\757\758\759\760\761\762\763\764\765\766\767\768\769\770\771\772\773\774\775\776\777\778\779\780\781\782\783\784\785\786\787\788\789\790\791\792\793\794\795\796\797\798\799\800\801\802\803\804\805\806\807\808\809\810\811\812\813\814\815\816\817\818\819\820\821\822\823\824\825\826\827\828\829\830\831\832\833\834\835\836\837\838\839\840\841\842\843\844\845\846\847\848\849\850\851\852\853\854\855\856\857\858\859\860\861\862\863\864\865\866\867\868\869\870\871\872\873\874\875\876\877\878\879\880\881\882\883\884\885\886\887\888\889\890\891\892\893\894\895\896\897\898\899\900\901\902\903\904\905\906\907\908\909\910\911\912\913\914\915\916\917\918\919\920\921\922\923\924\925\926\927\928\929\930\931\932\933\934\935\936\937\938\939\940\941\942\943\944\945\946\947\948\949\950\951\952\953\954\955\956\957\958\959\960\961\962\963\964\965\966\967\968\969\970\971\972\973\974\975\976\977\978\979\980\981\982\983\984\985\986\987\988\989\990\991\992\993\994\995\996\997\998\999\1000</X2Data>
   <Y1Data>1.23\0.949\0.857\0.851\0.839\0.832\0.829\0.838\0.848\0.862\0.873\0.883\0.891\0.897\0.901\0.904\0.906\0.908\0.91\0.912\0.914\0.916\0.917\0.918\0.92\0.921\0.922\0.923\0.924\0.925\0.926\0.927\0.928\0.928\0.929\0.93\0.93\0.931\0.931\0.932\0.932\0.933\0.933\0.933\0.933\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.934\0.933\0.933\0.933\0.933\0.933\0.932\0.932\0.932\0.932\0.932\0.931\0.931\0.931\0.931\0.931\0.93\0.93\0.93\0.93\0.93\0.93\0.929\0.929\0.929\0.929\0.929\0.929\0.928\0.928\0.928\0.928\0.928\0.928\0.927\0.927\0.927\0.927\0.927\0.927\0.926\0.926\0.926\0.926\0.926\0.925\0.925\0.925\0.925\0.925\0.925\0.924\0.924\0.924\0.924\0.924\0.924\0.924\0.923\0.923\0.923\0.923\0.923\0.923\0.923\0.922\0.922\0.922\0.922\0.922\0.922\0.922\0.921\0.921\0.921\0.921\0.921\0.921\0.921\0.921\0.92\0.92\0.92\0.92\0.92\0.92\0.92\0.92\0.92\0.919\0.919\0.919\0.919\0.919\0.919\0.919\0.919\0.919\0.919\0.919\0.919\0.918\0.918\0.918\0.918\0.918\0.918\0.918\0.918\0.918\0.918\0.918\0.918\0.918\0.918\0.917\0.917\0.917\0.917\0.917\0.917\0.917\0.917\0.917\0.917\0.917\0.917\0.916\0.916\0.916\0.916\0.916\0.916\0.916\0.916\0.916\0.916\0.916\0.916\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.915\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.914\0.913\0.913\0.913\0.913\0.913\0.913\0.913\0.913\0.913\0.913\0.913\0.913\0.912\0.912\0.912\0.912\0.912\0.912\0.912\0.912\0.912\0.912\0.911\0.911\0.911\0.911\0.911\0.911\0.911\0.911\0.911\0.911\0.91\0.91\0.91\0.91\0.91\0.91\0.91\0.909\0.909\0.909\0.909\0.909\0.909\0.909\0.909\0.908\0.908\0.908\0.908\0.908\0.908\0.908\0.907\0.907\0.907\0.907\0.907\0.907\0.907\0.906\0.906\0.906\0.906\0.906\0.906\0.906\0.906\0.906\0.905\0.905\0.905\0.905\0.905\0.905\0.905\0.905\0.904\0.904\0.904\0.904\0.904\0.904\0.904\0.904\0.903\0.903\0.903\0.903\0.903\0.903\0.903\0.903\0.903\0.903\0.902\0.902\0.902\0.902\0.902\0.902\0.902\0.902\0.902\0.901\0.901\0.901\0.901\0.901\0.901\0.901\0.901\0.901\0.901\0.901\0.901\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.9\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.898\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.897\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.896\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.895\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.894\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.889\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.888\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.887\0.886\0.886\0.886\0.886\0.886\0.886\0.886\0.886\0.886\0.886\0.886\0.886\0.886\0.886\0.886\0.886\0.885\0.885\0.885\0.885\0.885\0.885\0.885\0.885\0.885\0.885\0.885\0.885\0.885\0.885\0.884\0.884\0.884\0.884\0.884\0.884\0.884\0.884\0.884\0.884\0.884\0.883\0.883\0.883\0.883\0.883\0.883\0.883\0.883\0.883\0.883\0.883\0.882\0.882\0.882\0.882\0.882\0.882\0.882\0.882\0.882\0.882\0.882\0.881\0.881\0.881\0.881\0.881\0.881\0.881\0.881\0.881\0.881\0.88\0.88\0.88\0.88\0.88\0.88\0.88\0.88\0.88\0.879\0.879\0.879\0.879\0.879\0.879\0.879\0.878\0.878\0.878\0.878\0.878\0.878\0.878\0.877\0.877\0.877\0.877\0.877\0.877\0.877\0.877\0.876\0.876\0.876\0.876\0.876\0.876\0.875\0.875\0.875\0.875\0.875\0.875\0.874\0.874\0.874\0.874\0.874\0.873\0.873\0.873\0.873\0.873\0.873\0.872\0.872\0.872\0.872\0.872\0.871\0.871\0.871\0.871\0.871\0.87\0.87\0.87\0.87\0.87\0.869\0.869\0.869\0.869\0.869\0.868\0.868\0.868\0.868\0.867\0.867\0.867\0.867\0.866\0.866\0.866\0.866\0.865\0.865\0.865\0.865\0.864\0.864\0.864\0.864\0.864\0.863\0.863\0.863\0.863\0.863\0.862\0.862\0.862\0.862\0.862\0.861\0.861\0.861\0.861\0.861\0.86\0.86\0.86\0.86\0.86\0.859\0.859\0.859\0.859\0.859\0.858\0.858\0.858\0.858\0.858\0.857\0.857\0.857\0.857\0.857\0.856\0.856\0.856\0.856\0.856\0.856\0.856\0.855\0.855\0.855\0.855\0.855\0.855\0.855\0.855\0.854\0.854\0.854\0.854\0.854\0.854\0.854\0.854\0.854\0.854\0.853</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.918\0.738\0.774\0.715\0.671\0.666\0.67\0.674\0.679\0.685\0.69\0.695\0.699\0.702\0.704\0.706\0.708\0.709\0.71\0.712\0.713\0.714\0.715\0.715\0.716\0.717\0.718\0.718\0.719\0.719\0.72\0.72\0.721\0.721\0.722\0.722\0.722\0.723\0.723\0.723\0.724\0.724\0.724\0.724\0.725\0.725\0.725\0.725\0.725\0.725\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.726\0.725\0.725\0.725\0.725\0.725\0.725\0.725\0.725\0.725\0.725\0.725\0.725\0.725\0.725\0.725\0.725\0.724\0.724\0.724\0.724\0.724\0.724\0.724\0.724\0.724\0.724\0.724\0.724\0.724\0.723\0.723\0.723\0.723\0.723\0.723\0.723\0.723\0.723\0.723\0.723\0.723\0.723\0.723\0.723\0.722\0.722\0.722\0.722\0.722\0.722\0.722\0.722\0.722\0.722\0.722\0.722\0.722\0.722\0.722\0.722\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.717\0.716\0.716\0.716\0.716\0.716\0.716\0.716\0.716\0.716\0.716\0.716\0.716\0.716\0.716\0.716\0.716\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.715\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.714\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.713\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.712\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.705\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.704\0.703\0.703\0.703\0.703\0.703\0.703\0.703\0.703\0.703\0.703\0.703\0.703\0.703\0.703\0.703\0.702\0.702\0.702\0.702\0.702\0.702\0.702\0.702\0.702\0.702\0.702\0.702\0.701\0.701\0.701\0.701\0.701\0.701\0.701\0.701\0.701\0.701\0.701\0.701\0.7\0.7\0.7\0.7\0.7\0.7\0.7\0.7\0.7\0.7\0.7\0.7\0.7\0.7\0.699\0.699\0.699\0.699\0.699\0.699\0.699\0.699\0.699\0.699\0.699\0.699\0.699\0.699\0.698\0.698\0.698\0.698\0.698\0.698\0.698\0.698\0.698\0.698\0.698\0.698\0.698\0.698\0.698\0.698\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>1001</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="4fUGmw" Title="Quasi-Newton method results">
   <Caption Id="Nh07AF">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.853
0.697
1000
00:04:08
Maximum number of epochs
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="g7eT9X" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="3JmJp8" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="Tnq0EN" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="BT2Pkb">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.414074\111.586\16.4025\13.9909
0.00265432\0.715292\0.105144\0.0896856
0.265432\71.5292\10.5144\8.96856</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="q24No3" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="s0TyV0">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.404631\319.406\10.111\15.304
0.00119009\0.93943\0.0297382\0.0450118
0.119009\93.943\2.97382\4.50118</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="K6tAo3" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="5Q4EWo">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0549469\26.9375\5.52781\4.75915
0.00022156\0.108619\0.0222895\0.0191901
0.022156\10.8619\2.22896\1.91901</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="gXHIr7" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="9pbvn0">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0133286\39.5862\8.48311\5.26133
0.000185119\0.549809\0.117821\0.0730741
0.0185119\54.9809\11.7821\7.30741</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="nATaAu" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="V0LJlv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0300493\13.9355\1.7149\1.00148
0.00176761\0.819735\0.100877\0.0589105
0.176761\81.9735\10.0877\5.89105</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="eiyfsq" Title="PM2.5(microg/m3)_ahead_2 errors statistics">
   <Caption Id="g4dm6A">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.412746\111.587\16.4159\13.9881
0.00264581\0.7153\0.10523\0.0896676
0.264581\71.53\10.523\8.96676</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="IqYpfb" Title="PM10(microg/m3)_ahead_2 errors statistics">
   <Caption Id="cULFJa">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.400442\319.407\10.1176\15.3022
0.00117777\0.939433\0.0297575\0.0450065
0.117777\93.9433\2.97575\4.50065</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="1BOMEm" Title="O3(microg/m3)_ahead_2 errors statistics">
   <Caption Id="X3TTeV">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00127411\24.9441\5.49087\4.77023
5.13754e-6\0.100581\0.0221406\0.0192348
0.000513754\10.0581\2.21406\1.92348</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="cdv6EC" Title="NO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="KAw9PM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.008564\39.4228\8.5789\5.30473
0.000118944\0.547539\0.119151\0.0736768
0.0118944\54.7538\11.9151\7.36767</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="JpsNOz" Title="SO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="LqYMpm">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00315571\13.9227\1.84098\1.03315
0.00018563\0.818981\0.108293\0.0607736
0.018563\81.8981\10.8293\6.07736</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="yWjggd" Title="PM2.5(microg/m3)_ahead_3 errors statistics">
   <Caption Id="AtT6zn">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.420525\111.579\16.4026\13.9882
0.00269567\0.715253\0.105145\0.0896682
0.269567\71.5253\10.5145\8.96682</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="OiY5H3" Title="PM10(microg/m3)_ahead_3 errors statistics">
   <Caption Id="KJ6aL3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.398546\319.404\10.1132\15.3046
0.00117219\0.939422\0.0297447\0.0450135
0.117219\93.9422\2.97447\4.50135</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="T67J6Y" Title="O3(microg/m3)_ahead_3 errors statistics">
   <Caption Id="okd1VQ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00824738\24.0925\5.47742\4.73489
3.32555e-5\0.0971471\0.0220864\0.0190923
0.00332555\9.71471\2.20864\1.90923</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="yKScic" Title="NO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="F9KJGg">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0152893\39.4155\8.62301\5.33426
0.000212351\0.547437\0.119764\0.0740869
0.0212351\54.7437\11.9764\7.40869</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="t0V6rz" Title="SO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="dE2jM0">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0157011\13.8807\1.80669\1.02484
0.000923591\0.81651\0.106276\0.0602848
0.0923591\81.6511\10.6276\6.02848</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="FDJfnk" Title="PM2.5(microg/m3)_ahead_4 errors statistics">
   <Caption Id="gUJlMr">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.426468\111.573\16.4123\13.9786
0.00273377\0.715209\0.105207\0.0896066
0.273377\71.5209\10.5207\8.96066</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="aZGgTH" Title="PM10(microg/m3)_ahead_4 errors statistics">
   <Caption Id="pwpuqj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.397858\319.398\10.0936\15.3077
0.00117017\0.939407\0.029687\0.0450226
0.117017\93.9407\2.9687\4.50226</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="fuIRcR" Title="O3(microg/m3)_ahead_4 errors statistics">
   <Caption Id="O2ow6h">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0220032\22.6647\5.57987\4.77234
8.87225e-5\0.0913898\0.0224995\0.0192433
0.00887225\9.13898\2.24995\1.92433</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="VYeCOQ" Title="NO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="6zqIeZ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.101789\39.4398\8.61608\5.3369
0.00141374\0.547775\0.119668\0.0741236
0.141374\54.7775\11.9668\7.41236</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="53zChg" Title="SO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="pKrRE4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00968146\13.5488\1.9075\1.03713
0.000569498\0.79699\0.112206\0.0610079
0.0569498\79.699\11.2206\6.10079</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="kohWQX" Title="WINDSPEED(km/h)_ahead_4 errors statistics">
   <Caption Id="PqOK2P">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0215139\18.5763\4.08227\3.12794
0.000627229\0.541582\0.119017\0.0911936
0.0627229\54.1582\11.9017\9.11936</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="hUILo2" Title="PM2.5(microg/m3)_ahead_5 errors statistics">
   <Caption Id="FU7KRf">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.431774\111.568\16.3849\13.9861
0.00276778\0.71518\0.105031\0.0896543
0.276778\71.5181\10.5031\8.96543</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="a791aL" Title="PM10(microg/m3)_ahead_5 errors statistics">
   <Caption Id="lHRLPH">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.396139\319.401\10.0787\15.31
0.00116512\0.939415\0.0296433\0.0450295
0.116512\93.9415\2.96433\4.50295</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uUoxWK" Title="O3(microg/m3)_ahead_5 errors statistics">
   <Caption Id="Y7WJQ2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0305519\22.4146\5.44277\4.5622
0.000123193\0.0903815\0.0219467\0.0183959
0.0123193\9.03815\2.19467\1.83959</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="jXTkpO" Title="NO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="fpbzY5">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00294876\39.3158\8.55924\5.32244
4.0955e-5\0.546053\0.118878\0.0739228
0.0040955\54.6053\11.8878\7.39228</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="hbb7mx" Title="SO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="mcxYJh">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00959396\13.5527\1.84365\1.02244
0.000564351\0.797217\0.10845\0.0601435
0.0564351\79.7217\10.845\6.01435</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="XNZKK0" Title="PM2.5(microg/m3)_ahead_6 errors statistics">
   <Caption Id="QxRUwD">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.418392\111.58\16.3695\13.9925
0.002682\0.715256\0.104933\0.0896955
0.2682\71.5256\10.4933\8.96955</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="kN80N3" Title="PM10(microg/m3)_ahead_6 errors statistics">
   <Caption Id="0JNOIP">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.395891\319.401\10.0697\15.314
0.00116439\0.939416\0.0296167\0.0450412
0.116439\93.9416\2.96167\4.50412</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="oaFQQd" Title="O3(microg/m3)_ahead_6 errors statistics">
   <Caption Id="n566Zf">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0185165\23.9604\5.80204\4.96086
7.46635e-5\0.0966146\0.0233953\0.0200034
0.00746635\9.66146\2.33953\2.00034</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="SoZPoU" Title="NO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="NpgX8r">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.305281\39.4263\8.66928\5.36518
0.00424001\0.547588\0.120407\0.0745164
0.424001\54.7588\12.0407\7.45164</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="5yZ92Y" Title="SO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="tNe5nC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0255184\13.3012\1.89864\1.02592
0.00150108\0.782425\0.111685\0.0603482
0.150108\78.2425\11.1685\6.03482</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Wp0Ppg" Title="PM2.5(microg/m3)_ahead_7 errors statistics">
   <Caption Id="iotjXS">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.413692\111.586\16.3673\13.9949
0.00265187\0.715296\0.104918\0.089711
0.265187\71.5296\10.4918\8.9711</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="79c7ep" Title="PM10(microg/m3)_ahead_7 errors statistics">
   <Caption Id="wuqsx3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.39921\319.399\10.0931\15.3153
0.00117415\0.93941\0.0296856\0.045045
0.117415\93.941\2.96856\4.5045</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="pkCzOH" Title="O3(microg/m3)_ahead_7 errors statistics">
   <Caption Id="g0fycJ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.010128\24.034\5.82471\5.00283
4.08388e-5\0.0969112\0.0234867\0.0201727
0.00408388\9.69111\2.34867\2.01727</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="PmGZUw" Title="NO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="sYHzBh">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0129452\39.1548\8.49916\5.29031
0.000179794\0.543817\0.118044\0.0734765
0.0179794\54.3817\11.8044\7.34765</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="zvavNO" Title="SO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="W4DBXa">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0140433\13.2991\1.88236\1.01943
0.000826078\0.782297\0.110727\0.0599667
0.0826078\78.2297\11.0727\5.99667</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="9OhUY5" Title="PM2.5(microg/m3)_ahead_8 errors statistics">
   <Caption Id="9pXwBF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.410351\111.59\16.4\13.9805
0.00263045\0.715318\0.105129\0.0896185
0.263045\71.5318\10.5128\8.96185</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="JqYQeX" Title="PM10(microg/m3)_ahead_8 errors statistics">
   <Caption Id="DFkh65">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.395536\319.396\10.1091\15.3178
0.00116334\0.939399\0.0297328\0.0450523
0.116334\93.9399\2.97328\4.50523</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="s1GstP" Title="O3(microg/m3)_ahead_8 errors statistics">
   <Caption Id="0q4gKs">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0111141\25.1141\5.79955\4.90631
4.4815e-5\0.101267\0.0233853\0.0197835
0.0044815\10.1267\2.33853\1.97835</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="AP5DO8" Title="NO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="WnCWRf">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0397625\39.3326\8.48302\5.27336
0.000552257\0.546287\0.11782\0.0732411
0.0552257\54.6287\11.782\7.32411</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ci2UKG" Title="SO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="Mq0lK9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0599892\13.3076\1.92456\1.02464
0.00352878\0.782799\0.113209\0.0602729
0.352878\78.2799\11.3209\6.02729</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="4hyNJa" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="DJ32dA" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 3816.</Text>
 </Task>
 <Task Id="TpLBIj" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="YKpjkH" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="Vl4RPK" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="AQdmUb" Title="Quasi-Newton method errors history">
   <Caption Id="Vb5Elg">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.95452, and the final value after 868 epochs is 0.773108.
The initial value of the selection error is 2.66226, and the final value after 868 epochs is 0.626962.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661\662\663\664\665\666\667\668\669\670\671\672\673\674\675\676\677\678\679\680\681\682\683\684\685\686\687\688\689\690\691\692\693\694\695\696\697\698\699\700\701\702\703\704\705\706\707\708\709\710\711\712\713\714\715\716\717\718\719\720\721\722\723\724\725\726\727\728\729\730\731\732\733\734\735\736\737\738\739\740\741\742\743\744\745\746\747\748\749\750\751\752\753\754\755\756\757\758\759\760\761\762\763\764\765\766\767\768\769\770\771\772\773\774\775\776\777\778\779\780\781\782\783\784\785\786\787\788\789\790\791\792\793\794\795\796\797\798\799\800\801\802\803\804\805\806\807\808\809\810\811\812\813\814\815\816\817\818\819\820\821\822\823\824\825\826\827\828\829\830\831\832\833\834\835\836\837\838\839\840\841\842\843\844\845\846\847\848\849\850\851\852\853\854\855\856\857\858\859\860\861\862\863\864\865\866\867\868</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661\662\663\664\665\666\667\668\669\670\671\672\673\674\675\676\677\678\679\680\681\682\683\684\685\686\687\688\689\690\691\692\693\694\695\696\697\698\699\700\701\702\703\704\705\706\707\708\709\710\711\712\713\714\715\716\717\718\719\720\721\722\723\724\725\726\727\728\729\730\731\732\733\734\735\736\737\738\739\740\741\742\743\744\745\746\747\748\749\750\751\752\753\754\755\756\757\758\759\760\761\762\763\764\765\766\767\768\769\770\771\772\773\774\775\776\777\778\779\780\781\782\783\784\785\786\787\788\789\790\791\792\793\794\795\796\797\798\799\800\801\802\803\804\805\806\807\808\809\810\811\812\813\814\815\816\817\818\819\820\821\822\823\824\825\826\827\828\829\830\831\832\833\834\835\836\837\838\839\840\841\842\843\844\845\846\847\848\849\850\851\852\853\854\855\856\857\858\859\860\861\862\863\864\865\866\867\868</X2Data>
   <Y1Data>2.95\1.27\1.02\0.92\0.946\0.877\0.865\0.843\0.841\0.823\0.82\0.823\0.827\0.823\0.824\0.825\0.828\0.832\0.833\0.834\0.836\0.836\0.837\0.839\0.84\0.841\0.841\0.842\0.842\0.842\0.843\0.844\0.844\0.844\0.844\0.844\0.843\0.843\0.843\0.843\0.842\0.842\0.842\0.841\0.841\0.841\0.841\0.84\0.84\0.84\0.84\0.839\0.839\0.839\0.838\0.838\0.838\0.837\0.837\0.837\0.836\0.836\0.836\0.835\0.835\0.835\0.834\0.834\0.834\0.833\0.833\0.833\0.832\0.832\0.832\0.831\0.831\0.831\0.83\0.83\0.83\0.829\0.829\0.829\0.829\0.828\0.828\0.827\0.827\0.827\0.826\0.826\0.826\0.826\0.825\0.825\0.825\0.825\0.825\0.825\0.824\0.824\0.824\0.824\0.824\0.823\0.823\0.823\0.823\0.823\0.823\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.821\0.821\0.821\0.821\0.821\0.821\0.82\0.82\0.82\0.82\0.82\0.82\0.819\0.819\0.819\0.819\0.819\0.818\0.818\0.818\0.817\0.817\0.817\0.817\0.817\0.816\0.816\0.816\0.816\0.815\0.815\0.815\0.815\0.814\0.814\0.814\0.813\0.813\0.813\0.813\0.813\0.812\0.812\0.812\0.812\0.812\0.811\0.811\0.811\0.811\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.81\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.809\0.808\0.808\0.808\0.808\0.808\0.808\0.808\0.808\0.808\0.808\0.808\0.808\0.808\0.808\0.808\0.807\0.807\0.807\0.807\0.807\0.807\0.807\0.807\0.807\0.806\0.806\0.806\0.806\0.806\0.805\0.805\0.805\0.805\0.805\0.805\0.805\0.805\0.805\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.804\0.803\0.803\0.803\0.803\0.803\0.803\0.803\0.803\0.802\0.802\0.802\0.802\0.802\0.802\0.802\0.801\0.801\0.801\0.8\0.8\0.8\0.8\0.799\0.799\0.799\0.798\0.798\0.798\0.798\0.798\0.797\0.797\0.797\0.797\0.796\0.796\0.796\0.796\0.796\0.795\0.795\0.795\0.795\0.795\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.794\0.793\0.793\0.793\0.793\0.793\0.793\0.793\0.793\0.793\0.793\0.793\0.793\0.792\0.792\0.792\0.792\0.792\0.792\0.792\0.792\0.792\0.792\0.791\0.791\0.791\0.791\0.791\0.791\0.791\0.79\0.79\0.79\0.79\0.79\0.79\0.79\0.789\0.789\0.789\0.789\0.789\0.789\0.789\0.789\0.788\0.788\0.788\0.788\0.788\0.788\0.788\0.787\0.787\0.787\0.787\0.787\0.787\0.787\0.787\0.786\0.786\0.786\0.786\0.786\0.786\0.786\0.786\0.785\0.785\0.785\0.785\0.785\0.785\0.785\0.784\0.784\0.784\0.784\0.784\0.784\0.784\0.784\0.784\0.784\0.784\0.784\0.784\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.783\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.782\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.781\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.78\0.779\0.779\0.779\0.779\0.779\0.779\0.779\0.779\0.779\0.779\0.778\0.778\0.778\0.778\0.778\0.778\0.778\0.778\0.778\0.777\0.777\0.777\0.777\0.777\0.777\0.776\0.776\0.776\0.776\0.776\0.775\0.775\0.775\0.775\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.775\0.775\0.775\0.775\0.775\0.775\0.775\0.775\0.775\0.774\0.774\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.773\0.773\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.774\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773\0.773</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.66\1.17\1\0.817\0.777\0.734\0.695\0.654\0.676\0.685\0.675\0.664\0.658\0.657\0.658\0.657\0.658\0.659\0.66\0.659\0.661\0.662\0.663\0.664\0.664\0.664\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.666\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.656\0.655\0.655\0.655\0.655\0.655\0.655\0.655\0.655\0.655\0.655\0.655\0.655\0.655\0.654\0.654\0.654\0.654\0.654\0.654\0.654\0.653\0.653\0.653\0.653\0.653\0.652\0.652\0.652\0.652\0.652\0.652\0.652\0.652\0.651\0.651\0.651\0.651\0.651\0.651\0.651\0.651\0.651\0.65\0.65\0.65\0.65\0.65\0.65\0.65\0.65\0.65\0.65\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.649\0.648\0.648\0.648\0.648\0.648\0.648\0.648\0.648\0.648\0.648\0.648\0.648\0.647\0.647\0.647\0.647\0.647\0.647\0.647\0.647\0.646\0.646\0.646\0.646\0.646\0.646\0.646\0.646\0.645\0.645\0.645\0.645\0.645\0.645\0.645\0.645\0.644\0.644\0.644\0.644\0.644\0.644\0.644\0.644\0.643\0.643\0.643\0.643\0.643\0.643\0.643\0.643\0.643\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.642\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.641\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.64\0.639\0.639\0.639\0.639\0.639\0.639\0.639\0.639\0.639\0.639\0.639\0.639\0.639\0.639\0.639\0.639\0.638\0.638\0.638\0.638\0.638\0.638\0.638\0.638\0.638\0.638\0.638\0.638\0.638\0.638\0.638\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.637\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.636\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.635\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.634\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.633\0.632\0.632\0.632\0.632\0.632\0.632\0.632\0.632\0.632\0.632\0.632\0.631\0.631\0.631\0.631\0.631\0.631\0.631\0.631\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.63\0.629\0.629\0.629\0.629\0.629\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.628\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627\0.627</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>869</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="CQg6db" Title="Quasi-Newton method results">
   <Caption Id="xS8PBn">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.773
0.627
868
00:00:18
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="djuayJ" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="azV7Uu" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="9Lwyqg" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ISxutM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0340843\100.829\10.6533\10.9294
0.000218489\0.646339\0.0682901\0.0700604
0.0218489\64.6339\6.82901\7.00604</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uaI9bl" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="zbY8EF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00260925\312.965\7.86817\14.6156
7.67427e-6\0.920485\0.0231417\0.0429871
0.000767427\92.0485\2.31417\4.29871</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Gxh0L3" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="TSrfpK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00502777\25.5781\4.93212\4.38998
2.02733e-5\0.103137\0.0198876\0.0177015
0.00202733\10.3137\1.98876\1.77015</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="JQ74Zw" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="5GYVzf">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0328426\38.8213\8.51835\5.29475
0.000456148\0.539185\0.11831\0.0735382
0.0456148\53.9185\11.831\7.35382</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="NY96cu" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ksXqrJ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00142097\15.3998\0.862264\0.938842
8.35867e-5\0.905871\0.0507214\0.055226
0.00835868\90.5871\5.07214\5.5226</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="mT7EKq" Title="PM2.5(microg/m3)_ahead_2 errors statistics">
   <Caption Id="px257W">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0432243\100.761\12.505\12.3529
0.000277079\0.645904\0.0801599\0.0791852
0.0277079\64.5904\8.01599\7.91852</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="f5Ml1c" Title="PM10(microg/m3)_ahead_2 errors statistics">
   <Caption Id="Imuy8g">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00314903\321.795\9.34982\15.2212
9.26186e-6\0.946455\0.0274995\0.0447683
0.000926186\94.6455\2.74995\4.47683</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="5XDl8f" Title="O3(microg/m3)_ahead_2 errors statistics">
   <Caption Id="j8unKd">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0100021\25.0706\5.09262\4.49722
4.03312e-5\0.101091\0.0205348\0.018134
0.00403312\10.1091\2.05348\1.8134</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ZV3E57" Title="NO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="RQWfK2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0127449\38.6792\8.58673\5.34767
0.000177013\0.537211\0.11926\0.0742732
0.0177013\53.7211\11.926\7.42732</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="R7mRO7" Title="SO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="DRKzDN">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00211787\15.331\0.970942\0.979234
0.000124581\0.901821\0.0571143\0.057602
0.0124581\90.1821\5.71143\5.7602</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="jNx7Pz" Title="PM2.5(microg/m3)_ahead_3 errors statistics">
   <Caption Id="Hiuh5S">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0390663\116.014\14.9706\13.495
0.000250425\0.74368\0.0959655\0.0865063
0.0250425\74.368\9.59655\8.65063</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="b717kq" Title="PM10(microg/m3)_ahead_3 errors statistics">
   <Caption Id="hC6kns">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.329853\319.455\10.1029\15.3041
0.000970156\0.939575\0.0297144\0.0450119
0.0970156\93.9575\2.97144\4.50119</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="tPJtLP" Title="O3(microg/m3)_ahead_3 errors statistics">
   <Caption Id="YNyLEo">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00463867\25.3102\5.12841\4.43827
1.87043e-5\0.102057\0.0206791\0.0178962
0.00187043\10.2057\2.06791\1.78962</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="f6RIcl" Title="NO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="2tSja6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0322418\38.686\8.60478\5.35659
0.000447803\0.537305\0.119511\0.0743971
0.0447803\53.7305\11.9511\7.43971</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="RacHvq" Title="SO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="MwrbGZ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.002352\15.2217\1.02682\0.989219
0.000138353\0.895393\0.0604011\0.0581893
0.0138353\89.5393\6.04011\5.81893</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="mSodnf" Title="PM2.5(microg/m3)_ahead_4 errors statistics">
   <Caption Id="iHfITL">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0389366\112.063\16.3201\13.9295
0.000249594\0.718354\0.104616\0.0892916
0.0249594\71.8354\10.4616\8.92916</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="OoZF78" Title="PM10(microg/m3)_ahead_4 errors statistics">
   <Caption Id="aeVCqM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.394373\319.404\10.0927\15.3077
0.00115992\0.939422\0.0296845\0.0450228
0.115992\93.9422\2.96845\4.50228</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="TsLj8l" Title="O3(microg/m3)_ahead_4 errors statistics">
   <Caption Id="UOPuKG">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00346565\23.5291\4.99221\4.1672
1.39744e-5\0.0948754\0.0201299\0.0168032
0.00139744\9.48754\2.01299\1.68032</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="epA22l" Title="NO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="u3FtJj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00895691\38.8611\8.57349\5.33883
0.000124402\0.539738\0.119076\0.0741505
0.0124402\53.9738\11.9076\7.41505</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="gAkeIp" Title="SO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="3MyLr4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.000641823\15.1526\1.05747\0.983623
3.77543e-5\0.891329\0.0622039\0.0578602
0.00377543\89.1329\6.22039\5.78602</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="UJsHg5" Title="WINDSPEED(km/h)_ahead_4 errors statistics">
   <Caption Id="i1ldmz">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0244541\18.5754\4.08155\3.12781
0.000712948\0.541557\0.118996\0.0911899
0.0712948\54.1557\11.8996\9.11899</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="aoxpaJ" Title="PM2.5(microg/m3)_ahead_5 errors statistics">
   <Caption Id="RwthZk">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.42794\111.571\16.3842\13.9853
0.00274321\0.7152\0.105027\0.0896495
0.274321\71.52\10.5027\8.96495</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="0vTjFn" Title="PM10(microg/m3)_ahead_5 errors statistics">
   <Caption Id="mu1SsZ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.390532\319.393\10.0776\15.3095
0.00114862\0.939391\0.0296401\0.045028
0.114862\93.9391\2.96401\4.5028</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="9uNOqh" Title="O3(microg/m3)_ahead_5 errors statistics">
   <Caption Id="JaCJdm">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00038147\21.9495\5.04792\4.16968
1.53818e-6\0.0885062\0.0203545\0.0168132
0.000153818\8.85062\2.03545\1.68132</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="nBcxfA" Title="NO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="omHINp">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00107002\38.9423\8.58592\5.34962
1.48614e-5\0.540865\0.119249\0.0743002
0.00148614\54.0865\11.9249\7.43002</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="IuFzHI" Title="SO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="PY2NR4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0043788\15.0663\1.08256\0.973658
0.000257576\0.886252\0.0636803\0.057274
0.0257576\88.6252\6.36803\5.7274</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uV08wi" Title="PM2.5(microg/m3)_ahead_6 errors statistics">
   <Caption Id="Eh3W5p">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.418087\111.58\16.3699\13.9928
0.00268004\0.715257\0.104936\0.0896973
0.268004\71.5257\10.4936\8.96973</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="07Wn64" Title="PM10(microg/m3)_ahead_6 errors statistics">
   <Caption Id="11FwBC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.392658\319.394\10.0714\15.3133
0.00115488\0.939395\0.0296218\0.0450393
0.115488\93.9395\2.96218\4.50393</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="eAvUhY" Title="O3(microg/m3)_ahead_6 errors statistics">
   <Caption Id="pRvJc0">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0174179\21.59\5.03221\4.0953
7.02335e-5\0.0870565\0.0202912\0.0165133
0.00702335\8.70565\2.02912\1.65133</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="02F4yc" Title="NO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="V05dT2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.220562\39.4941\8.63574\5.34908
0.00306336\0.548529\0.119941\0.0742928
0.306336\54.8529\11.9941\7.42928</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Leg4sB" Title="SO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="j9TMwc">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.000941515\14.8882\1.09475\0.968914
5.53832e-5\0.875775\0.0643969\0.0569949
0.00553832\87.5775\6.43969\5.69949</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="caCeCk" Title="PM2.5(microg/m3)_ahead_7 errors statistics">
   <Caption Id="UOkOWe">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.411346\111.587\16.3675\13.9952
0.00263684\0.715299\0.10492\0.0897128
0.263684\71.5299\10.492\8.97128</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="3b9SiS" Title="PM10(microg/m3)_ahead_7 errors statistics">
   <Caption Id="ixJsej">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.394348\319.402\10.0919\15.3152
0.00115985\0.939418\0.0296821\0.0450447
0.115985\93.9418\2.96821\4.50447</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="5Yq9OM" Title="O3(microg/m3)_ahead_7 errors statistics">
   <Caption Id="r5Sjzu">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00680542\23.0553\5.19956\4.28703
2.74412e-5\0.0929651\0.020966\0.0172864
0.00274412\9.29651\2.0966\1.72864</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="zDrZey" Title="NO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="dogvFY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00233269\38.8442\8.56284\5.34302
3.23984e-5\0.539503\0.118928\0.0742087
0.00323984\53.9503\11.8928\7.42087</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="so5hho" Title="SO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="FBuVED">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0102627\14.3722\1.11639\0.945948
0.00060369\0.845423\0.0656702\0.055644
0.060369\84.5423\6.56702\5.5644</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="13HzvF" Title="PM2.5(microg/m3)_ahead_8 errors statistics">
   <Caption Id="RnvYbA">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.410992\111.589\16.4\13.9803
0.00263456\0.715313\0.105128\0.0896171
0.263456\71.5313\10.5128\8.96171</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="o2NAet" Title="PM10(microg/m3)_ahead_8 errors statistics">
   <Caption Id="Yfd4B7">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.391672\319.398\10.1091\15.3177
0.00115198\0.939405\0.0297325\0.0450522
0.115198\93.9405\2.97325\4.50522</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="1tmYac" Title="O3(microg/m3)_ahead_8 errors statistics">
   <Caption Id="M3KfeR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00766373\25.042\5.39288\4.45576
3.09021e-5\0.100976\0.0217455\0.0179668
0.00309021\10.0976\2.17455\1.79668</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="zXWyUn" Title="NO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="1J1wtY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0324345\38.8497\8.55166\5.33485
0.000450479\0.539579\0.118773\0.0740951
0.0450479\53.9579\11.8773\7.40951</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="b7L3j8" Title="SO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="MFHs8S">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.000721216\14.7504\1.16106\0.956774
4.24245e-5\0.867668\0.0682978\0.0562808
0.00424245\86.7668\6.82978\5.62808</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="Ln27b8" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="T5KDja" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 796.</Text>
 </Task>
 <Task Id="ntI0TC" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="tc1TN1" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="ZPvdSn" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="QQxYSg" Title="Quasi-Newton method errors history">
   <Caption Id="IvMuif">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 5.26941, and the final value after 232 epochs is 0.842791.
The initial value of the selection error is 4.89754, and the final value after 232 epochs is 0.680253.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232</X2Data>
   <Y1Data>5.27\2.61\1.56\1.34\0.993\1.06\0.922\1.03\0.93\0.823\0.836\0.856\0.823\0.816\0.821\0.799\0.803\0.808\0.809\0.818\0.82\0.826\0.826\0.827\0.833\0.837\0.843\0.846\0.847\0.849\0.851\0.851\0.851\0.852\0.851\0.851\0.851\0.852\0.852\0.853\0.853\0.853\0.854\0.854\0.855\0.855\0.855\0.855\0.856\0.856\0.856\0.856\0.856\0.856\0.856\0.857\0.857\0.857\0.857\0.857\0.857\0.857\0.857\0.856\0.856\0.856\0.855\0.855\0.855\0.855\0.855\0.854\0.854\0.854\0.854\0.854\0.854\0.853\0.853\0.853\0.853\0.853\0.853\0.853\0.853\0.853\0.853\0.853\0.853\0.853\0.853\0.853\0.853\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.852\0.851\0.851\0.851\0.851\0.851\0.851\0.851\0.851\0.851\0.851\0.851\0.851\0.851\0.85\0.85\0.85\0.85\0.85\0.85\0.849\0.849\0.849\0.849\0.849\0.849\0.848\0.848\0.848\0.848\0.848\0.848\0.847\0.847\0.847\0.847\0.847\0.846\0.846\0.846\0.846\0.846\0.845\0.845\0.845\0.845\0.845\0.845\0.845\0.844\0.844\0.844\0.844\0.844\0.844\0.844\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.842\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843\0.843</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>4.9\2.45\1.39\1.36\0.88\0.926\0.829\0.912\0.848\0.717\0.728\0.73\0.688\0.681\0.682\0.664\0.661\0.663\0.662\0.664\0.666\0.668\0.669\0.668\0.67\0.672\0.675\0.676\0.677\0.679\0.68\0.68\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.682\0.682\0.682\0.682\0.682\0.683\0.683\0.683\0.683\0.683\0.683\0.684\0.684\0.684\0.684\0.684\0.684\0.684\0.685\0.685\0.685\0.685\0.685\0.684\0.684\0.684\0.684\0.684\0.684\0.684\0.684\0.683\0.683\0.683\0.683\0.683\0.683\0.683\0.683\0.683\0.683\0.683\0.683\0.683\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.681\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>233</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>6</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="tSw5a1" Title="Quasi-Newton method results">
   <Caption Id="3QUMri">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.843
0.68
232
00:00:18
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="apABna" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="bgFu99" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="ItWwzz" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="tvLPX7">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.41037\111.587\16.4015\13.9899
0.00263058\0.7153\0.105138\0.0896787
0.263058\71.53\10.5138\8.96787</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="y8XQOT" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="bg6HyU">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.372135\319.422\10.1048\15.3039
0.00109452\0.939475\0.02972\0.0450114
0.109452\93.9475\2.972\4.50114</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="CIVqO3" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="ry8c8q">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0153065\27.0789\5.34441\4.64395
6.17196e-5\0.109189\0.0215501\0.0187256
0.00617197\10.9189\2.155\1.87256</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="woRpIF" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="XhWbfp">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0629005\39.7726\8.40215\5.20608
0.000873619\0.552398\0.116697\0.0723067
0.0873619\55.2398\11.6697\7.23067</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="RQM5Zv" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="UkEMyE">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00462604\15.0528\1.02736\0.935688
0.00027212\0.885461\0.0604327\0.0550405
0.027212\88.5461\6.04327\5.50405</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ukGImp" Title="PM2.5(microg/m3)_ahead_2 errors statistics">
   <Caption Id="UrRs7L">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.415684\111.584\16.4164\13.9879
0.00266464\0.71528\0.105233\0.0896661
0.266464\71.528\10.5233\8.96661</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="2peiNd" Title="PM10(microg/m3)_ahead_2 errors statistics">
   <Caption Id="z4zBQl">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.395082\319.411\10.116\15.3022
0.00116201\0.939444\0.0297528\0.0450066
0.116201\93.9444\2.97528\4.50066</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="fMYA22" Title="O3(microg/m3)_ahead_2 errors statistics">
   <Caption Id="PEKeVG">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0180969\26.4617\5.36571\4.62612
7.29715e-5\0.1067\0.0216359\0.0186537
0.00729715\10.67\2.16359\1.86537</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="VNgMP1" Title="NO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="Xhagki">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.132267\39.1972\8.66024\5.34405
0.00183704\0.544406\0.120281\0.0742229
0.183704\54.4406\12.0281\7.42229</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4tD0NF" Title="SO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="aWb8cy">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.000943661\15.009\1.12894\0.970081
5.55095e-5\0.88288\0.0664081\0.0570636
0.00555095\88.288\6.64081\5.70636</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Ulg28L" Title="PM2.5(microg/m3)_ahead_3 errors statistics">
   <Caption Id="uXLJB6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.422634\111.577\16.4026\13.9875
0.00270919\0.715236\0.105145\0.0896638
0.270919\71.5236\10.5145\8.96638</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="JW0dj8" Title="PM10(microg/m3)_ahead_3 errors statistics">
   <Caption Id="ZZatjB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.351469\319.429\10.1024\15.3046
0.00103373\0.939498\0.0297129\0.0450136
0.103373\93.9498\2.97129\4.50136</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="IDyUZl" Title="O3(microg/m3)_ahead_3 errors statistics">
   <Caption Id="JUIAwD">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00405502\25.5088\5.36598\4.55051
1.63509e-5\0.102858\0.021637\0.0183489
0.00163509\10.2858\2.1637\1.83488</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="MtPxXN" Title="NO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="pd7cu3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.266029\39.5665\8.67508\5.35666
0.00369485\0.549535\0.120487\0.0743981
0.369485\54.9535\12.0487\7.43981</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="zAGyVE" Title="SO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="Zd9U9e">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.000505209\14.9279\1.18032\0.975674
2.97182e-5\0.878109\0.0694307\0.0573926
0.00297182\87.8109\6.94307\5.73926</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="mCJ7J5" Title="PM2.5(microg/m3)_ahead_4 errors statistics">
   <Caption Id="H5WpaK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.4146\111.583\16.4112\13.9792
0.00265769\0.715273\0.1052\0.0896102
0.265769\71.5273\10.52\8.96102</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="CYrpmT" Title="PM10(microg/m3)_ahead_4 errors statistics">
   <Caption Id="qqByLS">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.39045\319.402\10.0921\15.3076
0.00114838\0.939418\0.0296825\0.0450222
0.114838\93.9418\2.96825\4.50222</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ILt1d4" Title="O3(microg/m3)_ahead_4 errors statistics">
   <Caption Id="aHzs3p">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00851822\24.5203\5.36736\4.51751
3.43477e-5\0.0988721\0.0216426\0.0182158
0.00343477\9.88721\2.16426\1.82158</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="IahrD6" Title="NO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="h77Vxu">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.290033\39.6017\8.65079\5.35076
0.00402824\0.550024\0.12015\0.0743162
0.402824\55.0024\12.015\7.43162</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="IGVatT" Title="SO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="Gtaiz3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00529766\14.9036\1.18408\0.971415
0.000311627\0.876685\0.0696516\0.0571421
0.0311627\87.6685\6.96516\5.71421</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="EH31wd" Title="WINDSPEED(km/h)_ahead_4 errors statistics">
   <Caption Id="Ygfw2Z">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0248766\18.5751\4.0817\3.12776
0.000725265\0.541549\0.119\0.0911883
0.0725265\54.1549\11.9\9.11884</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="x9dKt4" Title="PM2.5(microg/m3)_ahead_5 errors statistics">
   <Caption Id="cPC4hp">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.430744\111.568\16.385\13.9861
0.00276118\0.715179\0.105032\0.0896548
0.276118\71.5179\10.5032\8.96548</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="xknvuE" Title="PM10(microg/m3)_ahead_5 errors statistics">
   <Caption Id="nX75qK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.390923\319.407\10.0768\15.3102
0.00114977\0.939431\0.0296376\0.0450301
0.114977\93.9431\2.96376\4.50301</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="OKwJe4" Title="O3(microg/m3)_ahead_5 errors statistics">
   <Caption Id="4XNIKX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.000890732\24.0135\5.48708\4.69963
3.59166e-6\0.0968284\0.0221253\0.0189501
0.000359166\9.68284\2.21253\1.89501</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="LpGf0P" Title="NO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="ykSbC6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.308699\39.6148\8.63131\5.34738
0.00428748\0.550206\0.119879\0.0742692
0.428748\55.0206\11.9879\7.42692</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="tDPNm6" Title="SO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="1ydCZw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00481224\14.7631\1.26485\0.963039
0.000283073\0.868416\0.0744032\0.0566494
0.0283073\86.8416\7.44032\5.66494</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="k9RS2P" Title="PM2.5(microg/m3)_ahead_6 errors statistics">
   <Caption Id="Vv8ZiR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.416721\111.581\16.3694\13.9924
0.00267129\0.71526\0.104932\0.0896947
0.267129\71.526\10.4932\8.96947</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="FKuaJ5" Title="PM10(microg/m3)_ahead_6 errors statistics">
   <Caption Id="2k7UYy">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.391628\319.407\10.068\15.3141
0.00115185\0.939431\0.0296118\0.0450415
0.115185\93.9431\2.96118\4.50415</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="VG8YyT" Title="O3(microg/m3)_ahead_6 errors statistics">
   <Caption Id="LKDWBK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0150452\23.7069\5.50904\4.67609
6.0666e-5\0.0955924\0.0222139\0.0188552
0.0060666\9.55924\2.22139\1.88552</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4GvGdF" Title="NO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="AgUGHj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.308586\39.6274\8.63165\5.34501
0.00428592\0.550381\0.119884\0.0742363
0.428592\55.0381\11.9884\7.42363</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="7rmWio" Title="SO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="awH7Y2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00346065\14.6267\1.2797\0.958152
0.000203567\0.860396\0.0752764\0.0563619
0.0203567\86.0396\7.52764\5.63619</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="jsON4H" Title="PM2.5(microg/m3)_ahead_7 errors statistics">
   <Caption Id="kT74WK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.409977\111.589\16.3667\13.9949
0.00262806\0.715312\0.104915\0.0897109
0.262806\71.5312\10.4915\8.97109</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="TYhHJb" Title="PM10(microg/m3)_ahead_7 errors statistics">
   <Caption Id="01nrqv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.371931\319.417\10.0861\15.3152
0.00109391\0.93946\0.0296649\0.0450448
0.109391\93.946\2.96649\4.50448</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="pNVptv" Title="O3(microg/m3)_ahead_7 errors statistics">
   <Caption Id="3ejUt1">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00134277\24.0026\5.63033\4.80837
5.41441e-6\0.0967847\0.022703\0.0193886
0.000541441\9.67847\2.2703\1.93886</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="6ATZpX" Title="NO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="rY6MJv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.201736\39.4648\8.64211\5.35368
0.0028019\0.548123\0.120029\0.0743567
0.28019\54.8123\12.0029\7.43567</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="HE6T1D" Title="SO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="TH8qRq">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00338626\14.2955\1.3075\0.941645
0.000199192\0.840911\0.0769118\0.0553909
0.0199192\84.0911\7.69118\5.53909</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="PiK1JP" Title="PM2.5(microg/m3)_ahead_8 errors statistics">
   <Caption Id="klKAWZ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.410706\111.587\16.4004\13.9804
0.00263273\0.715301\0.105131\0.0896177
0.263273\71.5301\10.5131\8.96177</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Klht9F" Title="PM10(microg/m3)_ahead_8 errors statistics">
   <Caption Id="t50wH9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.393606\319.397\10.1085\15.3178
0.00115767\0.939404\0.0297309\0.0450523
0.115767\93.9404\2.97309\4.50523</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="hkpEkq" Title="O3(microg/m3)_ahead_8 errors statistics">
   <Caption Id="iGbdxW">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.0086422\24.8337\5.76123\4.95539
3.48476e-5\0.100136\0.0232308\0.0199814
0.00348476\10.0136\2.32308\1.99814</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="KBQF7w" Title="NO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="0opMrx">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.311535\39.5987\8.64026\5.35063
0.00432687\0.549982\0.120004\0.0743143
0.432687\54.9982\12.0004\7.43143</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="719cdV" Title="SO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="sBZUSl">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0386971 and its percentage error 3.86993</Caption>
   <Data>0.00693226\14.5823\1.31707\0.949412
0.00040778\0.857783\0.0774749\0.0558478
0.040778\85.7783\7.74749\5.58478</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="nZQ4vX" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="nDuGJT" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 688.</Text>
 </Task>
 <Task Id="HaSC2R" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="05X72N" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="z71Dmr" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="KzNP7v" Title="Quasi-Newton method errors history">
   <Caption Id="2rY9lt">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.10718, and the final value after 900 epochs is 0.814107.
The initial value of the selection error is 0.805137, and the final value after 900 epochs is 0.656925.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661\662\663\664\665\666\667\668\669\670\671\672\673\674\675\676\677\678\679\680\681\682\683\684\685\686\687\688\689\690\691\692\693\694\695\696\697\698\699\700\701\702\703\704\705\706\707\708\709\710\711\712\713\714\715\716\717\718\719\720\721\722\723\724\725\726\727\728\729\730\731\732\733\734\735\736\737\738\739\740\741\742\743\744\745\746\747\748\749\750\751\752\753\754\755\756\757\758\759\760\761\762\763\764\765\766\767\768\769\770\771\772\773\774\775\776\777\778\779\780\781\782\783\784\785\786\787\788\789\790\791\792\793\794\795\796\797\798\799\800\801\802\803\804\805\806\807\808\809\810\811\812\813\814\815\816\817\818\819\820\821\822\823\824\825\826\827\828\829\830\831\832\833\834\835\836\837\838\839\840\841\842\843\844\845\846\847\848\849\850\851\852\853\854\855\856\857\858\859\860\861\862\863\864\865\866\867\868\869\870\871\872\873\874\875\876\877\878\879\880\881\882\883\884\885\886\887\888\889\890\891\892\893\894\895\896\897\898\899\900</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406\407\408\409\410\411\412\413\414\415\416\417\418\419\420\421\422\423\424\425\426\427\428\429\430\431\432\433\434\435\436\437\438\439\440\441\442\443\444\445\446\447\448\449\450\451\452\453\454\455\456\457\458\459\460\461\462\463\464\465\466\467\468\469\470\471\472\473\474\475\476\477\478\479\480\481\482\483\484\485\486\487\488\489\490\491\492\493\494\495\496\497\498\499\500\501\502\503\504\505\506\507\508\509\510\511\512\513\514\515\516\517\518\519\520\521\522\523\524\525\526\527\528\529\530\531\532\533\534\535\536\537\538\539\540\541\542\543\544\545\546\547\548\549\550\551\552\553\554\555\556\557\558\559\560\561\562\563\564\565\566\567\568\569\570\571\572\573\574\575\576\577\578\579\580\581\582\583\584\585\586\587\588\589\590\591\592\593\594\595\596\597\598\599\600\601\602\603\604\605\606\607\608\609\610\611\612\613\614\615\616\617\618\619\620\621\622\623\624\625\626\627\628\629\630\631\632\633\634\635\636\637\638\639\640\641\642\643\644\645\646\647\648\649\650\651\652\653\654\655\656\657\658\659\660\661\662\663\664\665\666\667\668\669\670\671\672\673\674\675\676\677\678\679\680\681\682\683\684\685\686\687\688\689\690\691\692\693\694\695\696\697\698\699\700\701\702\703\704\705\706\707\708\709\710\711\712\713\714\715\716\717\718\719\720\721\722\723\724\725\726\727\728\729\730\731\732\733\734\735\736\737\738\739\740\741\742\743\744\745\746\747\748\749\750\751\752\753\754\755\756\757\758\759\760\761\762\763\764\765\766\767\768\769\770\771\772\773\774\775\776\777\778\779\780\781\782\783\784\785\786\787\788\789\790\791\792\793\794\795\796\797\798\799\800\801\802\803\804\805\806\807\808\809\810\811\812\813\814\815\816\817\818\819\820\821\822\823\824\825\826\827\828\829\830\831\832\833\834\835\836\837\838\839\840\841\842\843\844\845\846\847\848\849\850\851\852\853\854\855\856\857\858\859\860\861\862\863\864\865\866\867\868\869\870\871\872\873\874\875\876\877\878\879\880\881\882\883\884\885\886\887\888\889\890\891\892\893\894\895\896\897\898\899\900</X2Data>
   <Y1Data>1.11\1.02\0.915\0.886\0.882\0.881\0.886\0.89\0.894\0.895\0.896\0.897\0.897\0.898\0.899\0.899\0.9\0.901\0.901\0.902\0.902\0.902\0.902\0.902\0.901\0.901\0.901\0.9\0.899\0.899\0.898\0.897\0.896\0.895\0.894\0.893\0.893\0.892\0.892\0.891\0.891\0.89\0.89\0.89\0.889\0.889\0.888\0.888\0.888\0.887\0.887\0.886\0.885\0.885\0.885\0.884\0.884\0.884\0.883\0.883\0.882\0.882\0.881\0.881\0.88\0.88\0.88\0.879\0.879\0.879\0.878\0.878\0.878\0.877\0.877\0.877\0.877\0.876\0.876\0.876\0.876\0.875\0.875\0.875\0.875\0.875\0.875\0.874\0.874\0.874\0.874\0.874\0.874\0.873\0.873\0.873\0.873\0.872\0.872\0.872\0.872\0.872\0.871\0.871\0.871\0.871\0.871\0.87\0.87\0.87\0.87\0.869\0.869\0.869\0.869\0.868\0.868\0.868\0.868\0.867\0.867\0.866\0.865\0.864\0.862\0.862\0.861\0.86\0.859\0.858\0.856\0.855\0.854\0.852\0.851\0.85\0.849\0.848\0.846\0.845\0.844\0.843\0.842\0.842\0.841\0.841\0.84\0.84\0.839\0.839\0.838\0.838\0.838\0.838\0.838\0.837\0.837\0.837\0.837\0.837\0.837\0.837\0.837\0.837\0.837\0.837\0.837\0.837\0.837\0.837\0.837\0.836\0.836\0.836\0.836\0.836\0.836\0.836\0.835\0.835\0.835\0.835\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.834\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.833\0.832\0.832\0.832\0.832\0.832\0.832\0.832\0.832\0.831\0.831\0.831\0.831\0.831\0.831\0.831\0.831\0.831\0.83\0.83\0.83\0.83\0.83\0.83\0.83\0.83\0.83\0.83\0.829\0.829\0.829\0.829\0.829\0.829\0.829\0.829\0.828\0.828\0.828\0.828\0.828\0.828\0.827\0.827\0.827\0.827\0.827\0.827\0.827\0.827\0.827\0.826\0.826\0.826\0.826\0.826\0.826\0.826\0.825\0.825\0.825\0.825\0.825\0.825\0.824\0.824\0.824\0.824\0.824\0.824\0.824\0.824\0.824\0.823\0.823\0.823\0.823\0.823\0.823\0.823\0.823\0.823\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.823\0.823\0.823\0.823\0.823\0.823\0.823\0.823\0.823\0.824\0.824\0.824\0.824\0.824\0.824\0.824\0.824\0.824\0.824\0.824\0.824\0.823\0.823\0.823\0.823\0.823\0.823\0.823\0.823\0.823\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.821\0.821\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.822\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.821\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.82\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.819\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.818\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.817\0.816\0.816\0.816\0.816\0.816\0.816\0.816\0.816\0.816\0.816\0.816\0.816\0.816\0.816\0.815\0.815\0.815\0.815\0.815\0.815\0.815\0.815\0.815\0.815\0.815\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814\0.814</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.805\0.77\0.725\0.704\0.701\0.703\0.707\0.71\0.712\0.713\0.714\0.714\0.715\0.716\0.717\0.718\0.718\0.719\0.719\0.72\0.72\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.72\0.72\0.72\0.719\0.719\0.719\0.718\0.718\0.717\0.717\0.717\0.717\0.717\0.716\0.716\0.716\0.716\0.716\0.715\0.715\0.715\0.715\0.714\0.714\0.714\0.714\0.713\0.713\0.713\0.713\0.713\0.713\0.712\0.712\0.712\0.712\0.712\0.711\0.711\0.711\0.711\0.711\0.711\0.711\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.71\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.709\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.708\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.707\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.706\0.705\0.705\0.705\0.705\0.704\0.704\0.704\0.703\0.702\0.702\0.701\0.701\0.7\0.7\0.699\0.698\0.697\0.696\0.696\0.695\0.695\0.694\0.693\0.692\0.692\0.691\0.691\0.69\0.69\0.69\0.689\0.689\0.689\0.689\0.689\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.684\0.684\0.684\0.684\0.684\0.684\0.684\0.684\0.684\0.684\0.683\0.683\0.683\0.683\0.683\0.683\0.683\0.683\0.683\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.682\0.681\0.681\0.681\0.681\0.681\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.68\0.679\0.679\0.679\0.679\0.679\0.678\0.678\0.678\0.678\0.678\0.678\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.677\0.676\0.676\0.676\0.676\0.676\0.676\0.676\0.676\0.676\0.675\0.675\0.675\0.675\0.674\0.674\0.674\0.674\0.674\0.674\0.673\0.673\0.673\0.672\0.672\0.671\0.671\0.67\0.67\0.67\0.67\0.67\0.67\0.67\0.669\0.669\0.669\0.669\0.669\0.669\0.669\0.668\0.668\0.668\0.668\0.668\0.668\0.668\0.667\0.667\0.667\0.667\0.667\0.667\0.667\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.663\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.662\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.661\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.66\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.659\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.657\0.657\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.658\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657\0.657</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>901</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="FzUBPw" Title="Quasi-Newton method results">
   <Caption Id="TLCAMz">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.814
0.657
900
00:00:15
Minimum loss decrease
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="1bLiCj" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="rXYpzK" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="0GDu5I" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="DkOWoS">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.414326\111.585\16.4024\13.9908
0.00265593\0.715291\0.105144\0.0896847
0.265593\71.5291\10.5144\8.96847</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="pfg3hW" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="a2EPLA">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.407003\319.407\10.1109\15.3042
0.00119707\0.939432\0.029738\0.0450123
0.119707\93.9432\2.9738\4.50123</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="6Y3TJf" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="NktLih">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.0129585\22.0373\4.88157\4.10451
5.22521e-5\0.0888602\0.0196837\0.0165504
0.00522521\8.88602\1.96837\1.65504</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="a4QGB6" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="3fHec4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.121428\39.739\8.61583\5.30843
0.00168649\0.551931\0.119664\0.0737281
0.168649\55.1931\11.9664\7.37282</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="kfBb2m" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="xBNMw9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.0019021\15.2881\0.818691\0.923762
0.000111888\0.899301\0.0481583\0.054339
0.0111888\89.9301\4.81583\5.4339</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="DlXGpj" Title="PM2.5(microg/m3)_ahead_2 errors statistics">
   <Caption Id="YADmwl">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.4146\111.585\16.4162\13.9881
0.00265769\0.715291\0.105232\0.0896673
0.265769\71.5291\10.5232\8.96673</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Js1zwF" Title="PM10(microg/m3)_ahead_2 errors statistics">
   <Caption Id="Zt5UKP">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.404797\319.405\10.1186\15.3022
0.00119058\0.939426\0.0297605\0.0450065
0.119058\93.9426\2.97605\4.50065</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="8kJtYj" Title="O3(microg/m3)_ahead_2 errors statistics">
   <Caption Id="Ox6gNC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.000423431\26.2098\5.17615\4.44946
1.70738e-6\0.105685\0.0208716\0.0179414
0.000170738\10.5685\2.08716\1.79414</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="1M199j" Title="NO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="7FIrxK">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.00893211\38.7423\8.62652\5.3696
0.000124057\0.538088\0.119813\0.0745778
0.0124057\53.8088\11.9813\7.45778</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="JzVsg4" Title="SO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="LyapVg">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.000190258\15.2946\0.961516\0.973456
1.11916e-5\0.899684\0.0565598\0.0572621
0.00111916\89.9684\5.65598\5.72621</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ky1jeV" Title="PM2.5(microg/m3)_ahead_3 errors statistics">
   <Caption Id="qynxbM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.420612\111.579\16.4026\13.9882
0.00269623\0.715252\0.105145\0.0896681
0.269623\71.5252\10.5145\8.96681</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="I9oQJF" Title="PM10(microg/m3)_ahead_3 errors statistics">
   <Caption Id="pQlUHY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.401697\319.402\10.114\15.3046
0.00118146\0.939417\0.029747\0.0450135
0.118146\93.9417\2.9747\4.50135</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="BGqCE0" Title="O3(microg/m3)_ahead_3 errors statistics">
   <Caption Id="706C99">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.00191879\23.324\5.30018\4.4824
7.73707e-6\0.0940483\0.0213717\0.0180742
0.000773707\9.40483\2.13717\1.80742</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="tJjAUU" Title="NO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="QwxKm2">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.00823784\38.5439\8.60396\5.37088
0.000114414\0.535332\0.119499\0.0745956
0.0114414\53.5332\11.9499\7.45956</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Y1HPSr" Title="SO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="CZhh4R">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.00403571\15.1455\1.03415\0.982052
0.000237395\0.890913\0.0608325\0.0577678
0.0237395\89.0912\6.08325\5.77678</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ns51tI" Title="PM2.5(microg/m3)_ahead_4 errors statistics">
   <Caption Id="kp9jMX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.428898\111.571\16.4124\13.9785
0.00274935\0.715199\0.105208\0.0896059
0.274935\71.5199\10.5208\8.96059</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="cFuqG5" Title="PM10(microg/m3)_ahead_4 errors statistics">
   <Caption Id="jbIRCs">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.400131\319.4\10.0933\15.3078
0.00117686\0.939413\0.0296861\0.0450231
0.117686\93.9413\2.96861\4.50231</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uCZCWE" Title="O3(microg/m3)_ahead_4 errors statistics">
   <Caption Id="WrHwPT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.0229301\23.293\5.36494\4.50299
9.24603e-5\0.0939233\0.0216328\0.0181572
0.00924603\9.39233\2.16328\1.81572</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="QP4k5N" Title="NO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="UtfhaW">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.0414295\38.6765\8.5993\5.36331
0.00057541\0.537173\0.119435\0.0744904
0.057541\53.7173\11.9435\7.44904</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="7Es9x2" Title="SO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="7hRc6u">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.000368834\15.0627\1.08796\0.972223
2.16961e-5\0.886039\0.0639975\0.0571896
0.00216961\88.6039\6.39975\5.71896</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="wWMY2p" Title="PM2.5(microg/m3)_ahead_5 errors statistics">
   <Caption Id="VmcO0I">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.431461\111.569\16.3848\13.9861
0.00276578\0.715183\0.105031\0.0896545
0.276578\71.5183\10.5031\8.96545</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="CsJIIB" Title="PM10(microg/m3)_ahead_5 errors statistics">
   <Caption Id="Hjn4KQ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.400702\319.401\10.0793\15.3102
0.00117853\0.939414\0.029645\0.0450299
0.117853\93.9414\2.9645\4.50299</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="vaLjho" Title="O3(microg/m3)_ahead_5 errors statistics">
   <Caption Id="zjO1TX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.00254822\23.3574\5.26303\4.57967
1.02751e-5\0.0941831\0.0212219\0.0184664
0.00102751\9.41831\2.12219\1.84664</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="FZBTyy" Title="NO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="hDKtWq">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.000440598\38.5461\8.57232\5.3713
6.11941e-6\0.535362\0.11906\0.0746013
0.000611941\53.5362\11.906\7.46013</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="9hw9Oy" Title="SO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="fJF8IB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.00629294\15.008\1.13843\0.965467
0.000370173\0.882822\0.0669662\0.0567922
0.0370173\88.2822\6.69662\5.67922</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="BQsvKq" Title="PM2.5(microg/m3)_ahead_6 errors statistics">
   <Caption Id="weQoHk">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.419498\111.581\16.3696\13.9926
0.00268909\0.71526\0.104934\0.0896964
0.268909\71.526\10.4934\8.96964</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="WZSA75" Title="PM10(microg/m3)_ahead_6 errors statistics">
   <Caption Id="CL23FM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.400974\319.401\10.0705\15.3142
0.00117934\0.939415\0.0296192\0.0450417
0.117934\93.9415\2.96192\4.50417</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="KnSIX1" Title="O3(microg/m3)_ahead_6 errors statistics">
   <Caption Id="gdi7pJ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.00149536\24.3291\5.34184\4.64644
6.02968e-6\0.0981011\0.0215397\0.0187357
0.000602968\9.81011\2.15397\1.87357</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="p04et9" Title="NO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="aRwMnD">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.0146103\38.6401\8.57028\5.38227
0.000202921\0.536668\0.119032\0.0747537
0.0202921\53.6668\11.9032\7.47537</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="WePhIM" Title="SO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="doLQA1">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.00594902\14.7926\1.1644\0.959708
0.000349942\0.870155\0.0684942\0.0564534
0.0349942\87.0155\6.84942\5.64534</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="TJt7w5" Title="PM2.5(microg/m3)_ahead_7 errors statistics">
   <Caption Id="uOXeJE">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.413647\111.586\16.3673\13.995
0.00265158\0.715297\0.104918\0.0897113
0.265158\71.5297\10.4918\8.97113</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="pkn0QK" Title="PM10(microg/m3)_ahead_7 errors statistics">
   <Caption Id="wnNoCW">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.399298\319.399\10.0932\15.3153
0.0011744\0.93941\0.0296858\0.045045
0.117441\93.941\2.96858\4.5045</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Ky8rWF" Title="O3(microg/m3)_ahead_7 errors statistics">
   <Caption Id="FImqxT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.006464\25.8183\5.41434\4.71598
2.60645e-5\0.104106\0.021832\0.019016
0.00260645\10.4106\2.1832\1.9016</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="J36KeJ" Title="NO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="tPhEZ3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.056057\38.7816\8.58703\5.37169
0.000778569\0.538634\0.119264\0.0746068
0.0778569\53.8634\11.9264\7.46068</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="lgMlol" Title="SO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="VThSbD">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.00511885\14.3699\1.18046\0.939926
0.000301109\0.845286\0.0694387\0.0552898
0.0301109\84.5286\6.94387\5.52898</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="mfb1Ll" Title="PM2.5(microg/m3)_ahead_8 errors statistics">
   <Caption Id="C19FsM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.4104\111.59\16.4\13.9805
0.00263077\0.715318\0.105128\0.0896184
0.263077\71.5318\10.5128\8.96184</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="TgBarm" Title="PM10(microg/m3)_ahead_8 errors statistics">
   <Caption Id="4qMxAH">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.395708\319.396\10.1091\15.3178
0.00116385\0.9394\0.0297326\0.0450523
0.116385\93.94\2.97326\4.50523</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="IDnQuC" Title="O3(microg/m3)_ahead_8 errors statistics">
   <Caption Id="zYLa5I">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.0306358\23.7916\5.49393\4.65925
0.000123532\0.0959339\0.0221529\0.0187873
0.0123532\9.59339\2.21529\1.87873</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Zuct3u" Title="NO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="bwsoof">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.0561447\38.5242\8.59371\5.36321
0.000779788\0.535058\0.119357\0.0744891
0.0779788\53.5058\11.9357\7.44891</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="v4KF9s" Title="SO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="hUbxPr">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0391406 and its percentage error 3.91421</Caption>
   <Data>0.00263095\14.6422\1.21998\0.947213
0.000154762\0.861305\0.0717636\0.0557184
0.0154762\86.1305\7.17636\5.57184</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="zs4iqC" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="lRC2tH" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 1080.</Text>
 </Task>
 <Task Id="w0vciY" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="qPhavJ" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="HnIqqb" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="X9VfEo" Title="Quasi-Newton method errors history">
   <Caption Id="4oFkS5">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.02388, and the final value after 328 epochs is 0.467813.
The initial value of the selection error is 0.989066, and the final value after 328 epochs is 0.531962.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328</X2Data>
   <Y1Data>1.02\0.938\0.806\0.754\0.679\0.651\0.638\0.618\0.608\0.597\0.594\0.588\0.585\0.582\0.58\0.576\0.574\0.571\0.568\0.565\0.562\0.56\0.557\0.554\0.551\0.548\0.546\0.543\0.54\0.538\0.535\0.531\0.528\0.525\0.521\0.518\0.515\0.512\0.51\0.507\0.505\0.503\0.502\0.501\0.499\0.498\0.498\0.497\0.496\0.495\0.495\0.494\0.493\0.493\0.491\0.49\0.489\0.488\0.486\0.485\0.484\0.482\0.481\0.48\0.479\0.479\0.478\0.477\0.477\0.477\0.477\0.476\0.476\0.476\0.477\0.477\0.477\0.477\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.477\0.477\0.477\0.477\0.477\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.473\0.473\0.473\0.473\0.473\0.473\0.473\0.473\0.473\0.473\0.473\0.472\0.472\0.472\0.472\0.472\0.472\0.472\0.472\0.472\0.472\0.472\0.472\0.472\0.472\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.471\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.47\0.469\0.469\0.469\0.469\0.469\0.469\0.469\0.469\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468\0.468</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.989\0.915\0.825\0.775\0.676\0.646\0.647\0.63\0.626\0.616\0.613\0.607\0.606\0.606\0.605\0.603\0.602\0.6\0.597\0.594\0.592\0.59\0.588\0.585\0.583\0.581\0.58\0.578\0.577\0.575\0.573\0.572\0.569\0.567\0.564\0.562\0.56\0.557\0.556\0.554\0.552\0.55\0.549\0.548\0.547\0.547\0.546\0.545\0.545\0.545\0.545\0.544\0.544\0.544\0.543\0.542\0.541\0.541\0.54\0.539\0.538\0.537\0.537\0.537\0.536\0.536\0.536\0.536\0.536\0.536\0.536\0.536\0.536\0.536\0.536\0.536\0.537\0.537\0.537\0.538\0.538\0.538\0.539\0.539\0.539\0.539\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.54\0.539\0.539\0.539\0.539\0.539\0.539\0.539\0.539\0.539\0.539\0.539\0.539\0.539\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.538\0.537\0.537\0.537\0.537\0.537\0.537\0.537\0.537\0.536\0.536\0.536\0.536\0.536\0.536\0.536\0.535\0.535\0.535\0.535\0.535\0.535\0.535\0.535\0.535\0.535\0.535\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.534\0.533\0.533\0.533\0.533\0.533\0.533\0.533\0.533\0.533\0.532\0.532\0.532\0.532\0.532\0.532\0.532\0.532\0.532\0.532\0.532\0.532\0.532\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.53\0.53\0.53\0.53\0.53\0.53\0.53\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.531\0.532\0.532\0.532\0.532\0.532\0.532\0.532\0.532\0.532</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>329</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="CV2FHP" Title="Quasi-Newton method results">
   <Caption Id="4G6KgO">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.468
0.532
328
00:00:08
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="EGddei" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="RchylI" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="EQQk0T" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="xKN1t4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0723756 and its percentage error 7.23754</Caption>
   <Data>0.0019455\108.41\7.33704\9.17953
1.24711e-5\0.694935\0.0470323\0.0588432
0.00124711\69.4935\4.70323\5.88432</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="kxoGoM" Title="PM2.5(microg/m3)_ahead_2 errors statistics">
   <Caption Id="BHguwQ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0723756 and its percentage error 7.23754</Caption>
   <Data>0.0291252\98.6566\11.2113\11.9195
0.0001867\0.632414\0.0718672\0.076407
0.01867\63.2414\7.18672\7.6407</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="uZyE3A" Title="PM2.5(microg/m3)_ahead_3 errors statistics">
   <Caption Id="X5tdPi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0723756 and its percentage error 7.23754</Caption>
   <Data>0.0165863\121.284\13.3236\12.9572
0.000106322\0.77746\0.0854078\0.0830587
0.0106322\77.746\8.54078\8.30587</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="nPH4E8" Title="PM2.5(microg/m3)_ahead_4 errors statistics">
   <Caption Id="QtspOb">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0723756 and its percentage error 7.23754</Caption>
   <Data>0.0624123\116.732\13.8217\13.1223
0.000400079\0.748281\0.0886008\0.0841171
0.0400079\74.8281\8.86008\8.41171</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ty2clI" Title="WINDSPEED(km/h)_ahead_4 errors statistics">
   <Caption Id="GZKDEw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0723756 and its percentage error 7.23754</Caption>
   <Data>0.000152588\14.5148\2.79505\2.39403
4.44863e-6\0.423172\0.0814885\0.0697967
0.000444863\42.3172\8.14884\6.97967</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="HVauT3" Title="PM2.5(microg/m3)_ahead_5 errors statistics">
   <Caption Id="yACPbz">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0723756 and its percentage error 7.23754</Caption>
   <Data>0.0198555\117.662\13.9701\13.0224
0.000127279\0.75424\0.0895521\0.0834767
0.0127279\75.424\8.95521\8.34766</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="L6ZOIT" Title="PM2.5(microg/m3)_ahead_6 errors statistics">
   <Caption Id="PIXcvj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0723756 and its percentage error 7.23754</Caption>
   <Data>0.0202866\113.227\14.2402\13.0172
0.000130042\0.725812\0.0912835\0.0834438
0.0130042\72.5812\9.12835\8.34438</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="5DFkXQ" Title="PM2.5(microg/m3)_ahead_7 errors statistics">
   <Caption Id="zTu0WV">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0723756 and its percentage error 7.23754</Caption>
   <Data>0.00646973\116.258\13.9155\13.2321
4.14726e-5\0.745243\0.0892017\0.0848213
0.00414726\74.5243\8.92017\8.48213</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="r8fQHH" Title="PM2.5(microg/m3)_ahead_8 errors statistics">
   <Caption Id="gZUmqB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0723756 and its percentage error 7.23754</Caption>
   <Data>0.0134773\122.601\14.201\13.3172
8.63931e-5\0.785901\0.0910322\0.0853665
0.00863931\78.5901\9.10322\8.53665</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ADDjxE" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="dixsnD" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 1079.</Text>
 </Task>
 <Task Id="E80kOv" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="RL9HTm" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="FOj6BM" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="WzjbdX" Title="Quasi-Newton method errors history">
   <Caption Id="WaggAH">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.87568, and the final value after 206 epochs is 0.474115.
The initial value of the selection error is 0.626945, and the final value after 206 epochs is 0.409297.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206</X2Data>
   <Y1Data>0.876\0.752\0.667\0.657\0.604\0.59\0.567\0.555\0.554\0.551\0.548\0.548\0.546\0.544\0.542\0.54\0.539\0.537\0.535\0.533\0.532\0.531\0.529\0.528\0.526\0.525\0.524\0.523\0.522\0.521\0.52\0.518\0.517\0.516\0.515\0.514\0.514\0.513\0.512\0.511\0.511\0.51\0.51\0.509\0.509\0.508\0.508\0.507\0.507\0.507\0.506\0.506\0.506\0.505\0.505\0.504\0.504\0.503\0.503\0.503\0.502\0.502\0.501\0.501\0.501\0.5\0.5\0.5\0.499\0.499\0.498\0.498\0.498\0.497\0.497\0.497\0.497\0.496\0.496\0.496\0.496\0.496\0.495\0.495\0.495\0.495\0.495\0.495\0.495\0.494\0.494\0.494\0.494\0.494\0.494\0.494\0.494\0.494\0.493\0.493\0.493\0.493\0.493\0.493\0.493\0.492\0.492\0.492\0.492\0.492\0.492\0.491\0.491\0.491\0.49\0.49\0.489\0.489\0.489\0.489\0.488\0.488\0.488\0.488\0.487\0.487\0.487\0.487\0.487\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.486\0.485\0.485\0.485\0.485\0.485\0.485\0.485\0.484\0.484\0.484\0.484\0.483\0.483\0.482\0.482\0.482\0.481\0.481\0.481\0.48\0.48\0.479\0.479\0.478\0.478\0.478\0.477\0.477\0.476\0.476\0.476\0.476\0.475\0.475\0.475\0.475\0.475\0.475\0.474\0.474\0.474\0.474</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.627\0.584\0.589\0.5\0.451\0.454\0.432\0.424\0.415\0.414\0.425\0.429\0.43\0.43\0.427\0.425\0.423\0.422\0.421\0.421\0.421\0.421\0.422\0.422\0.422\0.423\0.423\0.423\0.423\0.423\0.423\0.422\0.422\0.421\0.421\0.42\0.42\0.419\0.419\0.418\0.418\0.417\0.417\0.416\0.416\0.415\0.415\0.415\0.415\0.415\0.415\0.415\0.415\0.415\0.415\0.416\0.416\0.416\0.416\0.417\0.417\0.417\0.417\0.417\0.417\0.417\0.417\0.417\0.417\0.416\0.416\0.416\0.415\0.415\0.415\0.414\0.414\0.414\0.414\0.413\0.413\0.413\0.412\0.412\0.412\0.411\0.411\0.41\0.41\0.41\0.409\0.409\0.409\0.408\0.408\0.408\0.408\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.406\0.406\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.407\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.409\0.409\0.409\0.409\0.409\0.409\0.409\0.409\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.408\0.409\0.409\0.409\0.409\0.409\0.409\0.409\0.409</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>207</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="Vc5rfw" Title="Quasi-Newton method results">
   <Caption Id="A4PZYu">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.474
0.409
206
00:00:05
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="LMvVFi" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="PqSOZC" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="S7K75j" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="bkLVKx">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.0154686\312.578\7.00139\14.4141
4.54959e-5\0.919348\0.0205923\0.0423944
0.00454959\91.9348\2.05923\4.23944</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="1aiq22" Title="PM10(microg/m3)_ahead_2 errors statistics">
   <Caption Id="3o5J4V">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.0105991\326.709\7.66264\15.2006
3.11739e-5\0.960908\0.0225372\0.0447075
0.00311739\96.0908\2.25372\4.47075</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="QsWm1N" Title="PM10(microg/m3)_ahead_3 errors statistics">
   <Caption Id="Fq8g13">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.0149517\327.776\8.19814\15.4101
4.39756e-5\0.964047\0.0241122\0.0453238
0.00439756\96.4047\2.41122\4.53238</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="kBxSzV" Title="PM10(microg/m3)_ahead_4 errors statistics">
   <Caption Id="eeCd0I">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.0240135\324.982\8.39331\15.3129
7.0628e-5\0.955831\0.0246862\0.0450378
0.0070628\95.5831\2.46862\4.50378</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="3IEemf" Title="PM10(microg/m3)_ahead_5 errors statistics">
   <Caption Id="ALCa8z">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.00916672\321.166\8.11499\15.1383
2.69609e-5\0.944605\0.0238676\0.0445244
0.00269609\94.4605\2.38676\4.45244</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="yppVHV" Title="PM10(microg/m3)_ahead_6 errors statistics">
   <Caption Id="FiC4TN">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.000265121\327.892\8.3006\15.4337
7.79769e-7\0.964388\0.0244135\0.0453932
7.79769e-5\96.4388\2.44135\4.53932</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="MgKYBO" Title="PM10(microg/m3)_ahead_7 errors statistics">
   <Caption Id="Js2mHJ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.00313759\328.694\8.27143\15.4485
9.2282e-6\0.966748\0.0243277\0.0454366
0.00092282\96.6748\2.43277\4.54366</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="u94ze5" Title="PM10(microg/m3)_ahead_8 errors statistics">
   <Caption Id="zNZ0Ah">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.0430412\324.139\8.65388\15.3675
0.000126592\0.953351\0.0254526\0.0451987
0.0126592\95.3351\2.54526\4.51987</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="s1KMUC" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="jFcf0d" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 1079.</Text>
 </Task>
 <Task Id="iBSpsd" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="jeMdEf" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="reODem" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="LQ7Ntk" Title="Quasi-Newton method errors history">
   <Caption Id="Wz894j">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.15318, and the final value after 406 epochs is 0.549942.
The initial value of the selection error is 0.739393, and the final value after 406 epochs is 0.299872.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303\304\305\306\307\308\309\310\311\312\313\314\315\316\317\318\319\320\321\322\323\324\325\326\327\328\329\330\331\332\333\334\335\336\337\338\339\340\341\342\343\344\345\346\347\348\349\350\351\352\353\354\355\356\357\358\359\360\361\362\363\364\365\366\367\368\369\370\371\372\373\374\375\376\377\378\379\380\381\382\383\384\385\386\387\388\389\390\391\392\393\394\395\396\397\398\399\400\401\402\403\404\405\406</X2Data>
   <Y1Data>1.15\0.709\0.658\0.678\0.637\0.603\0.605\0.583\0.58\0.577\0.578\0.58\0.583\0.583\0.583\0.582\0.581\0.58\0.579\0.577\0.576\0.576\0.575\0.575\0.574\0.574\0.574\0.574\0.574\0.574\0.574\0.574\0.574\0.575\0.574\0.574\0.574\0.574\0.574\0.574\0.574\0.574\0.574\0.574\0.574\0.573\0.573\0.573\0.573\0.573\0.573\0.573\0.572\0.572\0.572\0.572\0.572\0.572\0.571\0.571\0.571\0.571\0.571\0.571\0.571\0.57\0.57\0.57\0.57\0.57\0.57\0.57\0.57\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.569\0.568\0.568\0.568\0.568\0.568\0.568\0.568\0.567\0.567\0.567\0.567\0.566\0.566\0.566\0.565\0.565\0.565\0.564\0.564\0.564\0.563\0.563\0.563\0.563\0.563\0.563\0.563\0.563\0.563\0.563\0.563\0.563\0.564\0.564\0.564\0.564\0.564\0.564\0.564\0.564\0.564\0.563\0.563\0.563\0.563\0.562\0.562\0.562\0.561\0.561\0.561\0.56\0.56\0.56\0.559\0.559\0.559\0.558\0.558\0.558\0.558\0.558\0.557\0.557\0.557\0.557\0.557\0.557\0.557\0.557\0.556\0.556\0.556\0.556\0.556\0.556\0.556\0.556\0.556\0.556\0.556\0.556\0.556\0.556\0.556\0.555\0.555\0.555\0.555\0.555\0.555\0.555\0.554\0.554\0.554\0.554\0.554\0.554\0.554\0.554\0.554\0.554\0.554\0.554\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.553\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.552\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.551\0.55\0.55\0.55\0.55\0.55\0.55\0.55\0.55</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.739\0.487\0.452\0.369\0.346\0.343\0.33\0.341\0.334\0.335\0.332\0.329\0.326\0.325\0.325\0.324\0.324\0.323\0.323\0.323\0.323\0.322\0.322\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.319\0.319\0.319\0.319\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.318\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.317\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.315\0.315\0.315\0.315\0.315\0.315\0.314\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.309\0.309\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.307\0.306\0.306\0.306\0.306\0.305\0.305\0.305\0.305\0.305\0.305\0.305\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.304\0.303\0.303\0.303\0.303\0.303\0.303\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.302\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.301\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.299\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3\0.3</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>407</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="v6A6Wh" Title="Quasi-Newton method results">
   <Caption Id="qzWFCY">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.55
0.3
406
00:00:09
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="27DzUZ" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="IjlbwQ" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="cqIusb" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="2INWvG">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.00998306\19.4848\4.34874\3.3463
4.02543e-5\0.0785678\0.0175352\0.0134931
0.00402543\7.85678\1.75352\1.34931</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="aJDaPS" Title="O3(microg/m3)_ahead_2 errors statistics">
   <Caption Id="B7Tj1x">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.000297546\24.4071\4.69619\3.7007
1.19978e-6\0.0984157\0.0189363\0.0149222
0.000119978\9.84157\1.89363\1.49222</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="gwja0J" Title="O3(microg/m3)_ahead_3 errors statistics">
   <Caption Id="0cQOde">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.0234947\23.4852\4.88049\3.78056
9.47368e-5\0.0946985\0.0196794\0.0152442
0.00947368\9.46985\1.96794\1.52442</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="cJAuiN" Title="O3(microg/m3)_ahead_4 errors statistics">
   <Caption Id="lmguMk">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.0404816\23.4304\4.90529\3.77149
0.000163232\0.0944773\0.0197794\0.0152076
0.0163232\9.44773\1.97794\1.52076</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4VHFLr" Title="O3(microg/m3)_ahead_5 errors statistics">
   <Caption Id="m2Ezfv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.0515099\23.9654\4.95228\3.67545
0.000207701\0.0966346\0.0199689\0.0148204
0.0207701\9.66346\1.99689\1.48204</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="d8XVIl" Title="O3(microg/m3)_ahead_6 errors statistics">
   <Caption Id="IzJDHL">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.0106983\22.9348\4.84567\3.60793
4.31384e-5\0.0924792\0.019539\0.0145481
0.00431384\9.24792\1.9539\1.45481</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="l7ZcgM" Title="O3(microg/m3)_ahead_7 errors statistics">
   <Caption Id="sD4tEF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.00427246\24.0609\5.22133\3.89221
1.72277e-5\0.0970196\0.0210538\0.0156944
0.00172277\9.70196\2.10538\1.56944</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="WVbSRK" Title="O3(microg/m3)_ahead_8 errors statistics">
   <Caption Id="wEcnYC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.00204468\26.4036\5.43286\4.15329
8.24467e-6\0.106466\0.0219067\0.0167471
0.000824467\10.6466\2.19067\1.67471</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="I1SGI2" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="5UIAgA" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 1079.</Text>
 </Task>
 <Task Id="wArYPx" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="U0jyXy" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="5VKcy2" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="dDkrQ6" Title="Quasi-Newton method errors history">
   <Caption Id="WcIeKP">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.14972, and the final value after 303 epochs is 0.457278.
The initial value of the selection error is 1.14053, and the final value after 303 epochs is 0.720513.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293\294\295\296\297\298\299\300\301\302\303</X2Data>
   <Y1Data>1.15\0.781\0.673\0.667\0.642\0.652\0.643\0.611\0.594\0.584\0.58\0.576\0.574\0.57\0.566\0.562\0.557\0.551\0.547\0.542\0.538\0.535\0.531\0.528\0.525\0.523\0.52\0.518\0.516\0.514\0.512\0.511\0.509\0.508\0.506\0.505\0.504\0.503\0.502\0.501\0.501\0.5\0.5\0.499\0.499\0.499\0.498\0.498\0.498\0.497\0.497\0.497\0.496\0.496\0.495\0.495\0.494\0.493\0.493\0.492\0.492\0.491\0.49\0.49\0.489\0.488\0.488\0.487\0.486\0.486\0.485\0.485\0.484\0.484\0.483\0.483\0.483\0.483\0.482\0.482\0.482\0.482\0.482\0.482\0.481\0.481\0.481\0.481\0.481\0.481\0.48\0.48\0.48\0.48\0.479\0.479\0.479\0.479\0.479\0.478\0.478\0.478\0.478\0.478\0.478\0.477\0.477\0.477\0.477\0.477\0.476\0.476\0.476\0.476\0.476\0.475\0.475\0.475\0.475\0.475\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.474\0.473\0.473\0.473\0.473\0.473\0.473\0.473\0.473\0.473\0.473\0.473\0.472\0.472\0.472\0.472\0.472\0.472\0.472\0.472\0.471\0.471\0.471\0.471\0.471\0.47\0.47\0.47\0.47\0.47\0.47\0.469\0.469\0.469\0.468\0.468\0.468\0.467\0.467\0.466\0.466\0.465\0.465\0.464\0.464\0.464\0.463\0.463\0.463\0.462\0.462\0.462\0.462\0.461\0.461\0.461\0.461\0.461\0.46\0.46\0.46\0.46\0.46\0.46\0.46\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.459\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.458\0.457\0.457\0.457\0.457\0.457\0.457\0.457\0.457\0.457\0.457\0.457</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.14\0.995\0.95\0.875\0.994\0.936\0.907\0.895\0.889\0.918\0.914\0.908\0.888\0.88\0.874\0.868\0.861\0.854\0.85\0.846\0.84\0.833\0.827\0.819\0.812\0.805\0.798\0.793\0.79\0.788\0.786\0.785\0.782\0.781\0.778\0.774\0.771\0.768\0.764\0.761\0.759\0.757\0.755\0.754\0.753\0.752\0.752\0.752\0.751\0.751\0.75\0.75\0.748\0.748\0.746\0.745\0.743\0.742\0.741\0.741\0.74\0.739\0.738\0.738\0.737\0.737\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.737\0.737\0.737\0.738\0.738\0.738\0.739\0.739\0.739\0.739\0.738\0.738\0.738\0.738\0.737\0.737\0.737\0.736\0.736\0.735\0.735\0.735\0.735\0.735\0.735\0.735\0.735\0.735\0.735\0.735\0.735\0.736\0.735\0.736\0.735\0.735\0.735\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.737\0.737\0.737\0.737\0.737\0.737\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.736\0.735\0.735\0.735\0.735\0.734\0.734\0.734\0.734\0.733\0.733\0.733\0.732\0.732\0.732\0.731\0.731\0.73\0.729\0.729\0.729\0.728\0.728\0.728\0.727\0.727\0.727\0.727\0.726\0.726\0.726\0.726\0.725\0.725\0.725\0.724\0.723\0.723\0.722\0.722\0.721\0.721\0.721\0.721\0.721\0.721\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.719\0.719\0.719\0.719\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.718\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.721\0.721\0.721\0.721\0.721\0.721\0.721\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.719\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.72\0.721</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>304</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="ksVK1V" Title="Quasi-Newton method results">
   <Caption Id="scI8UO">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.457
0.721
303
00:00:08
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="WqYkhv" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="x8TCxH" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="738lWh" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="RHDPRe">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0287457\42.6984\5.86908\4.16389
0.000399245\0.593033\0.081515\0.0578318
0.0399245\59.3033\8.1515\5.78318</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="lUZ1yr" Title="NO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="GWcfqB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.00201416\39.4687\6.91236\4.50469
2.79744e-5\0.548176\0.096005\0.0625652
0.00279744\54.8176\9.6005\6.25652</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ZNtviN" Title="NO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="VeMkNX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.00344467\36.5647\7.8409\4.91227
4.78427e-5\0.507843\0.108901\0.068226
0.00478427\50.7843\10.8901\6.8226</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="E4a3hd" Title="NO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="f6LDtp">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0456505\31.728\8.86266\4.96925
0.000634034\0.440666\0.123093\0.0690174
0.0634034\44.0666\12.3093\6.90174</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="VoYopI" Title="NO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="5KdKoQ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0001297\33.0333\8.85757\5.10483
1.80138e-6\0.458795\0.123022\0.0709004
0.000180138\45.8795\12.3022\7.09003</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="z1qNnx" Title="NO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="RDtyZk">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0512142\31.1955\8.59142\5.04542
0.000711309\0.433271\0.119325\0.0700753
0.0711309\43.3271\11.9325\7.00753</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="oupG1W" Title="NO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="iCjy6X">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0247192\30.5216\8.66592\5.11861
0.000343323\0.423911\0.12036\0.0710919
0.0343323\42.3911\12.036\7.10919</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4zW4oI" Title="NO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="xiFm0T">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0197372\34.9884\8.28483\5.35447
0.000274128\0.48595\0.115067\0.0743676
0.0274128\48.595\11.5067\7.43676</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="IMibHK" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="tOX2kU" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 1079.</Text>
 </Task>
 <Task Id="TMnF4n" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="l4a94X" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="Z8Wz9t" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="U1SpWV" Title="Quasi-Newton method errors history">
   <Caption Id="WHHhAU">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.23939, and the final value after 175 epochs is 0.433651.
The initial value of the selection error is 0.428381, and the final value after 175 epochs is 0.31046.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175</X2Data>
   <Y1Data>1.24\0.884\0.758\0.666\0.556\0.484\0.487\0.483\0.468\0.459\0.458\0.459\0.46\0.462\0.463\0.463\0.463\0.462\0.461\0.46\0.459\0.459\0.458\0.457\0.457\0.456\0.456\0.455\0.455\0.454\0.454\0.453\0.452\0.451\0.451\0.45\0.45\0.449\0.449\0.448\0.448\0.447\0.447\0.446\0.446\0.445\0.445\0.445\0.445\0.444\0.444\0.444\0.444\0.443\0.443\0.443\0.443\0.443\0.443\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.442\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.441\0.44\0.44\0.44\0.44\0.44\0.44\0.44\0.44\0.439\0.439\0.439\0.439\0.439\0.439\0.438\0.438\0.438\0.438\0.438\0.438\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.437\0.436\0.436\0.436\0.436\0.436\0.436\0.436\0.436\0.436\0.436\0.436\0.436\0.436\0.436\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.435\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434\0.434</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.428\0.605\0.572\0.422\0.46\0.327\0.332\0.287\0.283\0.29\0.295\0.296\0.297\0.298\0.298\0.298\0.299\0.3\0.3\0.301\0.303\0.305\0.306\0.307\0.309\0.31\0.311\0.312\0.312\0.312\0.313\0.312\0.312\0.312\0.312\0.312\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.314\0.314\0.314\0.314\0.315\0.315\0.315\0.315\0.315\0.315\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.316\0.315\0.315\0.315\0.315\0.315\0.315\0.315\0.314\0.314\0.314\0.314\0.314\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.313\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.312\0.311\0.311\0.311\0.311\0.311\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31\0.31</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>176</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="BSKJ63" Title="Quasi-Newton method results">
   <Caption Id="gixBbX">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.434
0.31
175
00:00:05
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="npiSLN" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="P5BYdI" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="PJAwgG" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="gnc0u9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00439811\15.9919\0.75451\0.99119
0.000258712\0.940697\0.044383\0.0583053
0.0258712\94.0697\4.4383\5.83053</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="0KWJXQ" Title="SO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="BpFHab">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00392199\15.7618\0.988796\1.04055
0.000230705\0.927166\0.0581644\0.0612088
0.0230705\92.7166\5.81644\6.12088</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="SA539a" Title="SO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="SuWBvM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00380921\15.3798\1.04911\1.061
0.000224071\0.904693\0.0617124\0.0624119
0.0224071\90.4693\6.17124\6.24119</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="1BzpdS" Title="SO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="OLkro3">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00380111\15.0348\1.15131\1.06153
0.000223595\0.884403\0.0677242\0.062443
0.0223595\88.4403\6.77242\6.2443</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="dIfDBW" Title="SO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="7sviYa">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.0070262\14.9087\1.20155\1.06533
0.000413306\0.876985\0.0706795\0.0626664
0.0413306\87.6985\7.06795\6.26664</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="K6DZNI" Title="SO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="y6zwTT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00136518\13.9847\1.19414\1.04651
8.0305e-5\0.822632\0.0702436\0.0615596
0.0080305\82.2632\7.02436\6.15596</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="qkjnMU" Title="SO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="00mPIS">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00300312\13.6533\1.16651\1.01449
0.000176654\0.803132\0.0686182\0.0596761
0.0176654\80.3132\6.86182\5.96761</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="wYnP2J" Title="SO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="W4EIRq">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00754738\14.1933\1.22895\1.03977
0.000443963\0.834901\0.0722912\0.0611627
0.0443963\83.4901\7.22912\6.11627</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="GIl3zO" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="iI7AkK" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 323.</Text>
 </Task>
 <Task Id="nqREol" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="Gt7acl" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="IC9XWo" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="rWOjvK" Title="Quasi-Newton method errors history">
   <Caption Id="TknBYI">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.18527, and the final value after 156 epochs is 0.475472.
The initial value of the selection error is 0.437559, and the final value after 156 epochs is 0.329664.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156</X2Data>
   <Y1Data>1.19\0.999\0.765\0.555\0.541\0.526\0.51\0.51\0.509\0.507\0.504\0.501\0.498\0.496\0.494\0.492\0.491\0.49\0.489\0.488\0.487\0.486\0.485\0.485\0.484\0.483\0.483\0.482\0.482\0.482\0.481\0.481\0.48\0.48\0.48\0.48\0.48\0.479\0.479\0.479\0.479\0.479\0.479\0.479\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.478\0.479\0.479\0.479\0.479\0.479\0.479\0.479\0.479\0.479\0.48\0.48\0.48\0.48\0.48\0.48\0.48\0.48\0.48\0.479\0.479\0.479\0.479\0.479\0.478\0.478\0.478\0.478\0.478\0.478\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.477\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.476\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475\0.475</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.438\0.443\0.583\0.353\0.318\0.322\0.328\0.321\0.319\0.322\0.323\0.323\0.323\0.321\0.319\0.318\0.318\0.318\0.319\0.32\0.321\0.322\0.323\0.323\0.324\0.324\0.324\0.324\0.324\0.325\0.325\0.325\0.326\0.326\0.326\0.326\0.326\0.326\0.326\0.325\0.325\0.324\0.324\0.323\0.323\0.323\0.323\0.323\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.324\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.324\0.325\0.325\0.326\0.327\0.327\0.328\0.328\0.328\0.328\0.328\0.327\0.327\0.326\0.326\0.325\0.324\0.324\0.323\0.323\0.322\0.321\0.321\0.321\0.321\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.32\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.321\0.322\0.322\0.322\0.322\0.322\0.323\0.323\0.323\0.323\0.323\0.323\0.323\0.324\0.324\0.324\0.324\0.324\0.325\0.325\0.325\0.325\0.326\0.326\0.326\0.327\0.327\0.327\0.328\0.328\0.328\0.329\0.329\0.329\0.33\0.33\0.331\0.332\0.333\0.333\0.333\0.333\0.333\0.332\0.332\0.331\0.33\0.33\0.329\0.329\0.329\0.33</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>157</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="e6ivGZ" Title="Quasi-Newton method results">
   <Caption Id="vB33jG">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.475
0.33
156
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="7WrubN" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="2Xfae1" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="GOw74r" Title="SO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="WvysHm">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00067544\16.09\0.726066\0.965359
3.97318e-5\0.946469\0.0427098\0.0567858
0.00397318\94.6469\4.27098\5.67858</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4g1Fcf" Title="SO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="HuS8Wg">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00150263\15.7359\0.956048\1.01653
8.83902e-5\0.925643\0.0562381\0.0597959
0.00883902\92.5643\5.62381\5.97959</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Ica8h8" Title="SO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="og58sR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00110698\15.6018\1.0691\1.02843
6.51163e-5\0.917752\0.0628881\0.0604957
0.00651163\91.7752\6.28881\6.04957</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="pikVSC" Title="SO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="fhDzXb">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.0032562\15.5787\1.11914\1.0278
0.000191541\0.916391\0.0658319\0.0604591
0.0191541\91.6391\6.58319\6.04591</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ZxoLkE" Title="SO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="QIs0aX">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.000684857\15.2117\1.19359\1.02528
4.02857e-5\0.894807\0.0702114\0.0603105
0.00402857\89.4807\7.02114\6.03105</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="WUXl66" Title="SO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="tzj9rl">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00663948\14.1118\1.22506\0.996392
0.000390558\0.830105\0.0720624\0.0586113
0.0390558\83.0105\7.20624\5.86113</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="LRgBXm" Title="SO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="4Mm6XQ">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.0169697\13.6623\1.22508\0.977327
0.000998217\0.803663\0.0720635\0.0574898
0.0998216\80.3663\7.20635\5.74898</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="1mkont" Title="SO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="fBbNkj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0222127 and its percentage error 2.2213</Caption>
   <Data>0.00219464\14.1113\1.2511\0.989845
0.000129097\0.830075\0.0735939\0.0582262
0.0129097\83.0075\7.35939\5.82262</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="ioztK6" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="3yYObv" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 314.</Text>
 </Task>
 <Task Id="RTmdrx" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="AYFzDm" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="7ZaYbF" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Fh83BR">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0214748\42.9214\8.99013\5.57274
0.000298262\0.59613\0.124863\0.0773992
0.0298262\59.613\12.4863\7.73992</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="YFnMT4" Title="NO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="HIMbWF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.015173\42.701\7.37654\4.95681
0.000210736\0.593069\0.102452\0.0688445
0.0210736\59.3069\10.2452\6.88445</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="44i0EC" Title="NO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="LwEXYB">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.00311279\40.7341\8.059\5.11528
4.32332e-5\0.565751\0.11193\0.0710455
0.00432332\56.5751\11.193\7.10455</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="2ul6zb" Title="NO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="ToiPSA">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0579262\43.0653\7.45027\4.8808
0.00080453\0.598129\0.103476\0.0677888
0.080453\59.8129\10.3476\6.77888</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ng6NF0" Title="NO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="luqw64">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0166512\41.1254\8.23379\5.12671
0.000231266\0.571185\0.114358\0.0712043
0.0231266\57.1185\11.4358\7.12043</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="X2qqTe" Title="NO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="aXYwNv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0253448\37.2396\8.81935\5.50881
0.000352012\0.517217\0.122491\0.0765112
0.0352012\51.7217\12.2491\7.65112</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="fpmLOh" Title="NO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="Js0ABC">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0108204\39.4158\8.64296\5.44365
0.000150283\0.547442\0.120041\0.0756062
0.0150283\54.7442\12.0041\7.56062</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="zygIoJ" Title="NO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="H3GmTv">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0169525\43.2455\7.95404\5.20706
0.000235452\0.600632\0.110473\0.0723203
0.0235452\60.0632\11.0473\7.23203</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="XCXp4c" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="Y8NfRC" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="FoRKBZ" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="I2ru1i" Title="Quasi-Newton method errors history">
   <Caption Id="6T8JJ8">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.07826, and the final value after 191 epochs is 0.686941.
The initial value of the selection error is 1.07859, and the final value after 191 epochs is 0.893662.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191</X2Data>
   <Y1Data>1.08\0.884\0.766\0.728\0.73\0.723\0.721\0.721\0.719\0.717\0.716\0.715\0.714\0.712\0.711\0.71\0.709\0.708\0.707\0.706\0.706\0.705\0.704\0.703\0.702\0.701\0.701\0.7\0.7\0.699\0.699\0.699\0.699\0.699\0.698\0.698\0.698\0.698\0.698\0.698\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.697\0.696\0.696\0.696\0.696\0.696\0.696\0.695\0.695\0.695\0.695\0.695\0.695\0.694\0.694\0.694\0.694\0.693\0.693\0.693\0.693\0.692\0.692\0.692\0.692\0.691\0.691\0.691\0.691\0.691\0.69\0.69\0.69\0.689\0.689\0.689\0.689\0.689\0.689\0.689\0.689\0.689\0.689\0.689\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>1.08\1.26\1.15\1.01\0.995\0.979\0.983\0.976\0.959\0.949\0.945\0.946\0.945\0.941\0.938\0.934\0.928\0.924\0.919\0.916\0.914\0.91\0.908\0.906\0.905\0.904\0.903\0.902\0.901\0.901\0.9\0.899\0.899\0.899\0.898\0.898\0.898\0.898\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.899\0.9\0.9\0.901\0.901\0.901\0.902\0.903\0.903\0.903\0.904\0.904\0.904\0.904\0.904\0.904\0.904\0.904\0.904\0.904\0.903\0.903\0.903\0.902\0.902\0.902\0.902\0.901\0.901\0.902\0.902\0.902\0.902\0.902\0.902\0.901\0.9\0.899\0.898\0.897\0.896\0.896\0.895\0.894\0.894\0.894\0.894\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.89\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.891\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.892\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.893\0.894\0.894\0.894\0.894</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>192</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="4bkjwW" Title="Quasi-Newton method results">
   <Caption Id="OsugUu">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.687
0.894
191
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="BM2GIS" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="XCi0Ph" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="9g6b6X" Title="NO2(microg/m3)_ahead_1 errors statistics">
   <Caption Id="i9zkcx">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.00414467\47.2993\5.82619\4.42566
5.75648e-5\0.656934\0.0809194\0.0614675
0.00575648\65.6934\8.09194\6.14675</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="6iIfH7" Title="NO2(microg/m3)_ahead_2 errors statistics">
   <Caption Id="vBVFAV">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0034008\43.2173\7.91146\5.33602
4.72334e-5\0.600241\0.109881\0.0741114
0.00472334\60.0241\10.9881\7.41114</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="yHUBg8" Title="NO2(microg/m3)_ahead_3 errors statistics">
   <Caption Id="0hfqe7">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.00354385\42.4956\8.36407\5.59292
4.92202e-5\0.590217\0.116168\0.0776794
0.00492202\59.0217\11.6168\7.76794</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="LTOrny" Title="NO2(microg/m3)_ahead_4 errors statistics">
   <Caption Id="bljnRw">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.00330353\39.0657\9.29031\5.95687
4.58823e-5\0.542579\0.129032\0.0827343
0.00458823\54.2579\12.9032\8.27343</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="SwIaud" Title="NO2(microg/m3)_ahead_5 errors statistics">
   <Caption Id="aXCCa1">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0155525\40.9988\9.16663\5.84763
0.000216007\0.569427\0.127314\0.081217
0.0216007\56.9427\12.7314\8.1217</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="GvvNuh" Title="NO2(microg/m3)_ahead_6 errors statistics">
   <Caption Id="DFCBXf">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0268669\40.7671\9.09956\5.87308
0.000373152\0.56621\0.126383\0.0815706
0.0373152\56.621\12.6383\8.15706</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Oq5gN1" Title="NO2(microg/m3)_ahead_7 errors statistics">
   <Caption Id="WmigQF">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0208054\38.2055\8.84145\5.78223
0.000288963\0.530632\0.122798\0.0803088
0.0288963\53.0632\12.2798\8.03088</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="cXOGRL" Title="NO2(microg/m3)_ahead_8 errors statistics">
   <Caption Id="ZY6Oly">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0822655 and its percentage error 8.22634</Caption>
   <Data>0.0316334\35.2857\9.15692\5.85501
0.000439352\0.490079\0.127179\0.0813196
0.0439352\49.0079\12.7179\8.13196</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="w8F8lz" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="hCzQdN" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 314.</Text>
 </Task>
 <Task Id="sNgHLc" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="CaqgyP" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="6uiW2h" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="8ADnOp" Title="Quasi-Newton method errors history">
   <Caption Id="dJ9lOt">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.2583, and the final value after 192 epochs is 0.608515.
The initial value of the selection error is 0.80453, and the final value after 192 epochs is 0.34905.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192</X2Data>
   <Y1Data>1.26\0.974\0.698\0.647\0.647\0.63\0.622\0.626\0.629\0.628\0.626\0.624\0.622\0.62\0.62\0.62\0.62\0.62\0.619\0.618\0.618\0.617\0.617\0.616\0.616\0.616\0.616\0.616\0.615\0.615\0.615\0.615\0.615\0.615\0.616\0.616\0.616\0.616\0.616\0.616\0.616\0.616\0.616\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.615\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.614\0.613\0.613\0.613\0.613\0.613\0.613\0.613\0.613\0.613\0.613\0.613\0.612\0.612\0.612\0.612\0.612\0.612\0.612\0.612\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.611\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.61\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.608\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609\0.609</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.805\0.59\0.432\0.435\0.38\0.37\0.372\0.365\0.37\0.367\0.363\0.361\0.358\0.357\0.356\0.355\0.354\0.353\0.352\0.352\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.35\0.35\0.35\0.35\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.348\0.348\0.348\0.348\0.348\0.348\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.349\0.349\0.349\0.349\0.349\0.349\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.349</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>193</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="BASarb" Title="Quasi-Newton method results">
   <Caption Id="iqPl2S">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.609
0.349
192
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="iLUzPF" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="gd8bTH" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="zHArmh" Title="O3(microg/m3)_ahead_1 errors statistics">
   <Caption Id="anGFF4">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.0135765\20.9835\4.51691\3.73072
5.4744e-5\0.084611\0.0182134\0.0150432
0.0054744\8.4611\1.82134\1.50432</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="VzIGQF" Title="O3(microg/m3)_ahead_2 errors statistics">
   <Caption Id="iqWzyT">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.00662613\27.8829\5.22596\4.1713
2.67183e-5\0.112431\0.0210724\0.0168197
0.00267183\11.2431\2.10724\1.68197</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="BMl1Nw" Title="O3(microg/m3)_ahead_3 errors statistics">
   <Caption Id="WTm3Ef">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.00404739\26.0968\5.42015\4.26127
1.63201e-5\0.105229\0.0218554\0.0171825
0.00163201\10.5229\2.18554\1.71825</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="OlLXgo" Title="O3(microg/m3)_ahead_4 errors statistics">
   <Caption Id="mmCcBN">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.021143\26.1409\5.42435\4.3391
8.52539e-5\0.105407\0.0218724\0.0174964
0.00852539\10.5407\2.18724\1.74964</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="f6Hxj2" Title="O3(microg/m3)_ahead_5 errors statistics">
   <Caption Id="dM9h9b">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.028986\26.6622\5.30248\4.35168
0.000116879\0.107509\0.021381\0.0175471
0.0116879\10.7509\2.1381\1.75471</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Zi5Ftm" Title="O3(microg/m3)_ahead_6 errors statistics">
   <Caption Id="pHKaUY">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.0174789\27.4972\5.35352\4.46388
7.04796e-5\0.110876\0.0215868\0.0179995
0.00704796\11.0876\2.15868\1.79995</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="vpgUHq" Title="O3(microg/m3)_ahead_7 errors statistics">
   <Caption Id="pwwrln">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.00743484\28.1922\5.43525\4.51415
2.99792e-5\0.113678\0.0219163\0.0182022
0.00299792\11.3678\2.19163\1.82022</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="OCTBnV" Title="O3(microg/m3)_ahead_8 errors statistics">
   <Caption Id="gYPMMD">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.079423 and its percentage error 7.94214</Caption>
   <Data>0.0178337\25.6856\5.40345\4.4714
7.19101e-5\0.103571\0.0217881\0.0180298
0.00719101\10.3571\2.17881\1.80298</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="lobZUk" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="83KMnU" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 314.</Text>
 </Task>
 <Task Id="mWmBtA" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="JDAJST" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="zcgceD" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="fULWIX" Title="Quasi-Newton method errors history">
   <Caption Id="NkBEvF">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.874806, and the final value after 165 epochs is 0.663696.
The initial value of the selection error is 0.625901, and the final value after 165 epochs is 0.519615.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165</X2Data>
   <Y1Data>0.875\0.761\0.703\0.691\0.696\0.695\0.694\0.69\0.686\0.684\0.683\0.681\0.679\0.678\0.677\0.676\0.675\0.675\0.674\0.673\0.673\0.672\0.671\0.671\0.67\0.669\0.669\0.668\0.668\0.668\0.667\0.667\0.667\0.667\0.667\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.666\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.665\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664\0.664</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.626\0.592\0.564\0.512\0.515\0.532\0.538\0.531\0.521\0.514\0.51\0.509\0.511\0.513\0.514\0.515\0.515\0.515\0.514\0.514\0.513\0.513\0.513\0.513\0.513\0.513\0.513\0.514\0.514\0.514\0.514\0.515\0.515\0.515\0.515\0.515\0.515\0.515\0.516\0.516\0.516\0.516\0.516\0.516\0.517\0.517\0.517\0.517\0.517\0.517\0.517\0.517\0.517\0.517\0.517\0.517\0.518\0.518\0.518\0.518\0.518\0.519\0.519\0.519\0.519\0.519\0.519\0.519\0.519\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.521\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52\0.52</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>166</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="hizP9q" Title="Quasi-Newton method results">
   <Caption Id="BW8QH0">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.664
0.52
165
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="w5fzSA" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="u5bpIb" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="BDSWFF" Title="PM10(microg/m3)_ahead_1 errors statistics">
   <Caption Id="FkBYnx">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.00887108\308.283\6.99635\14.3126
2.60914e-5\0.906716\0.0205775\0.042096
0.00260914\90.6716\2.05775\4.2096</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="wKCPRa" Title="PM10(microg/m3)_ahead_2 errors statistics">
   <Caption Id="uQYKYk">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.0036602\326.716\8.63497\15.424
1.07653e-5\0.960929\0.025397\0.0453647
0.00107653\96.0929\2.5397\4.53647</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="e0Bh9I" Title="PM10(microg/m3)_ahead_3 errors statistics">
   <Caption Id="3jYyOj">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.0144691\325.971\9.13058\15.6144
4.25563e-5\0.958739\0.0268546\0.0459246
0.00425563\95.8739\2.68546\4.59246</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="CwsujF" Title="PM10(microg/m3)_ahead_4 errors statistics">
   <Caption Id="wNzHJk">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.00670242\324.65\9.4057\15.5296
1.9713e-5\0.954854\0.0276638\0.0456754
0.0019713\95.4854\2.76638\4.56754</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="UuP7tl" Title="PM10(microg/m3)_ahead_5 errors statistics">
   <Caption Id="tame4T">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.0479317\318.131\9.46565\15.3721
0.000140976\0.93568\0.0278401\0.045212
0.0140975\93.568\2.78401\4.5212</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="4UVQn8" Title="PM10(microg/m3)_ahead_6 errors statistics">
   <Caption Id="xdnMsi">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.00286102\323.073\9.63124\15.5828
8.41477e-6\0.950215\0.0283272\0.0458319
0.000841477\95.0215\2.83272\4.58319</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="SIYeG1" Title="PM10(microg/m3)_ahead_7 errors statistics">
   <Caption Id="M83VRe">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.00863457\323.658\9.79837\15.6471
2.53958e-5\0.951934\0.0288187\0.0460209
0.00253958\95.1934\2.88187\4.60209</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="3c6xyS" Title="PM10(microg/m3)_ahead_8 errors statistics">
   <Caption Id="ef2pN6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0228084 and its percentage error 2.28081</Caption>
   <Data>0.0175762\323.439\9.90034\15.6667
5.16948e-5\0.951292\0.0291187\0.0460786
0.00516948\95.1292\2.91187\4.60786</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="rHgPx7" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="N35WUF" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 314.</Text>
 </Task>
 <Task Id="JvCJ54" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="EtzaU4" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="blXbpl" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="ywTQrk" Title="Quasi-Newton method errors history">
   <Caption Id="xkcFyn">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 0.981747, and the final value after 261 epochs is 0.685653.
The initial value of the selection error is 0.971493, and the final value after 261 epochs is 0.741199.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261</X2Data>
   <Y1Data>0.982\0.828\0.756\0.733\0.729\0.72\0.718\0.719\0.719\0.717\0.714\0.713\0.712\0.71\0.709\0.708\0.707\0.705\0.704\0.703\0.701\0.7\0.699\0.699\0.698\0.697\0.696\0.695\0.694\0.694\0.693\0.692\0.692\0.692\0.691\0.691\0.69\0.69\0.69\0.689\0.689\0.689\0.689\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.688\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.687\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.686\0.686\0.686\0.686\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.685\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686\0.686</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.971\0.837\0.775\0.758\0.76\0.755\0.751\0.751\0.749\0.746\0.743\0.742\0.742\0.741\0.74\0.741\0.741\0.741\0.741\0.74\0.74\0.74\0.739\0.739\0.739\0.739\0.739\0.74\0.74\0.741\0.741\0.742\0.742\0.743\0.743\0.743\0.743\0.743\0.743\0.744\0.744\0.745\0.745\0.746\0.746\0.746\0.746\0.746\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.748\0.749\0.749\0.749\0.749\0.749\0.749\0.75\0.75\0.75\0.75\0.75\0.75\0.75\0.75\0.75\0.75\0.75\0.75\0.75\0.75\0.75\0.749\0.749\0.748\0.748\0.748\0.748\0.748\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.747\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.746\0.745\0.745\0.745\0.745\0.745\0.745\0.745\0.745\0.745\0.745\0.745\0.745\0.745\0.745\0.745\0.744\0.744\0.744\0.744\0.744\0.744\0.744\0.744\0.744\0.744\0.744\0.744\0.743\0.743\0.743\0.743\0.743\0.743\0.743\0.742\0.742\0.742\0.742\0.741\0.74\0.74\0.74\0.739\0.739\0.739\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.738\0.739\0.739\0.739\0.739\0.74\0.74\0.74\0.741\0.741\0.742\0.743\0.743\0.743\0.743\0.743\0.742\0.742\0.742\0.742\0.742\0.742\0.742\0.741\0.741\0.741\0.741\0.741\0.741\0.741</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>262</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>1</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="mid7US" Title="Quasi-Newton method results">
   <Caption Id="a7BAKT">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.686
0.741
261
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="MHKRvP" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="RgJQ6m" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="9dxFrx" Title="PM2.5(microg/m3)_ahead_1 errors statistics">
   <Caption Id="Vr47WM">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0787927 and its percentage error 7.87934</Caption>
   <Data>0.0596447\118.659\7.63546\9.42039
0.000382338\0.760633\0.0489453\0.0603871
0.0382338\76.0633\4.89453\6.03871</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="U6Jy94" Title="PM2.5(microg/m3)_ahead_2 errors statistics">
   <Caption Id="BdKQS6">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0787927 and its percentage error 7.87934</Caption>
   <Data>0.0603676\111.648\12.2817\12.5008
0.000386972\0.715692\0.0787291\0.0801331
0.0386972\71.5692\7.87291\8.01331</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="kG2lx4" Title="PM2.5(microg/m3)_ahead_3 errors statistics">
   <Caption Id="6U3oY9">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0787927 and its percentage error 7.87934</Caption>
   <Data>0.0649529\123.713\14.7053\13.7948
0.000416364\0.793031\0.0942649\0.0884281
0.0416364\79.3031\9.42649\8.8428</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ogOFI9" Title="PM2.5(microg/m3)_ahead_4 errors statistics">
   <Caption Id="PWBS2U">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0787927 and its percentage error 7.87934</Caption>
   <Data>0.146439\120.825\15.6069\14.2225
0.000938709\0.774519\0.100044\0.0911699
0.0938709\77.4519\10.0044\9.11699</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="cYsUkY" Title="PM2.5(microg/m3)_ahead_5 errors statistics">
   <Caption Id="JZ4af8">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0787927 and its percentage error 7.87934</Caption>
   <Data>0.00226974\118.109\15.8585\14.3785
1.45496e-5\0.757109\0.101657\0.0921696
0.00145496\75.7109\10.1657\9.21696</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="5Hiv7L" Title="PM2.5(microg/m3)_ahead_6 errors statistics">
   <Caption Id="DtxA3M">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0787927 and its percentage error 7.87934</Caption>
   <Data>0.0286217\111.35\15.841\14.4459
0.000183472\0.713781\0.101545\0.0926021
0.0183472\71.3781\10.1545\9.26021</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="Z4PKTg" Title="PM2.5(microg/m3)_ahead_7 errors statistics">
   <Caption Id="KytS3p">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0787927 and its percentage error 7.87934</Caption>
   <Data>0.0164185\117.818\15.9137\14.5273
0.000105247\0.755246\0.102011\0.093124
0.0105247\75.5246\10.2011\9.3124</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="K5LdoM" Title="PM2.5(microg/m3)_ahead_8 errors statistics">
   <Caption Id="toiBnb">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0787927 and its percentage error 7.87934</Caption>
   <Data>0.0758934\118.463\16.06\14.452
0.000486496\0.759378\0.102949\0.0926408
0.0486496\75.9378\10.2949\9.26408</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="xOBkn4" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="VJQmhY" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="Vnee71" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="j89tu3" Title="Quasi-Newton method errors history">
   <Caption Id="XXnv63">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 2.80092, and the final value after 293 epochs is 0.356374.
The initial value of the selection error is 2.45167, and the final value after 293 epochs is 0.276682.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210\211\212\213\214\215\216\217\218\219\220\221\222\223\224\225\226\227\228\229\230\231\232\233\234\235\236\237\238\239\240\241\242\243\244\245\246\247\248\249\250\251\252\253\254\255\256\257\258\259\260\261\262\263\264\265\266\267\268\269\270\271\272\273\274\275\276\277\278\279\280\281\282\283\284\285\286\287\288\289\290\291\292\293</X2Data>
   <Y1Data>2.8\1.39\0.785\0.719\0.691\0.688\0.673\0.6\0.57\0.531\0.481\0.469\0.446\0.428\0.416\0.407\0.401\0.397\0.392\0.39\0.386\0.383\0.381\0.379\0.379\0.378\0.377\0.376\0.375\0.374\0.373\0.372\0.371\0.371\0.371\0.37\0.37\0.37\0.37\0.369\0.369\0.369\0.369\0.369\0.369\0.369\0.368\0.368\0.368\0.367\0.366\0.366\0.366\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.365\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.364\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.363\0.362\0.362\0.362\0.362\0.362\0.362\0.362\0.361\0.361\0.361\0.36\0.36\0.36\0.359\0.359\0.359\0.359\0.359\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.359\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.358\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.357\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356\0.356</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>2.45\1.5\0.802\0.663\0.536\0.589\0.667\0.633\0.502\0.455\0.427\0.422\0.374\0.383\0.376\0.362\0.35\0.343\0.335\0.339\0.341\0.335\0.33\0.327\0.325\0.324\0.323\0.323\0.321\0.318\0.316\0.315\0.314\0.312\0.311\0.31\0.309\0.309\0.309\0.308\0.308\0.308\0.308\0.307\0.307\0.307\0.306\0.306\0.305\0.304\0.304\0.303\0.302\0.301\0.301\0.3\0.299\0.298\0.297\0.296\0.296\0.295\0.295\0.294\0.294\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.293\0.292\0.292\0.292\0.292\0.292\0.291\0.291\0.291\0.29\0.29\0.289\0.289\0.289\0.289\0.288\0.288\0.288\0.288\0.288\0.288\0.287\0.287\0.287\0.287\0.286\0.286\0.285\0.285\0.284\0.284\0.283\0.283\0.282\0.282\0.281\0.28\0.28\0.279\0.278\0.277\0.276\0.276\0.275\0.274\0.274\0.273\0.273\0.273\0.273\0.273\0.273\0.273\0.274\0.274\0.275\0.275\0.276\0.276\0.276\0.276\0.276\0.276\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.277\0.277\0.277\0.277\0.277\0.277\0.278\0.278\0.278\0.279\0.279\0.279\0.279\0.279\0.28\0.28\0.28\0.28\0.28\0.28\0.281\0.281\0.281\0.28\0.28\0.28\0.28\0.28\0.279\0.279\0.279\0.279\0.279\0.279\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.278\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.277\0.277\0.277\0.277\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.276\0.277\0.277\0.277\0.277\0.277</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>294</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>3</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="AquueN" Title="Quasi-Newton method results">
   <Caption Id="5qHLen">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.356
0.277
293
00:00:01
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="7x34M4" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="7froXg" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="iuQYDW" Title="PM2.5(AQI)_ahead_1 errors statistics">
   <Caption Id="nO0ROx">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392727 and its percentage error 3.92724</Caption>
   <Data>0.000915527\111.604\6.91429\8.78759
5.86877e-6\0.715413\0.0443224\0.0563307
0.000586877\71.5413\4.43224\5.63307</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="c7p67P" Title="PM10(AQI)_ahead_1 errors statistics">
   <Caption Id="NYxL2I">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392727 and its percentage error 3.92724</Caption>
   <Data>0.0317993\306.161\6.91846\14.2059
9.35274e-5\0.900474\0.0203484\0.0417822
0.00935274\90.0474\2.03484\4.17822</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="YKKcW5" Title="O3(AQI)_ahead_1 errors statistics">
   <Caption Id="nPksR5">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392727 and its percentage error 3.92724</Caption>
   <Data>0.00760651\20.3214\4.62674\3.78373
3.06714e-5\0.0819413\0.0186562\0.015257
0.00306714\8.19413\1.86562\1.5257</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="c5BcZE" Title="NO2(AQI)_ahead_1 errors statistics">
   <Caption Id="XHyy2Z">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392727 and its percentage error 3.92724</Caption>
   <Data>0.00697327\44.2025\5.70316\4.32312
9.68509e-5\0.613924\0.0792106\0.0600434
0.00968509\61.3924\7.92106\6.00434</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="hsI77W" Title="SO2(AQI)_ahead_1 errors statistics">
   <Caption Id="uVJFQx">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0392727 and its percentage error 3.92724</Caption>
   <Data>0.000347853\15.4411\0.580123\0.928088
2.04619e-5\0.908303\0.0341249\0.0545934
0.00204619\90.8303\3.41249\5.45934</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="A966eK" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="uzeTtS" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="ExCgfg" Title="Neural network" Component="Neural network" Name="Randomize parameters">
  <Text Id="wIurdC" Title="Task description">Neural Designer has initialized the neural network parameters at random with a uniform distribution.
The number of randomized parameters is 419.</Text>
 </Task>
 <Task Id="mFMOBx" Title="Training" Component="Training strategy" Name="Perform training">
  <Text Id="Wat2Yg" Title="Task description">The procedure used to carry out the learning process is called the training (or learning) strategy.
The training strategy is applied to the neural network to obtain the best possible loss.
The type of training is determined by how the adjustment of the parameters in the neural network takes place.
</Text>
  <Text Id="cIxz4F" Title="Optimization algorithm">The quasi-Newton method is used here for training.
It is based on Newton's method but does not require the calculation of second derivatives.
Instead, the quasi-Newton method computes an approximation of the inverse Hessian at each iteration of the algorithm by only using gradient information. </Text>
  <DoubleLineChart Id="4bDsUN" Title="Quasi-Newton method errors history">
   <Caption Id="EQen7H">The following plot shows the training and selection errors in each iteration.
The blue line represents the training error, and the orange line represents the selection error.
The initial value of the training error is 1.0608, and the final value after 210 epochs is 0.344065.
The initial value of the selection error is 0.91911, and the final value after 210 epochs is 0.340762.
</Caption>
   <X1Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210</X1Data>
   <X2Data>0\1\2\3\4\5\6\7\8\9\10\11\12\13\14\15\16\17\18\19\20\21\22\23\24\25\26\27\28\29\30\31\32\33\34\35\36\37\38\39\40\41\42\43\44\45\46\47\48\49\50\51\52\53\54\55\56\57\58\59\60\61\62\63\64\65\66\67\68\69\70\71\72\73\74\75\76\77\78\79\80\81\82\83\84\85\86\87\88\89\90\91\92\93\94\95\96\97\98\99\100\101\102\103\104\105\106\107\108\109\110\111\112\113\114\115\116\117\118\119\120\121\122\123\124\125\126\127\128\129\130\131\132\133\134\135\136\137\138\139\140\141\142\143\144\145\146\147\148\149\150\151\152\153\154\155\156\157\158\159\160\161\162\163\164\165\166\167\168\169\170\171\172\173\174\175\176\177\178\179\180\181\182\183\184\185\186\187\188\189\190\191\192\193\194\195\196\197\198\199\200\201\202\203\204\205\206\207\208\209\210</X2Data>
   <Y1Data>1.06\0.74\0.605\0.556\0.526\0.497\0.464\0.434\0.42\0.408\0.398\0.392\0.381\0.376\0.371\0.368\0.366\0.365\0.363\0.361\0.36\0.358\0.356\0.355\0.354\0.354\0.353\0.353\0.353\0.352\0.352\0.352\0.352\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.351\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.35\0.349\0.349\0.349\0.349\0.349\0.349\0.349\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.347\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.346\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.345\0.344\0.344\0.344\0.344\0.344\0.344\0.344\0.344</Y1Data>
   <Y1Name>Training error</Y1Name>
   <Color1>#437CC0</Color1>
   <Width1>1</Width1>
   <Areas1>false</Areas1>
   <Y2Data>0.919\0.79\0.825\0.743\0.644\0.564\0.54\0.491\0.457\0.45\0.434\0.43\0.421\0.415\0.409\0.407\0.403\0.401\0.397\0.394\0.391\0.387\0.383\0.381\0.379\0.377\0.375\0.373\0.371\0.368\0.367\0.365\0.365\0.364\0.363\0.362\0.361\0.36\0.359\0.358\0.357\0.357\0.356\0.355\0.354\0.353\0.352\0.351\0.351\0.35\0.35\0.35\0.35\0.349\0.349\0.349\0.348\0.348\0.348\0.348\0.348\0.348\0.348\0.347\0.347\0.347\0.346\0.346\0.345\0.345\0.344\0.344\0.343\0.343\0.342\0.342\0.341\0.341\0.34\0.34\0.34\0.339\0.339\0.339\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.337\0.337\0.337\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.337\0.337\0.337\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.338\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.339\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.34\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341\0.341</Y2Data>
   <Y2Name>Selection error</Y2Name>
   <Color2>#FF4F4A</Color2>
   <Width2>1</Width2>
   <Areas2>false</Areas2>
   <Metadata/>
   <MetadataName/>
   <XLabel>Epoch</XLabel>
   <YLabel>Normalized squared error</YLabel>
   <XMinimum>0</XMinimum>
   <XMaximum>211</XMaximum>
   <YMinimum>0</YMinimum>
   <YMaximum>2</YMaximum>
   <Dates>false</Dates>
  </DoubleLineChart>
  <Table Id="lk8cnk" Title="Quasi-Newton method results">
   <Caption Id="0YkTg6">The following table shows the training results by the quasi-Newton method.
They include some final states from the neural network, the loss index, and the optimization algorithm.
</Caption>
   <Data>0.344
0.341
210
00:00:00
Maximum selection error increases
</Data>
   <RowsName>Training error\Selection error\Epochs number\Elapsed time\Stopping criterion\</RowsName>
   <ColumnsName>Value</ColumnsName>
   <RowHeadingsWidth>13</RowHeadingsWidth>
   <ColumnHeadingsWidth>20</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
 <Task Id="zBrJ8I" Title="Errors statistics" Component="Testing analysis" Name="Calculate errors statistics">
  <Text Id="KF8N1W" Title="Task description">The errors statistics measure the minimums, maximums, means, and standard deviations of the errors between the neural network and the testing samples in the data set.
 They provide a valuable tool for testing the quality of a model. </Text>
  <Table Id="NmAXhw" Title="PM2.5(AQI)_ahead_1 errors statistics">
   <Caption Id="BlBc9Q">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0412501 and its percentage error 4.12499</Caption>
   <Data>0.055275\71.0676\7.39672\8.26403
0.000354327\0.455562\0.0474149\0.0529746
0.0354327\45.5562\4.74149\5.29746</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="KNqiJt" Title="PM10(AQI)_ahead_1 errors statistics">
   <Caption Id="rNsN8I">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0412501 and its percentage error 4.12499</Caption>
   <Data>0.0479088\307.446\7.40696\16.7887
0.000140908\0.904253\0.0217852\0.0493785
0.0140908\90.4253\2.17852\4.93785</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="dgeLhV" Title="O3(AQI)_ahead_1 errors statistics">
   <Caption Id="e13iqr">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0412501 and its percentage error 4.12499</Caption>
   <Data>0.00849152\18.6509\4.80947\3.9552
3.424e-5\0.0752051\0.019393\0.0159484
0.003424\7.52051\1.9393\1.59484</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="dVx8ko" Title="NO2(AQI)_ahead_1 errors statistics">
   <Caption Id="cyXe4S">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0412501 and its percentage error 4.12499</Caption>
   <Data>0.0582047\20.5015\5.57888\3.97535
0.000808398\0.284742\0.0774845\0.0552133
0.0808398\28.4742\7.74845\5.52133</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
  <Table Id="ffQqnl" Title="SO2(AQI)_ahead_1 errors statistics">
   <Caption Id="7mPyX0">The table below shows the minimums, maximums, means, and standard deviations of the absolute and percentage errors of the neural network for the testing data.
The mean relative error obtained by using the previous value as the prediction is 0.0412501 and its percentage error 4.12499</Caption>
   <Data>0.00670576\7.68692\0.555692\0.696871
0.000394457\0.452172\0.0326878\0.0409924
0.0394457\45.2172\3.26878\4.09924</Data>
   <RowsName>Absolute error\Relative error\Percentage error</RowsName>
   <ColumnsName>Minimum\Maximum\Mean\Deviation</ColumnsName>
   <RowHeadingsWidth>12</RowHeadingsWidth>
   <ColumnHeadingsWidth>6</ColumnHeadingsWidth>
   <Alignment>right</Alignment>
  </Table>
 </Task>
</NeuralDesignerOutput>
